<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stay Hungry,Stay Foolish.</title>
  
  <subtitle>莫坠青云志</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tobiaslee.top/"/>
  <updated>2021-09-22T13:02:47.116Z</updated>
  <id>https://tobiaslee.top/</id>
  
  <author>
    <name>TobiasLee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>研一这一年</title>
    <link href="https://tobiaslee.top/2021/09/10/My-First-Graduate-Year-at-PKU/"/>
    <id>https://tobiaslee.top/2021/09/10/My-First-Graduate-Year-at-PKU/</id>
    <published>2021-09-10T14:54:57.000Z</published>
    <updated>2021-09-22T13:02:47.116Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎大家关注我的公众号<strong>三石杂货铺</strong>，第一时间收获更新：</p><p><img src="/img/qrcode.png" alt="QR Code"></p><hr><p>园子里的稚嫩面孔多了起来，熙熙攘攘的校门迎来送往，我也在这个门中，度过了完整的春夏秋冬。北京的秋天是很凉爽的，或许当得上是最舒服的一个季节。一年级的第二个学期也比第一个学期舒服了许多。</p><p>课程方面，顺利修满了课程的学分，绩点都还看得过去。有趣的是选了一门讲座课，和周明老师吃了个便餐，佩服他从首席科学家到创新工场投资人的转变，也豁然觉得学而优则天使投资人似乎成为了一个新的人生规划选择。这门课更加令我惊讶的是最后的课程展示，我们小组摸鱼的成果居然得了第一名，并且获得了 3000 元的奖金，简直比做梦还梦幻（PS：隔壁寝室拿了第二，这课包圆了被我们）。</p><p>生活方面，买了小电驴，对于居住在万柳的我而言幸福指数直线上升，妈妈再也不用担心我要赶班车啦。通勤更加方便以外，有了车之后的活动半径也扩大了很多，可以动不动到五道口吃个网红餐厅。如果说有什么缺点的话，就是下雨的时候不太友好，送的雨披约等于没有并且感觉穿雨衣骑车也很不舒服。好在北京的雨多是阵雨，若是碰上特别大的雨，等一会再打伞骑回去就行。新开的家园食堂也大大丰富了早饭的选择，一方面是开的比较晚，萝卜干星人表示家一早上的萝卜干+菊花粥/豆浆+油条简直是dream breakfast。</p><p>运动方面，今年应该是算是超额完成了运动量。首当其中的是在包括黄教练@hlz、王教练@wpy、许教练@xrx、李教练@lyf 以及梦觉教练@梦觉教游泳的帮助下，勉强能够学会游泳。说来惭愧，作为一个游泳大省的学生，居然研一才学会游泳，正是太不应该了。不过相比于一开始连漂浮都困难，目前我已经能够坚持蛙泳<del>50</del> 900米，可以算是一个巨大突破了，希望以后能够多多精进，学学酷炫的其他泳姿。在北大杯代表信科完成首秀，并且出道即巅峰在第一场比赛中完成传射，之后水平断崖式下跌沦为管理员，在对抗比较强的比赛里我还是太容易着急和慌张了，需要多练练。</p><h2 id="Research"><a href="#Research" class="headerlink" title="Research"></a>Research</h2><p>研究部分得单独列一节来讲讲，这一年下来，经历了几轮的投稿，反复摩擦之后，有挺多感想的，这边就从整个投 paper 的过程来分别谈谈这一年的收获。</p><p><strong>选题</strong>：</p><ol><li>赛道很重要，比较卷的领域，出 paper 真的有挑战。这一年我关注在 Efficient NLP 上，很多公司都在卷这个领域，指标也很明确，说白了就是更快更强。除了刷完指标以外，故事还要能讲的好听，TinyBERT 是很强的 Model 但是最后还是只被 Findings 录用大约就是这个原因，这个领域不是特别的好讲故事。此外，新兴的 topic 容易讲故事则会更容易被接收，但也要思考新兴 topic 是否是本质的问题，有很多昙花一现的 idea，盲目追热点并不可取，还是要守住自己的一亩三分地。对于很卷的领域，我的一个想法是可以<strong>另辟蹊径，不要盯着数字，而去在设定以及进行原理的探究</strong>，或许是提高录用率的办法。</li><li>有趣和有用。我选取的 Efficient NLP 这个领域或许会更加接近有用的研究，但是有用的研究或许是比较枯燥的，考虑和其他同学合作一些自己感兴趣的 topic，是一种增添趣味、增进友谊、增加 publication 的 win-win-win 方案。</li></ol><p><strong>实验</strong>：</p><ol><li><p><strong>请做正确的假设检验</strong>，可以参考这篇<a href="https://aclanthology.org/P18-1128.pdf" target="_blank" rel="noopener">文章</a>，平均数也可能是 misleading，因为结果的分布可能是大幅度的 overlap。</p></li><li><p>对于最终效果而言，数据 &gt;&gt; 对任务的理解 &gt; 模型。这一点的基础是目前大家的Backbone都是 BERT 之流的模型，模型本身的能力已经接近天花板了，数据和任务 inductive bias 的效果比花式叠 blocks 是更加有用的。</p></li></ol><p><strong>写作</strong>：</p><p>论文写作最难的在于 Introduction，而 Introduction 最难的部分在于你需要 sell your paper，特别需要强调两个方面：</p><ol><li>Challenges: 为什么之前的工作没有做好，他们做不好的原因是什么，这里或许就是体现 insight 的地方</li><li>Non-trivial: solution/method 要<strong>不简单</strong>。这里的不简单是指说，换了别人做不了，非你不可。Transformer 核心 idea 很简单，但是其中的设计以及超参的设置并不是随随便便就能做 work 的。内核简单，而外在需要复杂性以体现工作的独到之处，才是一篇好文章。</li></ol><p><strong>合作</strong>：理解自己的 leader 精神，考虑他人的 workload，合理 credit co-author。</p><p><strong>心态</strong>：把 research 当成 lifestyle，该吃吃该喝喝，被拒稿再正常不过，认真对待审稿人的意见，甄别性地加以吸收和利用，*CL 投不中没关系，或许换到 AI、Data Mining 的会就能中了，再不行就投个 workshop。因为研究的时效性，一个 paper 的价值可能是不断在衰减的，我们能做的也就只有准备工作的时候尽可能通过选题、方法延长他的半衰期，完成之后，在价值湮灭之前，为他找到归宿就行。</p><h2 id="Misc"><a href="#Misc" class="headerlink" title="Misc"></a>Misc</h2><ul><li><p>开了微信读书的会员，但因为骑小电驴没法玩手机，碎片阅读时间大幅缩短，正确固定划拉时间努力回本</p></li><li><p>尝试了禁食减肥法，效果一度很好，但是还是忍不住得吃饭，又反弹了</p></li><li>Blog 的产出还是不够，不过 paper 还行（逃</li><li>美股原地踏步，抄底新东方，让子弹再飞一会吧</li></ul><p>新的学年，日拱一卒，坚定向前！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;欢迎大家关注我的公众号&lt;strong&gt;三石杂货铺&lt;/strong&gt;，第一时间收获更新：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/qrcode.png&quot; alt=&quot;QR Code&quot;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;园子里的稚嫩面孔多了起来，熙熙攘攘的校门迎来送往，我也在这个门中
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>TinyBERT 蒸馏速度实现加速小记</title>
    <link href="https://tobiaslee.top/2021/04/16/Improving-Training-Efficiency-of-TinyBERT/"/>
    <id>https://tobiaslee.top/2021/04/16/Improving-Training-Efficiency-of-TinyBERT/</id>
    <published>2021-04-16T06:08:51.000Z</published>
    <updated>2021-04-17T03:37:07.085Z</updated>
    
    <content type="html"><![CDATA[<p>最近做的一个 project 需要复现 EMNLP 2020 Findings 的 <a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT" target="_blank" rel="noopener">TinyBERT</a>，这篇文章就是在复现过程对踩到坑，以及对应的解决方案和实现加速的一个记录。</p><h2 id="Overview-of-TinyBERT"><a href="#Overview-of-TinyBERT" class="headerlink" title="Overview of TinyBERT"></a>Overview of TinyBERT</h2><p>BERT 效果虽好，其较大内存消耗和较长的推理延时会对其上线部署造成一定挑战。内存消耗方面，一系列知识蒸馏的工作，例如 <a href="https://arxiv.org/abs/1910.01108" target="_blank" rel="noopener">DistilBERT</a>、<a href="https://arxiv.org/abs/1908.09355" target="_blank" rel="noopener">BERT-PKD</a> 和 <a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT" target="_blank" rel="noopener">TinyBERT</a> 被提出来来降低模型的参数（主要是层数）以及相应地减少时间；推理加速方面，也有例如 <a href="https://arxiv.org/abs/2004.12993" target="_blank" rel="noopener">DeeBERT</a>、<a href="https://arxiv.org/abs/2004.02178" target="_blank" rel="noopener">FastBERT</a> 以及 <a href="https://arxiv.org/abs/2012.14682" target="_blank" rel="noopener">CascadeBERT</a> 等方案来动态地根据样本难度进行模型的执行从而提升推理效率。其中比较具备代表性便是 TinyBERT，其核心框架如下：</p><p><img src="/img/tinybert.png" alt="TinyBERT"></p><p>分为两个阶段：</p><ul><li>General Distillation：在通用的语料，例如 BookCorpus， EnglishWiki 上进行知识蒸馏，目标函数包括  Transformer Layer Attention 矩阵以及 Layer Hidden States 的对齐；</li><li>Task Distillation：在具体的任务数据集上进行蒸馏，又被进一步分成两个步骤：<ul><li>Task Transformer Disitllation: 在任务数据集上对齐 Student 和已经 fine-tuned  Teacher model 的 attention map 和 hidden states；</li><li>Task Prediction Distillation：在任务数据集上对 student model 和 teacher model 的 output distritbuion 利用 KL loss / MSE loss 进行对齐。</li></ul></li></ul><p>TinyBERT 提供了经过 General Distillation 阶段的 checkpoint，可以认为是一个小的 BERT，包括了 6L786H 版本以及 4L312H 版本。而我们后续的复现就是基于 4L312H v2 版本的。值得注意的是，TinyBERT 对任务数据集进行了数据增强操作，通过基于 Glove 的 Embedding Distance 的相近词替换以及 BERT MLM 预测替换，会将原本的数据集扩增到 20 倍。而我们遇到的第一个 bug 就是在数据增强阶段。</p><h2 id="Bug-in-Data-Augmentation"><a href="#Bug-in-Data-Augmentation" class="headerlink" title="Bug in Data Augmentation"></a>Bug in Data Augmentation</h2><p>我们可以按照官方给出的代码对数据进行增强操作，但是在 QNLI 上会报错：</p><blockquote><p>Index Error: index 514 is out of dimension 1 with size 512</p></blockquote><p>造成数据增强到一半程序就崩溃了，为什么呢？</p><p>很简单，因为数据增强代码 BERT MLM 换词模块对于超长（&gt; 512）的句子没有特殊处理，造成下标越界，具体可以参考 <a href="https://github.com/huawei-noah/Pretrained-Language-Model/issues/50" target="_blank" rel="noopener">#Issue50</a>。</p><p>在对应的函数中进行边界的判断即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_masked_language_model</span><span class="params">(self, sent, word_pieces, mask_id)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mask_id &gt; <span class="number">511</span>: <span class="comment"># if mask id is longer than max length </span></span><br><span class="line">        <span class="keyword">return</span> [] </span><br><span class="line">    tokenized_text = self.tokenizer.tokenize(sent)</span><br><span class="line">    tokenized_text = [<span class="string">'[CLS]'</span>] + tokenized_text</span><br><span class="line">    tokenized_len = len(tokenized_text)</span><br><span class="line">    tokenized_text = word_pieces + [<span class="string">'[SEP]'</span>] + tokenized_text[<span class="number">1</span>:] + [<span class="string">'[SEP]'</span>]</span><br><span class="line">    segments_ids = [<span class="number">0</span>] * (tokenized_len + <span class="number">1</span>) + [<span class="number">1</span>] * (len(tokenized_text) - tokenized_len - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> len(tokenized_text) &gt; <span class="number">512</span>: <span class="comment">#  truncation </span></span><br><span class="line">        tokenized_text = tokenized_text[:<span class="number">512</span>]</span><br><span class="line">        segments_ids = segments_ids[:<span class="number">512</span>]  </span><br><span class="line">    token_ids = self.tokenizer.convert_tokens_to_ids(tokenized_text)</span><br><span class="line">    tokens_tensor = torch.tensor([token_ids]).to(device)</span><br><span class="line">    segments_tensor = torch.tensor([segments_ids]).to(device)</span><br><span class="line">    self.model.to(device)</span><br><span class="line">    predictions = self.model(tokens_tensor, segments_tensor)</span><br><span class="line">    word_candidates = torch.argsort(predictions[<span class="number">0</span>, mask_id], descending=<span class="keyword">True</span>)[:self.M].tolist()</span><br><span class="line">    word_candidates = self.tokenizer.convert_ids_to_tokens(word_candidates)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> list(filter(<span class="keyword">lambda</span> x: x.find(<span class="string">"##"</span>), word_candidates))</span><br></pre></td></tr></table></figure><h2 id="Acceleration-of-Data-Parallel"><a href="#Acceleration-of-Data-Parallel" class="headerlink" title="Acceleration of Data Parallel"></a>Acceleration of Data Parallel</h2><p>当我们<del>费劲</del>愉快地完成数据增强之后，下一步就是要进行 Task Specific 蒸馏里的 Step 1，General Distillation 了。对于一些小数据集像 MRPC，增广 20 倍之后的数据量依旧是 80k 不到，因此训练速度还是很快的，20 轮单卡大概半天也能跑完。但是对于像 MNLI 这样 GLUE 中最大的数据集（390k），20 倍增广后的数据集（增广就花费了大约 2 天时间），如果用单卡训练个 10 轮那可能得跑上半个月了，到时候怕不是黄花菜都凉咯。遂打算用多卡训练，一看，官方的实现就通过 <code>nn.DataParallel</code> 支持了多卡。好嘛，直接 <code>CUDA_VISIBLE_DEVICES=&quot;0,1,2,3&quot;</code> 来上 4 块卡。不跑不知道，加载数据（tokenize, padding ）花费 1小时，好不容易跑起来了，一开 <code>nvidia-smi</code> 吓一跳，GPU 的利用率都在 50% 左右，再一看预估时间，大约 21h 一轮，10 epoch 那四舍五入就是一个半礼拜。好家伙，这我还做不做实验了？这时候就去翻看 PyTorch 文档，发现 PyTorch 现在都不再推荐使用 <code>nn.DataParallel</code> 了，为什么呢？主要原因在于 DataParallel 的实现是单进程的，每次都是有一块主卡读入数据再发给其他卡，这一部分不进带来了额外的计算开销，而且会造成主卡的 GPU 显存占用会显著高于其他卡，进而造成潜在的 batch size 限制；此外，这种模式下，其他 GPU 算完之后要传回主卡进行同步，这一步又会受限于 Python 的线程之间的 GIL（global interpreter lock），进一步降低了效率。此外，还有多机以及模型切片等 DataParallel 不支持，但是另一个 DistributedDataParallel 模块支持的功能。所以，废话少说，得把原先 TinyBERT DataParallel（DP）改成 DistributedDataParallel(DDP)。那么，请问，把 DP 改成 DDP 需要几步？答：大概，就那么多步吧，实现可以参考这个<a href="https://zhuanlan.zhihu.com/p/98535650" target="_blank" rel="noopener">知乎-当代研究生需要掌握的并行训练技巧</a>。核心的代码就是做一下初始化，以及用 DDP 替换掉 DP：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist </span><br><span class="line"></span><br><span class="line"><span class="comment"># 给 parser 增加一个 local rank 参数来在启动的时候传入 rank </span></span><br><span class="line">parser.add_argument(<span class="string">'--local_rank'</span>,</span><br><span class="line">                        type=int,</span><br><span class="line">                        default=<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">logger.info(<span class="string">"Initializing Distributed Environment"</span>)</span><br><span class="line">torch.cuda.set_device(args.local_rank)</span><br><span class="line">dist.init_process_group(backend=<span class="string">"nccl"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 devicec</span></span><br><span class="line">local_rank = args.local_rank</span><br><span class="line">torch.cuda.set_device(local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型 并且 放到 device 上</span></span><br><span class="line">student_model = TinyBertForSequenceClassification.from_pretrained(args.student_model, num_labels=num_labels).to(device)    </span><br><span class="line">teacher_model = TinyBertForSequenceClassification.from_pretrained(args.teacher_model, num_labels=num_labels).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用 DDP 包裹模型</span></span><br><span class="line">student_model = DDP(student_model, device_ids=[local_rank], output_device=local_rank)</span><br><span class="line">teacher_model = DDP(teacher_model, device_ids=[local_rank], output_device=local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ..</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用 DistributedSampler 替换原来的 Random Sampler</span></span><br><span class="line">train_sampler = torch.utils.data.DistributedSampler(train_data)</span><br></pre></td></tr></table></figure><p>然后，大功告成，一键启动:</p><blockquote><p>GPU=”0,1,2,3”</p><p>CUDA_VISIBLE_DEVICEES=$GPU python -m torch.distributed.launch  –n_proc_per_node 4 task_disti.py </p></blockquote><p>启动成功了吗？模型又开始处理数据….</p><p>One hours later，机器突然卡住，程序的 log 也停了，打开 htop 一看，好家伙，256G 的内存都满了，程序都是 D 状态，咋回事？</p><h2 id="Acceleration-of-Data-Loading"><a href="#Acceleration-of-Data-Loading" class="headerlink" title="Acceleration of Data Loading"></a>Acceleration of Data Loading</h2><p>我先试了少量数据，降采样到 10k，程序运行没问题， DDP 速度很快；我再尝试了单卡加载，虽然又 load 了一个小时，但是 ok，程序还是能跑起来，那么，问题是如何发生的呢？单卡的时候我看了一眼加载全量数据完毕之后的内存占用，大约在 60G 左右，考虑到 DDP 是多进程的，因此，每个进程都要独立地加载数据，4 块卡 4个进程，大约就是 250 G 的内存，因此内存爆炸，到后面数据的 io 就卡住了（没法从磁盘 load 到内存），所以造成了程序 D 状态。看了下组里的机器，最大的也就是 250 G 内存，也就是说，如果我只用 3 块卡，那么是能够跑的，但是万一有别的同学上来开程序吃了一部分内存，那么就很可能爆内存，然后就是大家的程序都同归于尽的局面，不太妙。一种不太优雅的解决方案就是，把数据切块，然后读完一小块训练完，再读下一块，再训练，再读。咨询了一下组里资深的师兄，还有一种办法就是实现一种<strong>把数据存在磁盘上，每次要用的时候才 load 到内存的</strong>数据读取方案，这样就能够避免爆内存的问题。行吧，那就干吧，但是总不能从头造轮子吧？脸折师兄提到 huggingface(yyds) 的 <a href="https://huggingface.co/datasets" target="_blank" rel="noopener">datasets</a> 能够支持这个功能，check 了一下文档，发现他是基于 pyarrow 的实现了一个 memory map 的数据<a href="https://github.com/huggingface/datasets/blob/fcd3c3c8e3b1d9a2f3686a496082e21f06591380/src/datasets/table.py#L400" target="_blank" rel="noopener">读取</a>，以我的 huggingface transformers 的经验，似乎是能够实现这个功能的，所以摩拳擦掌，准备动手。</p><p>首先，要把增广的数据 load 进来，datasets 提供的 <code>load_dataset</code> 函数最接近的就是 <code>load_dataset(&#39;csv&#39;, data_file)</code>，然后我们就可以逐个 column 的拿到数据并且进行预处理了。写了一会，发现总是报读取一部分数据后 columns 数目不对的错误，猜测可能原始 MNLI 数据集就不太能保证每个列都是在的，检查了一下 <code>MnliProcessor</code> 里处理的代码，发现其写死了 <code>line[8]</code> 和 <code>line[9]</code> 作为 sentence_a 和 sentence_b。无奈之下，只能采取最粗暴地方式，用 <code>text</code> mode 读进来，每一行是一个数据，再 split：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> </span><br><span class="line"></span><br><span class="line">processor = processors[task_name]()</span><br><span class="line">output_mode = output_modes[task_name]</span><br><span class="line">label_list = processor.get_labels()</span><br><span class="line">num_labels = len(label_list)</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(args.student_model, do_lower_case=args.do_lower_case)</span><br><span class="line"><span class="comment"># 用 text</span></span><br><span class="line">mnli_datasets = load_dataset(<span class="string">"text"</span>, data_files=os.path.join(args.data_dir, <span class="string">"train_aug.tsv"</span>))</span><br><span class="line">label_classes = processor.get_labels()</span><br><span class="line">label_map = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(label_classes)&#125;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">preprocess_func</span><span class="params">(examples, max_seq_length=args.max_seq_length)</span>:</span></span><br><span class="line">            splits = [e.split(<span class="string">'\t'</span>) <span class="keyword">for</span> e <span class="keyword">in</span> examples[<span class="string">'text'</span>]] <span class="comment"># split</span></span><br><span class="line">            <span class="comment"># tokenize for sent1 &amp; sent2</span></span><br><span class="line">            tokens_s1 = [tokenizer.tokenize(e[<span class="number">8</span>]) <span class="keyword">for</span> e <span class="keyword">in</span> splits] </span><br><span class="line">            tokens_s2 = [tokenizer.tokenize(e[<span class="number">9</span>]) <span class="keyword">for</span> e <span class="keyword">in</span> splits]</span><br><span class="line">            <span class="keyword">for</span> t1, t2 <span class="keyword">in</span> zip(tokens_s1, tokens_s2):</span><br><span class="line">                truncate_seq_pair(t1, t2, max_length=max_seq_length - <span class="number">3</span>)</span><br><span class="line">            input_ids_list = []</span><br><span class="line">            input_mask_list = []</span><br><span class="line">            segment_ids_list = []</span><br><span class="line">            seq_length_list = []</span><br><span class="line">            labels_list = []</span><br><span class="line">            labels = [e[<span class="number">-1</span>] <span class="keyword">for</span> e <span class="keyword">in</span> splits] <span class="comment"># last column is label column </span></span><br><span class="line">            <span class="keyword">for</span> token_a, token_b, l <span class="keyword">in</span> zip(tokens_s1, tokens_s2, labels):  <span class="comment"># zip(tokens_as, tokens_bs):</span></span><br><span class="line">                tokens = [<span class="string">"[CLS]"</span>] + token_a + [<span class="string">"[SEP]"</span>]</span><br><span class="line">                segment_ids = [<span class="number">0</span>] * len(tokens)</span><br><span class="line">                tokens += token_b + [<span class="string">"[SEP]"</span>]</span><br><span class="line">                segment_ids += [<span class="number">1</span>] * (len(token_b) + <span class="number">1</span>)</span><br><span class="line">                input_ids = tokenizer.convert_tokens_to_ids(tokens) <span class="comment"># tokenize to id </span></span><br><span class="line">                input_mask = [<span class="number">1</span>] * len(input_ids)</span><br><span class="line">                seq_length = len(input_ids)</span><br><span class="line">                padding = [<span class="number">0</span>] * (max_seq_length - len(input_ids))</span><br><span class="line">                input_ids += padding</span><br><span class="line">                input_mask += padding</span><br><span class="line">                segment_ids += padding</span><br><span class="line">                <span class="keyword">assert</span> len(input_ids) == max_seq_length</span><br><span class="line">                <span class="keyword">assert</span> len(input_mask) == max_seq_length</span><br><span class="line">                <span class="keyword">assert</span> len(segment_ids) == max_seq_length</span><br><span class="line">                input_ids_list.append(input_ids)</span><br><span class="line">                input_mask_list.append(input_mask)</span><br><span class="line">                segment_ids_list.append(segment_ids)</span><br><span class="line">                seq_length_list.append(seq_length)</span><br><span class="line">                labels_list.append(label_map[l])</span><br><span class="line"></span><br><span class="line">            results = &#123;<span class="string">"input_ids"</span>: input_ids_list,</span><br><span class="line">                       <span class="string">"input_mask"</span>: input_mask_list,</span><br><span class="line">                       <span class="string">"segment_ids"</span>: segment_ids_list,</span><br><span class="line">                       <span class="string">"seq_length"</span>: seq_length_list,</span><br><span class="line">                       <span class="string">"label_ids"</span>: labels_list&#125;</span><br><span class="line">            <span class="keyword">return</span> results</span><br><span class="line"><span class="comment"># map datasets</span></span><br><span class="line">mnli_datasets = mnli_datasets.map(preprocess_func, batched=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># remove column</span></span><br><span class="line">train_data = mnli_datasets[<span class="string">'train'</span>].remove_columns(<span class="string">'text'</span>)</span><br></pre></td></tr></table></figure><p>写完这个 <code>preprocess_func</code> ，我觉得胜利在望，但还有几个小坑需要解决s：</p><ul><li><p>map 完之后，返回的还是一个 DatasetDict，得手动取一下 <code>train</code> set；</p></li><li><p>对于原先存在的列，map 函数并不会去除掉，所以如果不用的列，需要手动 <code>.remove_columns()</code> </p></li><li><p>在配合 DDP 使用的时候，因为 DistributedSample 取数据的维度是在第一维取的，所以取到的数据可能是个 seq_len 长的列表，里面的 tensor 是 [bsz]  形状的，需要在交给 model 之前 stack 一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items():</span><br><span class="line">    <span class="keyword">if</span> isinstance(v, torch.Tensor):</span><br><span class="line">        inputs[k] = v.to(device)</span><br><span class="line">    <span class="keyword">elif</span> isinstance(v, List):</span><br><span class="line">        inputs[k] = torch.stack(v, dim=<span class="number">1</span>).to(device)</span><br></pre></td></tr></table></figure></li></ul><p>至此，只要把之前代码的 train_data 都换成现在的版本即可。</p><p>此外，为了进一步加速，我还把混合精度也整合了进来，现在 Pytorch 以及自带对混合精度的支持，代码量也很少，但是有个坑就是<strong>loss 的计算必须被 auto() 包裹住</strong>，同时，所有模型的输出都要参与到 loss 的计算，这对于只做 prediction 或者是 hidden state 对齐的 loss 很不友好，所以只能手动再额外计算一项为系数为 0 的 loss 项（这样他参与到训练但是不会影响梯度）。</p><h2 id="Finally"><a href="#Finally" class="headerlink" title="Finally"></a>Finally</h2><p>最后，改版过的代码在我的 GitHub <a href="https://github.com/TobiasLee/Pretrained-Language-Model/blob/master/TinyBERT/fast_td.py" target="_blank" rel="noopener">fork</a> 版本中，我不要脸地起名为 <code>fast_td</code> 。实际上，改版后的有点有一下几个：</p><ul><li>数据加载方面，第一次加载/处理 780w 大约耗时 50m，但是不会多卡都消耗内存，实际占用不到 2G；同时，得益于 datasets 的支持，后续加载不会重复处理数据而是直接读取之前的 cache；</li><li>模型训练方面，得益于 DDP 和 混合精度，在 MNLI 上训增强数据 10 轮，3 块卡花费的时间大约在 20h 左右，提速了 10 倍。</li></ul><p>这次修改代码大概花了 2 天时间来实现和 debug，不过感觉收益还是挺大的，此处需要感谢任大佬 &amp; 脸折师兄的建议，以及 andy 提供的知乎文章，撒花~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近做的一个 project 需要复现 EMNLP 2020 Findings 的 &lt;a href=&quot;https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT&quot; target=
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="PyTorch" scheme="https://tobiaslee.top/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Goodbye，2020！</title>
    <link href="https://tobiaslee.top/2021/02/04/Goodbye-2020/"/>
    <id>https://tobiaslee.top/2021/02/04/Goodbye-2020/</id>
    <published>2021-02-04T15:47:17.000Z</published>
    <updated>2021-02-04T09:10:07.569Z</updated>
    
    <content type="html"><![CDATA[<p>前一直没写年度总结很重要的一个原因也是因为觉得，手头的文章没投出去，还是不能算是一个完整的年度。经历了 softconf 爆炸 ACL 的 ddl 延期一天之后，终于，ACL 结束了，也算是给2020年画上一个句号。收拾收拾心情，准备写今年的年度总结。</p><h2 id="PKU"><a href="#PKU" class="headerlink" title="PKU"></a>PKU</h2><p>在 P 大度过了第一个学期，一开始还是蛮新鲜的，老老实实地选课上课，后来就开始油起来，开始选择性上课。第一学期选的课不多，总体上的压力不大，最后也都顺利摸过去了。当初选课的时候 Dr. Xiang 就说<strong>对于研究生来说，选课不是为了去学习这门课的内容，而是为了拿到对应的学分</strong>，真正的功夫还是在课堂以外的，自己去看 paper 和 coding，一学期下来我觉得还是很有道理的。不过这学期选的计算语言学课我觉得还是蛮不错的，第一次 Pre 大家展示的东西让我有一种我在某个会议 Workshop 的错觉，看别人做的东西有些还是蛮有 insight 的，果然 dalao 们都是不鸣则已一鸣惊人的。下学期的选课应该会延续这个思路，<del>能划则划</del>天天向上。</p><p>除了上课，我大部分的时间都还是在所里，过着早上九点班车——鸡蛋饼永远滴神——所里打卡的生活。在所里打交道最多的除了师兄师姐们以外，就是同一级的 daolao 们，介绍如下：</p><p><strong>Andy </strong>：不和没发过 ACL 的人做朋友——信科被取消晚会<del>主持人</del>倒霉蛋——中午必回寝室午睡的男人；</p><p><strong>Juicy</strong>：知名<del>开合跳</del>千手观音表演艺术家，后踢腿踢不到自己屁股的 Keep 达人，鸡蛋饼永远滴神教拥趸；</p><p><strong>Keven</strong>：Backdoor Attack 专家，数学证明达人，乒乓x篮球双修选手，九点班车忠实爱好者；</p><p><strong>GRD</strong>：“处在崩溃边缘”作者；</p><p><strong>Peter</strong>：运动健将，出现的地点包括但不限于邱德拔游泳馆篮球场健身房以及，最大概率在所里；</p><p><strong>William</strong>：可能是语言所歌王（x</p><p><strong>Windy</strong>：目前所里我们唯一的，非常勤奋的师妹。</p><p>我想，研究生的日子呆在一起最多的就是这些可爱的朋友们了，希望大家新的一年都能够身体健康，Paper 多多，快快乐乐地度过或三年或五年的时光。</p><h2 id="Research"><a href="#Research" class="headerlink" title="Research"></a>Research</h2><p>去年这个时候，我虽然大致知道研究生做研究大致是什么样的，但是对于具体要做什么，以及怎么做，还是没有一个比较清楚的认知的。也一度因此，陷入比较焦虑的状态。和本科的时候在师兄的指导下去做一些尝试不一样，研究生阶段，可以认为每个人都是自己的 paper/project 的 leader，要考虑到很多事情，这学期也在这方面不断的探索，总结如下：</p><ul><li><strong>选题</strong>：对于接触不多的同学来说，这个可能反倒会轻松一些，可以挑选组里有沉淀的方向以及和高年级师兄师姐们一起开始去挖坑填坑都是不错的选择；之前做过一些生成的工作，起初我也沿着生成去调研了一部分觉得可能蛮有意思的问题，但总的来说觉得生成方向的坑有一些自娱自乐的感觉，而一些 long standing problem 也没有比较好的解决思路，遂决定转战别处。学期初和老板讨论的时候他指出要做一些和时代大趋势挂钩的研究，那么显然就是预训练模型，但是直接拿来做应用似乎也比较 superficial。最后兜兜转转，敲定决定在压缩、加速等方面做一些研究，一方面在随着模型越来越大的同时大家开始关注其效率的问题，我对于让预训练模型在尽可能小、推理速度尽可能快的情况下，保持性能这一目标还是很有兴趣的；另外这个对于实际落地也可能也能够带来一定的帮助，对于之后可能很快就会进入就业市场的我也有一定帮助。总之，选题方面要结合个人的兴趣以及多和老板师兄师姐们沟通，尽可能<strong>找到个人趣味、实验室沉淀以及落地价值之间比较好的平衡点</strong>。</li><li><strong>实验</strong>：这块可能是整个 paper 里面看起来最重要但是又不那么重要的一环。但是有一些做好一些小细节有利于加快迭代速度：<ul><li>选一个好的框架，例如 transformers / fairseq 之类的框架提供了非常好的支持，深入理解框架并且能够灵活定制有助于实验的快速推进；</li><li>快速验证：这个是这学期吃的一个亏让我意识到，实验不是一开始就越fancy越好，有一些验证idea的实验一定要尽早做，这样就可以省下之后很多走弯路的功夫；</li><li>调试：Log 很重要，以及对于关键语句是否执行的判断在 debug 初期也是很重要的，剩下的就是看 Google 的熟练运用程度了；</li></ul></li><li><strong>写作</strong>：多看多写，写作前翻阅刘洋老师的论文写作技巧；自己多读几遍，再找各种同学来读，接受他们的建议；时刻牢记 reader 和 writer 之间的 gap，be humble；</li><li><strong>合作</strong>：现在的 paper 很少有单枪匹马完成的，大多是团队合作的成果。不过我觉得在合作过程之中作为 leader 还是要有着一个主人翁的精神，尽可能地把合作者当成一个判别器，而不是视作生成器依赖他们来<em>生成</em>解决方案，那对合作者的要求会很高。例如，在和高年级师兄师姐合作的时候，他们也有着很多的任务，不应该依赖于他们来直接给出解决方案，遇到问题之后可以一起分析，分析完了自己尝试想想有没有可能的 solution，再和师兄师姐们去讨论，借助他们的经验来判断可行性。</li><li><strong>心态</strong>：可能前面一些方面的感想都能够比较直接地体现出来，但是心态上，我觉得真的做研究，急不来。之前也讨论过卷的问题，我觉得一个比较好的境界是<strong>外卷内松</strong>，即外在来看，保证工作时间到位，思考的时间充足，迭代的实验充足；内心则对于结果不加过强的预设，能够平静看待最后的产出。用比较 fashion 的话说的话，就是<strong>因上努力，果上随缘</strong>。我这学期一开始的时候就非常焦虑，迫切想要产出文章。但是越着急，并不能带来正面的结果，反倒是会在一些无谓的事情耗费精力。我干过的蠢事就是用两天写了一个的短文准备投 NAACL，首先是 idea 方面并没有任何的 novelty，以及写作因为是 rush 的 paper 也存在很大的问题。最后在摇了一个号之后我还是 withdraw 了这个 paper，因为最终还是没能过我自己那关。对于投出去的 paper，还是希望过了心中的 bar 再投，不然只是为  community 徒增审稿负担罢辽。</li></ul><p>最后，要感谢这一学期一来给了我很多帮助的 lurker、deli、guangxiang、yankai、yige、zhenghua 等师兄师姐们，希望师兄师姐们最终都能够有满意的去处。</p><h2 id="Misc"><a href="#Misc" class="headerlink" title="Misc"></a>Misc</h2><p>下面是一些不成体系的胡言乱语：</p><ul><li>杂七杂八地读了不少书，估摸着应该达到预期的阅读量；</li><li><p>美股取得正收益，获得了翻两倍的股票若干，另外基金收益也不错，不过新的一年应该降低期望；</p></li><li><p>拿到院队队服，希望明年能在北大杯上和信科一起冲击冠军；</p></li><li>体重保持恒定，在一个能接受的范围内波动，新年的 flag 还是要控制体重；</li><li>Blog 的文章产出没有达到预期，学术风的 blog 似乎有点走偏了，寒假期间应该会努力产出一些有趣的 paper 解读文章；</li><li>组建奶茶群，也不知道拿了多少次奶茶（去拿奶茶的路上消耗热量所以喝奶茶就没热量了！）</li></ul><p>新的一年，保持好奇，内心平静，慢慢成长~</p><p>提前祝大家新年快乐！！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前一直没写年度总结很重要的一个原因也是因为觉得，手头的文章没投出去，还是不能算是一个完整的年度。经历了 softconf 爆炸 ACL 的 ddl 延期一天之后，终于，ACL 结束了，也算是给2020年画上一个句号。收拾收拾心情，准备写今年的年度总结。&lt;/p&gt;
&lt;h2 id
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>当我们面对卷的时候，我们该做些什么？</title>
    <link href="https://tobiaslee.top/2020/11/08/inner-peace/"/>
    <id>https://tobiaslee.top/2020/11/08/inner-peace/</id>
    <published>2020-11-08T05:49:15.000Z</published>
    <updated>2020-11-08T15:48:26.315Z</updated>
    
    <content type="html"><![CDATA[<p>燕园的叶子又黄了，落了，距离秃了约莫还有半个月的光景吧。去年这个时候的我，看着满地萧瑟的时候，充斥着的即将成为这里一份子的激动，因而反倒是并不觉得心情低落。今年的这个时候，已是园中人的自己，却因为一次又一次的实验失败，以及受到内卷气氛的影响，开始感到了焦虑。一般这种时候，我的疗法就是给自己猛灌鸡汤，再打打鸡血，写篇<em>战斗檄文</em>，第二天就又斗志昂扬了。今天就来故技重施，看看能不能提振一下士气。</p><h2 id="卷和躺平"><a href="#卷和躺平" class="headerlink" title="卷和躺平"></a>卷和躺平</h2><p>内卷这个词不知道是什么时候突然兴起的，指的就是大家都削减了脑袋学习、工作，以获得某些资源上的优势。带来的后果就是，你加班到 8 点，别人为了比你更勤奋就加班到 9 点，进入一个恶性的循环，最后，谁也别走了。老板自然是乐得见到自己的员工拼命加班，只是同为打工人，何必难为打工人？躺平则就是另外一个角度了，能摸鱼的时候就尽可能摸鱼，6 点下班绝对不 6 点 1 分走，完成交代的任务然后，顺其自然。</p><p>巴菲特说，人生像是滚雪球，找到湿的雪和和很长的坡道，雪球就会越滚越大。我觉得他漏了一点，就是初始的雪球的大小。总的来说，现在坡道上的雪已经不多，如果一开始的那个小雪球很小，那么，滚雪球前半段中的消耗是很有可能大于积累的，滚着滚着，雪球不但不会变大，反倒会变小。以我的经历来说，选择坡道，和专业、职业的选择是有一些对应的，例如，程序员这条坡道前段的雪还挺多，但是后段的雪就不那么多了；公务员则是国家负责给你的坡道上铺上一层不薄也不厚的雪，能不能滚的很大，初始资本的积累就显得尤为重要。</p><p>那么我是哪一派的人呢？我的理念是：<strong>该卷的时候，我努力卷；能不卷的时候，或者说，卷的意义不大的时候，就不卷</strong>。学校里的氛围，相对来说氛围其实是比较和谐的，没有那么多的乌七八糟，而在这个时候，卷的收益还是蛮大的，实际上你可以认为就是在搓一开始的那个小雪球，如果你能够在开始滚雪球的时候，半径比别人大上那么一小圈，那么你后面积累的速度也就会上去，并且如果在同一个坡道上的话，你会越滚越快，别人只能在你后面吃土，而你，赢家通吃。所以，在学校里，适当地卷一些，在我看来，收益是很大的。当然，我强烈建议已经不愁吃喝的朋友们，直接躺平，多去创造一些精神财富留给后代。卷的意义不大的时候是指，你加班到 9 点，实际上是从 6 点摸鱼到 9 点，可能抽了那么 15 min 做了点工作；或者是著名类似学堂路上边骑车边 Linux 这种，前者无效率，后者有可能造成巨大安全上的后果，我都是坚决反对的。</p><h2 id="挫商"><a href="#挫商" class="headerlink" title="挫商"></a>挫商</h2><p>之前谈到过一个观点，做科研的奖赏函数，是非常稀疏的，可能早些年会 dense 一些。我最近这个 6 个月，对此是深有体会，基本上没有收到什么正向的激励，很是有挫败感，开始逐渐怀疑自己。我觉得这个感觉其实有这几方面的原因：</p><ul><li>信息的传播速度变快了：经常看各大公众号推拳打脚踢，天才少年，被它们的这些文章带的很焦虑；同时，pyq 里面也有越来越多的从业者，我数了下，朋友圈里面大大小小 best paper 得主的数量两只手都快数不过来了，看他们发 paper accepted 的内容，更焦虑了；</li><li>Taste 更挑剔了，实验难度更大了：为什么实验不太成功呢？因为现在做的工作的难度相对于之前会更有挑战性，一方面这是大势所趋，简单的应用新方法到旧问题（A+B）模式的 paper 被接收的概率大大降低；另外一方面，也是做科研的 taste 逐渐提高了，可能有些想法依旧不是特别的成熟，但那些非常 straightforward 的 idea 大多过不了自己这关，因而也就不会有特别容易做的实验；</li><li>正式成为了研究生，肩上会有了研究任务以及一个明确的期限：有师兄师姐在研二上如果还没有发出 paper，就会特别焦虑，也是这么个原因，因为马上就要找工作所以随着秋招春招的迫近，紧迫感随之而来；而入学之后，也会有导师分配的项目，同样也有结题的压力的因素。</li></ul><p>综上，在正式开始研究生生涯不久，我就感觉到了精神上的疲惫；身体上倒还好，或许是得益于没事就去踢野球蹭友谊赛（PS：昨天友谊赛还制造了点球并且罚进了哈哈哈）以及还算合理的作息，头发啥的还没开始掉，有望保持到35岁。我能想到的解决这种焦虑的方法，其实也就是少和别人比较，多往前看看，<strong>做好每天的工作笔记</strong>，方便在点滴之间回顾一些小的结果；多和亲朋好友沟通，交流感情；以及，适当地进行一些放松，毕竟<strong>坡道很长</strong>，人生也很长；吃点好的，现在开始很认同一句话：<strong>垃圾食品是人类性后的救赎</strong>，偶尔放纵一下，感受可乐入口时的刺激与爽快，真的很解压。</p><h2 id="明天会更好"><a href="#明天会更好" class="headerlink" title="明天会更好"></a>明天会更好</h2><p>我觉得不单单是我，也许有很多同学朋友也都会面临这样的烦恼，希望这些不成体系的废话能够博君一笑，若是有所帮助，那自然是再好不过了。</p><p>衷心希望，每个研究生都能永葆头发，科研顺利，明天又是新的一天！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;燕园的叶子又黄了，落了，距离秃了约莫还有半个月的光景吧。去年这个时候的我，看着满地萧瑟的时候，充斥着的即将成为这里一份子的激动，因而反倒是并不觉得心情低落。今年的这个时候，已是园中人的自己，却因为一次又一次的实验失败，以及受到内卷气氛的影响，开始感到了焦虑。一般这种时候，我
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Contrastive Learning 学习笔记</title>
    <link href="https://tobiaslee.top/2020/05/18/contrastive-learning-notes/"/>
    <id>https://tobiaslee.top/2020/05/18/contrastive-learning-notes/</id>
    <published>2020-05-18T13:47:29.000Z</published>
    <updated>2020-05-18T03:35:41.840Z</updated>
    
    <content type="html"><![CDATA[<p>最近深度学习两巨头 Bengio 和  LeCun 在 ICLR 2020 上点名 Self-Supervised Learning（SSL，自监督学习） 是 AI 的未来，而其的代表的 Framework 便是 Contrastive Learning（CL，对比学习）。 另一巨头 Hinton 和 Kaiming 两尊大神也在这问题上隔空过招，MoCo、SimCLR、MoCo V2 打得火热，这和 BERT 之后，各大公司出 XL-Net、RoBerta 刷榜的场景何其相似。本篇文章，将会从对比学习的背后的直觉原理出发，介绍其框架，并且对目前的一些相关的工作进行简要介绍，希望能够为感兴趣的同学提供一些帮助。</p><h2 id="Motivation-amp-Framework"><a href="#Motivation-amp-Framework" class="headerlink" title="Motivation &amp; Framework"></a>Motivation &amp; Framework</h2><p>很多研究者认为，深度学习的本质就是做两件事情：Representation Learning（表示学习） 和 Inductive Bias Learning（归纳偏好学习）。目前的一个趋势就是，学好了样本的表示，在一些不涉及逻辑、推理等的问题上，例如判断句子的情感极性、识别图像中有哪些东西，AI 系统都可以完成非常不错；而涉及到更高层的语义、组合逻辑，则需要设计一些过程来辅助 AI 系统去分解复杂的任务，<a href="http://arxiv.org/abs/1904.12584" target="_blank" rel="noopener">ICLR 19</a> 的一篇 oral 就是做的类似的事情。因为归纳偏好的设计更多的是 任务相关的，复杂的过程需要非常精心的设计，所以很多工作都开始关注到表示学习上，NLP 最近大火的预训练模型，例如 BERT，就是利用大规模的语料预训练得到文本的好的表示。那么，CV 领域的 BERT 是什么呢？答案已经呼之欲出，就是对比学习。</p><h3 id="Illustrative-Example"><a href="#Illustrative-Example" class="headerlink" title="Illustrative Example"></a>Illustrative Example</h3><p><img src="/img/dollar.png" alt="Illustration Example"></p><p><em>当你被要求画一张美元，左边是没有钞票在你面前，右边是面前摆着一张钞票画出来的结果</em></p><p>上面这个例子来自于 <a href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html" target="_blank" rel="noopener">Contrastive Self-supervised Learning</a> 这篇 Blog，表达的一个核心思想就是：尽管我们已经见过很多次钞票长什么样子，但我们很少能一模一样的画出钞票；虽然我们画不出栩栩如生的钞票，但我们依旧可以轻易地辨别出钞票。基于此，也就意味着<strong>表示学习算法并不一定要关注到样本的每一个细节，只要学到的特征能够使其和其他样本区别开来就行</strong>，这就是对比学习和对抗生成网络（GAN）的一个主要不同所在。</p><a id="more"></a><h3 id="Contrastive-Learning-Framework"><a href="#Contrastive-Learning-Framework" class="headerlink" title="Contrastive Learning Framework"></a>Contrastive Learning Framework</h3><p>既然是表示学习，那么我们的核心就是要学习一个映射函数 $f$，把样本 $x$ 编码成其表示 $f(x)$，对比学习的核心就是使得这个 $f$ 满足下面这个式子：</p><p>$$s ( f(x), f(x^+)) &gt;&gt; s ( f(x), f(x^-))$$</p><p>这里的 $x^+$ 就是和 $x$ 类似的样本，$x^-$ 就是和 $x$ 不相似的样本，$s(\cdot,\cdot)$ 这是一个度量样本之间相似程度的函数，一个比较典型的 score 函数就是就是向量内积，即优化下面这一期望：</p><p>$$ \underset{x, x^{+}, x^{-}}{\mathbb{E}}\left[-\log \left(\frac{e^{f(x)^{T} f\left(x^{+}\right)}}{e^{f(x)^{T} f\left(x^{+}\right)}+e^{f(x)^{T} f\left(x^{-}\right)}}\right)\right]$$</p><p>如果对于一个 $x$，我们有 $1$ 个正例和 $N-1$ 个负例，那么这个 loss 就可以看做是一个 N 分类问题，实际上就是一个交叉熵，而这个函数在对比学习的文章中被称之为 InfoNCE。事实上，最小化这一 loss 能够最大化 $f(x)$ 和 $f(x^+)$ 互信息的下界，让二者的表示更为接近。理解了这个式子其实就理解了整个对比学习的框架，后续研究的核心往往就聚焦于这个式子的两个方面：</p><ul><li>如何定义目标函数？最简单的一种就是上面提到的内积函数，另外一中 triplet 的形式就是 $l = max(0, η + s (x, x^+) − s (x, x^−))$ ，直观上理解，就是希望正例 pair 和负例 pair 隔开至少 $\eta$ 的距离，这一函数同样可以写成另外一种形式，让正例 pair 和负例 pair 采用不同的 $s$ 函数，例如，$s(x, x^+) = | \max (0, f(x)-f(x^+)|$ ，$s(x, x^+) = | \max (\eta, f(x)-f(x^-)|$。</li><li>如何构建正例和负例？针对不同类型数据，例如图像、文本和音频，如何合理的定义哪些样本应该被视作是 $x^+$，哪些该被视作是 $x^-$，；如何增加负例样本的数量，也就是上面式子里的 $N$？这个问题是目前很多 paper 关注的一个方向，因为虽然自监督的数据有很多，但是<strong>设计出合理的正例和负例 pair，并且尽可能提升 pair 能够 cover 的 semantic relation，才能让得到的表示在 downstream task 表现的更好</strong>。</li></ul><p>接下来，就会介绍一下 MoCo、SimCLR 以及 Contrasitve Predictive Coding（CPC） 这三篇文章，在构建对比样例中的一些核心观点。</p><h2 id="Contrastive-Pair"><a href="#Contrastive-Pair" class="headerlink" title="Contrastive Pair"></a>Contrastive Pair</h2><h3 id="MoCo"><a href="#MoCo" class="headerlink" title="MoCo"></a>MoCo</h3><p><img src="/img/moco.png" alt="MoCo"></p><p>前面提到了，样本数量对于学习到的样本质量有很大的影响。<a href="http://arxiv.org/abs/1911.05722" target="_blank" rel="noopener">MoCo</a> 做的事情很简单，就是把负例样本的 encoder $f(\cdot)$和 mini-batch 大小解耦。也就是说，原先在算目标函数的时候，负例样本对也会为 loss 产生贡献，因为也就会有梯度回传给对应的 encoder，那么这样在实现的时候，样本数量必然会受到 batch size 的限制，从而影响学习到表示的质量。为此，Memory Bank 提出我把所有样本的表示都存起来，然后每次随机采样，这样就可以认为我的负例样本理论上可以达到所有样本的数量，具体的做法就是每一轮来 encode 一次所有的变量，显然，这样很吃内存，并且得到的表示也和参数更新存在一定的滞后。MoCo 则改善了上述的两个缺点，一方面，用一个 queue 来维护当前的 negative candidates pool，queue 有着进出的动态更新机制，一方面能够和 Mini-batch 解耦，queue size 可以设置的比较大，另外一方面也就不用对所有样本做类似预处理的进行编码；对于负例样本的参数，采用 Momentum update 的方式，来把正例 encoder 的参数$\theta_q$ copy 给负例 encoder $\theta_k$：</p><p>$$\theta_k = m \theta_k + (1-m) \theta_q$$</p><p>三种方式的示意图也在这一小节的开头给出了，可以清楚的看到三种方式的区别。<strong>这种对比画图的方式对于说明问题很有帮助，可以在论文中进行尝试</strong>。</p><h3 id="SimCLR"><a href="#SimCLR" class="headerlink" title="SimCLR"></a>SimCLR</h3><p>MoCo 刚说完样本数量对于对比学习很重要，这边 SimCLR 就从另外一个角度，说构建负例的方式（图像上，就是对于图像的 transformation）也很重要，探究了 transformation 、batch-size 大小等对于学习到的表示的影响，并且把这个框架用下面这张图来说明：</p><p><img src="/img/simclr_framework.png" alt="SimCLR-Framework"></p><p>文章主要得出了下面几个结论：</p><ul><li>对于样本进行变化，即构建正例和负例的 transformation 对于结果至关重要；</li><li>用 entropy loss 的 Contrastive Learning，可以通过 normalize representation embedding 以及 temperature adjustment 提点；</li><li>在计算 loss 之前，让表示再过一个 non-linear hard 能大幅提升效果，即上面框架图中的 $g(\cdot)$；</li><li>大 batch-size 对于 CL 的增益比 Supervised Learning 更大。</li></ul><p>其中最后一个结论，和 MoCo 的初衷是符合的，并且作者虽说不用 Memory-bank，但是 SimCLR 尝试的 bsz 也达到了令人发指的 8192，用了 128 块 TPU，又是算力党的一大胜利。MoCo v2 也是利用了上面的第一点和第三点，在 MoCo 基础上得到了进一步的提升，然后作者还也明确的点名了 SimCLR，称不需要使用那么大的 batch size 也能超过它，可能这就是神仙打架吧。</p><h3 id="CPC"><a href="#CPC" class="headerlink" title="CPC"></a>CPC</h3><p>前面讨论的两篇文章主要集中在图像数据上，那么对于文本、音频这样的数据，常见的裁剪、旋转等变换操作就无法适用了，并且，因为其数据本身的时序性，设计合理的方法来把这一点考虑进去是至关重要的。Contrastive Predictive Coding（CPC） 这篇文章就提出，可以利用一定窗口内的 $x_t$ 和 $x_{t+k}$ 作为 Positive pair，并从输入序列之中随机采样一个输入 $x_{t*}$ 作为负例，下图说明了 CPC 的工作过程：</p><p><img src="/img/cpc.png" alt="CPC"></p><p>为了把历史的信息也加入进去，作者提出可以在 $z_t$ 上额外增加一个自递归模型，例如 GRU，来在表示之中融入时序关系，得到相应的 $c_t$ 来进行对比学习。在下游任务中，既可以使用 $z_t$ 也可以使用 $c_t$ ，又或者是二者的融合，可以根据任务需要来进行灵活的选择。</p><h2 id="Theory-amp-Application"><a href="#Theory-amp-Application" class="headerlink" title="Theory &amp; Application"></a>Theory &amp; Application</h2><p>接下来，会简要的讨论几篇关于对比学习的理论和应用类的文章：</p><h3 id="A-theoretical-analysis-of-contrastive-unsupervised-representation-learning"><a href="#A-theoretical-analysis-of-contrastive-unsupervised-representation-learning" class="headerlink" title="A theoretical analysis of contrastive unsupervised representation learning"></a>A theoretical analysis of contrastive unsupervised representation learning</h3><p>这篇文章发表在 ICML 2019 上，对比学习这一框架虽然在直觉上非常 make sense，但是理论上为什么得到的表示就能够在 downstream 例如 classification 上表现良好？这篇文章通过定义 latent class 以及样本和 latent class 的距离入手，推导出了二分类情况下的 loss bound;，保证了其的泛化性能。文章提出了一个改进算法就是进行 block 处理，不再直接优化各个 pair 的 inner product，而是转而优化 positive block以及 negative block 的内积：</p><p>$$ \begin{array}{l}<br>L^{b l o c k}(f):= \<br>\mathbb{E}\left[\ell\left(f(x)^{T}\left(\frac{\sum_{i} f\left(x_{i}^{+}\right)}{b}-\frac{\sum_{i} f\left(x_{i}^{-}\right)}{b}\right)\right)\right]<br>\end{array}$$</p><p>文章在后续的实验上也验证了这一方法会优于内积方法。</p><p>###Contrastive learning for image captioning</p><p>这篇文章希望通过使用对比学习来解决 image captioning 中标题文本<strong>可区别性</strong>的问题，即尽可能让标题描述和唯一的一张图片对应，而不是笼统而又模糊的可能和多张图片对应。作者引入对比学习，把对应的图像和标题作为正例 pair $(c, I^+)$，并把其中的图像随机采样得到负例 pair $(c, I^-)$，并且在已有的 sota 模型上优化 $p(c|I^+) - p(c|I^-)$，提升生成的 caption 的效果。</p><p>###Contrastive learning of structured world models</p><p>前面提到，表示学习能够较好的解决一些简单的任务，但是理解物体之间的关系以及建模其间的交互关系不单单需要好的表示，同样需要一个好的归纳偏好。这篇文章就是通过利用 state set 来表示世界中各个物体的状态，并且利用图神经网络来建模其之间的交互，再进一步地利用对比学习来提升性能，下图给出了模型的示意图：</p><p><img src="/img/cl-world-model.png" alt="CL-World Model"></p><p>这里的对比学习是从 TransE 架构迁移而来，具体地，在 TransE 中，我们会希望一个三元组 $(e_t, r_t, o_t)$ 的能够让 $H = d( f(e_t) + g(r_t), f(o_t))$ 尽可能的小，即 $e_t$ 的表示加上 relation $r_t$ 的表示和 $o_t$ 的表示尽可能地接近，而迁移到世界模型中，就是要将 entity 换成物体的 state，relation 换成 action，即经过图卷积后的得到的新的表示，通过下面的式子进行优化：</p><p>$$\Delta z_{t}=T\left(z_{t}, a_{t}\right)=\operatorname{GNN}\left(\left{\left(z_{t}^{k}, a_{t}^{k}\right)\right}_{k=1}^{K}\right)$$</p><p>$$d\left(z_{t}+T\left(z_{t}, a_{t}\right), z_{t+1}\right)+\max \left(0, \gamma-d\left(\tilde{z}<em>{t}, z</em>{t+1}\right)\right)$$</p><p>这里的 $\tilde z_t$ 是从 experience buffer 中采样得到的负例样本，文章在后续多物体交互环境的模拟实验中验证了其方法的优越性。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文介绍了关于对比学习背后的动机，以及一系列在图像、文本上的一些工作，在计算机视觉领域，其习得的表示能够很好地在下游任务泛化，甚至能够超过监督学习的方法。</p><p>回过头来看，预训练模型从 ImageNet 开始，后来这一思想迁移到 NLP，有了 BERT 等一系列通过自监督的预训练方法来学习表示，后来这一想法又反哺了计算机视觉领域，引出了诸如 MoCo、SimCLR 等工作，在一系列分割、分类任务上都取得了惊人的表现。那么，这一思想会不会又再次和 NLP 结合，碰撞出新的火花呢，让我们拭目以待。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近深度学习两巨头 Bengio 和  LeCun 在 ICLR 2020 上点名 Self-Supervised Learning（SSL，自监督学习） 是 AI 的未来，而其的代表的 Framework 便是 Contrastive Learning（CL，对比学习）。 另一巨头 Hinton 和 Kaiming 两尊大神也在这问题上隔空过招，MoCo、SimCLR、MoCo V2 打得火热，这和 BERT 之后，各大公司出 XL-Net、RoBerta 刷榜的场景何其相似。本篇文章，将会从对比学习的背后的直觉原理出发，介绍其框架，并且对目前的一些相关的工作进行简要介绍，希望能够为感兴趣的同学提供一些帮助。&lt;/p&gt;
&lt;h2 id=&quot;Motivation-amp-Framework&quot;&gt;&lt;a href=&quot;#Motivation-amp-Framework&quot; class=&quot;headerlink&quot; title=&quot;Motivation &amp;amp; Framework&quot;&gt;&lt;/a&gt;Motivation &amp;amp; Framework&lt;/h2&gt;&lt;p&gt;很多研究者认为，深度学习的本质就是做两件事情：Representation Learning（表示学习） 和 Inductive Bias Learning（归纳偏好学习）。目前的一个趋势就是，学好了样本的表示，在一些不涉及逻辑、推理等的问题上，例如判断句子的情感极性、识别图像中有哪些东西，AI 系统都可以完成非常不错；而涉及到更高层的语义、组合逻辑，则需要设计一些过程来辅助 AI 系统去分解复杂的任务，&lt;a href=&quot;http://arxiv.org/abs/1904.12584&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ICLR 19&lt;/a&gt; 的一篇 oral 就是做的类似的事情。因为归纳偏好的设计更多的是 任务相关的，复杂的过程需要非常精心的设计，所以很多工作都开始关注到表示学习上，NLP 最近大火的预训练模型，例如 BERT，就是利用大规模的语料预训练得到文本的好的表示。那么，CV 领域的 BERT 是什么呢？答案已经呼之欲出，就是对比学习。&lt;/p&gt;
&lt;h3 id=&quot;Illustrative-Example&quot;&gt;&lt;a href=&quot;#Illustrative-Example&quot; class=&quot;headerlink&quot; title=&quot;Illustrative Example&quot;&gt;&lt;/a&gt;Illustrative Example&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/img/dollar.png&quot; alt=&quot;Illustration Example&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;当你被要求画一张美元，左边是没有钞票在你面前，右边是面前摆着一张钞票画出来的结果&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;上面这个例子来自于 &lt;a href=&quot;https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Contrastive Self-supervised Learning&lt;/a&gt; 这篇 Blog，表达的一个核心思想就是：尽管我们已经见过很多次钞票长什么样子，但我们很少能一模一样的画出钞票；虽然我们画不出栩栩如生的钞票，但我们依旧可以轻易地辨别出钞票。基于此，也就意味着&lt;strong&gt;表示学习算法并不一定要关注到样本的每一个细节，只要学到的特征能够使其和其他样本区别开来就行&lt;/strong&gt;，这就是对比学习和对抗生成网络（GAN）的一个主要不同所在。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://tobiaslee.top/tags/Machine-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>三分之一，二零二零</title>
    <link href="https://tobiaslee.top/2020/05/03/thoughts-under-covoid19/"/>
    <id>https://tobiaslee.top/2020/05/03/thoughts-under-covoid19/</id>
    <published>2020-05-03T04:51:05.000Z</published>
    <updated>2020-05-03T06:37:24.199Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>2020 年的开端，或许会成为我们十年后为数不多有着清晰记忆的四个月，光怪陆离到让人不敢相信它是真的，而每次口罩下的呼吸却又在真真切切地提醒着我们，发生了什么。</p></blockquote><p>新冠疫情、A 股熔断、美股熔断、原油暴跌，这是一个让巴菲特都感慨“我还是太年轻的”的一年，也是我在 XDU 的最后一年。在学校里，学期内的划分让我对时间的流逝明确的感受，一般半年也就该写篇博（fei）客（hua）来吹吹水了，在家，除了觉得日子过的很快，确实没什么想要写点东西的冲动，所以导致一直没有更新。但说实在话，很久不更新我也有一种愧疚感，博客于我而言，是记录，也是纪念。<strong>输出文字，是很重要的一种能力，也能敦促我反过来系统性地回顾、总结，写的是生活也好，科研也罢，或许这些日后看来非常稚嫩无知的文字，是这一路最忠实的见证者。</strong></p><h2 id="Life"><a href="#Life" class="headerlink" title="Life"></a>Life</h2><p>我一直对于老家不是特别感冒，一直在杭州读书，逢年过节回老家赶场式地吃完饭拜年就收拾回杭州了，确实很难说有特殊的情感。今年这个特殊情况，在老家呆了很长一段时间。纠结了很久才买的“买前生产力，买后爱奇艺”的 ipad， 居然也派上了很大的用场，最后甚至惊奇地发现还涨价了，小巴菲特石锤。</p><p>在老家的这段时间里，我在田间地头拔过葱（<del>因为想吃葱油拌面</del>），帮爷爷种土豆（<del>被大伯强制要求去的</del>），去鱼塘抓了一堆鱼（<del>我就是站边上看看</del>），也算是体验了一把乡村生活。看着一个一个安放小土豆的爷爷，我眼前又浮现出很多年前夏天他在收割水稻的身影，恍然间，爷爷已经耄耋之年，岁月流逝，陪他劳作的从儿子变成了孙子。<strong>从他身上，我能感受到那一代人与土地绵长不绝的关系，而我的身上流淌着他的血液，亦无法与脚下的土地分割。</strong>所以，即使未来有可能到国外读 PhD，我也会回来的，何处心安是吾乡，大洋彼岸的空气再甜，也填不满一个炎黄子孙的胃口与内心。另外，想到身边不少同学的爷爷奶奶已经去到了另一个美好的地方了，我感到一阵庆幸，希望所有的爷爷奶奶都能健康长寿，能让他们的孙子孙女陪他们钓钓鱼、种种菜。</p><p>回到杭州之后，和初中、高中同学都聚了一聚。不少初中同学已经走上了工作岗位，逐渐在向社会人转变。隔阂到说不上，确实能聊的东西不多，毕竟学校和社会之间的鸿沟，慢慢在同学情谊上拉出缝隙，变成平行线，也许只是时间问题。高中同学还好，在继续求学的居多，所以还都有的聊，甚至还能联机打把游戏。</p><p>再就是大学同学了，真的，谁能想到呢，想象中每天嗨到昏厥，彻夜把酒言欢的毕业季，就要这样结束了。希望能早日回到学校，毕竟，<strong>我所理解的生活，就是和喜欢的一切在一起</strong>。</p><h2 id="Research"><a href="#Research" class="headerlink" title="Research"></a>Research</h2><p>最近正在 On-going 的一些 project 还是蛮有意思的，朝着我之前说的更具备 insight 的方向努力。对于做 solid 的研究，我觉得首先要问自己下面三个问题：</p><ul><li>文献调研足够了吗？用好搜索引擎，对于 NLP 来说，主要是 Google Scholar + ACL Anthology。当然不同会议的侧重也会有所不同，例如，Data Mining 的 paper 还是得去 KDD、ICDM 上找找 related paper。把这些 paper 写成一个 list，尝试提炼出其脉络和框架，然后把自己的想法放到一个尽可能高的位置，<strong>即，能把别人的工作放进你的框架里</strong>。即使是已经确保没有人做过了，也要关注一下 arxiv，因为现在是个百舸争流的时代，虽说同时期的工作可以不作为被拒稿的理由，但是从别人的paper，找到自己没有看到的 point，复盘学习，也是很有价值的。</li><li>问题定义清楚了吗？A + B 式的研究逐渐成为过去式，并且，随着工具门槛的降低，算法工程师写代码这一侧的能力不再成为其核心竞争力，即，solution 的 toolkit 大家都有，那么，只有能够找到有价值的问题、定义出问题才是不可替代的。这方面我觉得我还是有所欠缺的，没想明白就开始动手，需要向 Senior 的同学老师学习。</li><li>实验可复现吗？对于 baseline，一方面要选比较强大的模型，比如现在做 NLP，是骡子还是马我觉得起码得和 BERT 比一比；另外，对于 baseline 的性能，也要考虑周到，不是随便跑一组看的过去的就好了，因为 task 不太一样，可能原先的 setting 并不适合新的问题，对于常见的几个超参跑个 grid search 是必需的。此外，注重统计上的显著性，最简单的，多跑几 个seed ，看看 mean 和 std，确保提升是显著的。</li></ul><p>另外一点，也是看刘洋老师的<a href="https://www.bilibili.com/video/av94614779/" target="_blank" rel="noopener">研究生论文选题方法</a>以及和师兄交流得到的一个感悟，就是要<strong>尽可能做成体系的工作</strong>。对于一个博士生来说，他的 thesis 应该就是他发表的若干文章，应该落在一个三角形之内，在这个三角形之内的内容，就是他<strong>没有人比我更懂</strong>的领域。或许对于一个硕士生来说，在一个领域开坑的难度角度，但是也应该尽可能把工作 focus 到一个较大的坑里，聚沙成塔，再努力挖出自己的一些小坑。一来，这样在写论文的时候，能够更有体系的组织已经发表的工作；二来，很多问题并不是一篇文章就能够解决的，多篇文章从不同的视角，去解决一个 topic，能够更加全面和完善，也能够在以后的找工作的时候更好地阐述自己研究生期间究竟做了什么。</p><p>最后，就上我看刘洋老师视频做的笔记，希望能常常翻看，不忘初心。</p><blockquote><p>问题应该是怎么样的？</p><ul><li>重要性：work on important problem, so you can do important work</li><li>创新性：</li><li>基础性：做树的根节点的工作，少做叶子节点；</li><li>复杂性：多个问题的 子问题 </li><li>系统性：子问题之间要有关联，e.g. 难表示 低资源 缺知识 不要做 xx 的若干问题研究 可以多向老师沟通 选一个适合自己的、感兴趣的题目</li><li>可行性：该问题应该具备在短期内被解决的可能性 </li><li>承接性：课题组的积累，利用组里的资源优势</li><li>适合性：自己感兴趣 能让自己觉得非常 exciting 斗志昂扬的 能发挥自己的优势</li></ul><p>如何选题：</p><p>战略性的思考，顶层设计</p><ol><li>调研：看 paper，和打仗中的侦查差不多；站在领域的前沿，时效和质量<ol><li>经典著作 教材  PRML manning 的 intro， famous scholars 的 thesis</li><li>Journal 、 Conference paper</li><li>Social media, twitter</li><li>Arxiv </li></ol></li><li>思索</li><li>判断</li></ol><p>怎么读论文：</p><ul><li>80% 只读标题</li><li>14%的文章 只读标题和摘要</li><li>5% 读标题 摘要 正文</li><li>1% 搞懂全部细节</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;2020 年的开端，或许会成为我们十年后为数不多有着清晰记忆的四个月，光怪陆离到让人不敢相信它是真的，而每次口罩下的呼吸却又在真真切切地提醒着我们，发生了什么。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;新冠疫情、A 股熔断、美股熔断、原油暴跌，这
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>我的黄金岁月——十年回首</title>
    <link href="https://tobiaslee.top/2019/12/31/my-golden-years/"/>
    <id>https://tobiaslee.top/2019/12/31/my-golden-years/</id>
    <published>2019-12-31T15:44:29.000Z</published>
    <updated>2019-12-31T15:30:45.358Z</updated>
    
    <content type="html"><![CDATA[<p>明天就是 2020 年了，其实 11 月份的时候就想起笔把今年的总结写了，但可能时间没到，没有那种氛围，写了一点就丢到了回收站里。到了 12 月 31 日，在陆续看了网易云等软件以及关注的人们的诸多的年度总结之后，总算积攒够了情绪。人是需要仪式感的动物，所以，开始动笔写这篇，年度总结，亦是从千禧年开始，一些我觉得印象深刻的回顾。</p><h2 id="记忆碎片"><a href="#记忆碎片" class="headerlink" title="记忆碎片"></a>记忆碎片</h2><p>我最遥远的记忆大约要从幼儿园开始。上幼儿园的第一天我哭了，这件事我印象深刻，没想到我接下来居然会这么多年的书，甚至可能要攻读一个象征着人类学历顶峰的 PhD 学位呢。所以，以后我若是有了小朋友（们），一定要告诉他（们），幼儿园只是你噩梦的开始哈哈哈。</p><p>那时候，和邻居家的小妹妹，跟着她爷爷，走过大约两个红绿灯的距离，去上幼儿园；那条路上有一块巨大的铁皮广告牌，风吹过的时候会发出巨大的声响，经过的时候我总会不自觉的加快脚步。听妈妈说，那个女孩还说过非我不嫁这样的话，想来也是天真的可爱了。</p><p>幼儿园上完我就去一个离家很远的地方读小学，公交车程得快 1 个小时，没想到我爸用一辆遥控赛车成功忽悠年少的我独自乘公交车上学、放学，心也是够大的。小学时候的我经常会在车上得到叔叔阿姨的让座，在下班高峰期的时候会蜷缩在两个座椅之中打盹，睡醒之后若是坐过站又会很着急，眼泪打着转地寻找回家的方向。<strong>那时候的我，身体虽然很小，但也很坚强啊</strong>。在车上我经常会睡觉，也会想一些事情，想来这似乎就是我们现在所谓的“暗时间”，神奇。小学的 6 年里，我觉得班主任并不太喜欢我，虽然我的成绩还可以；我大约在四年级的时候因为偷校门口小店的零食被严厉的批评，她甚至用了<strong>一辈子都没有出息</strong>这样的词来形容我的未来。我觉得批评是没问题的，但不要因此 judge 我的未来，因为<strong>谁能说得准呢</strong>。在一段时间被同学在背后指指点点之后，我意识到了做人一定要光明磊落，不然对不起我的名字。</p><p>带着些许的自卑，我上初中了。初中或许是我自信心提升最大的一个阶段，我去到一个竞争并不那么激烈的中学，凭借着我的小聪明，在老师们的喜爱中度过了我愉快的三年。回过头来看，作为义务教育的最后阶段，我的初中或许在成绩上并不完美（相比民办初中），但它确确实实是提供了一个完整的平台，供我去探索一些可能性。我开始学习吉他，接触了足球，为之后的丰富生活打下了基础。</p><p>初三的最后，我保送到了市里的重点中学，历史底蕴和文科是它的强项，初中老师认为很符合我的气质（我难道看着像当领导的料吗hh）。没想到最终的我还是选择了理科，甚至将要成为一名程序员。小学的时候，电脑老师对我们说：“云计算是未来”。或许，这句话为我的专业选择埋下了伏笔。高中里我虽然很努力学习，但数学总是考不太好，现在想来还是刷题不够，<strong>量变才能质变</strong>。最后高考也没有发挥好，但我觉得主要是均值不高，发挥不好最多也就是在一个 $\sigma$ 的范围之内。</p><h2 id="何其幸运"><a href="#何其幸运" class="headerlink" title="何其幸运"></a>何其幸运</h2><p>回顾这些年，我觉得我是非常幸运的。一方面，健健康康地成长到了这个年纪，没有在放学路上被拐走，也没有被突如其来的杀手带走生命，我深知，世界上很多地方的孩子甚至连一口干净的水都是奢望，活到现在，很不容易；另一方面，也没有早早地中止学业，身边很多的同学已经走上了社会，我还能够在学校里残喘苟延一段时间，我在初中毕业的日志里写到：</p><blockquote><p>读书不是唯一的出路，但是在当今中国，当今社会背景之下最好的出路。</p></blockquote><p>我觉得曾经自己还是有点 Naive 了；事实上，在过去的十年里，读书不是最优的选择，那是一个波澜壮阔的时代，满地都是机会。若是在 10 年去杭州四季青做服装批发，赶上淘宝的热潮，再趁早上了杭州房子的车，现在已经不用奋斗了（逃。<strong>没有如果，这就是人生的趣味所在</strong>，10 年，12 岁的小屁孩，能做的就是乖乖读书，这是如我一样家里没有矿产的人普通人的唯一解，虽然读书给不了你王思聪一样的起跑线，但它能让你有一份较为体面的工作，不至于过不了日子的这样一个下界，上界，就得看历史的进程和个人的奋斗了。</p><p>真的，得学会和时间做朋友，一时的困乏没什么。我有限的人生总得来说还是一路向上的，虽然有过一些现在看来只是小浪花的波折，但得充满信心地向前看，或许几年之后就要遭到社会的毒打了（逃。</p><h2 id="下一个十年"><a href="#下一个十年" class="headerlink" title="下一个十年"></a>下一个十年</h2><p>或许再次回头，就已经是三十而立的年纪。下一个十年，应当是真的成就一番事业的时候了。过去的日子里，我们在 08 年经历了地震、奥运、雪灾、金融危机；12 年之后移动互联网兴起，支付宝给银行、金融行业带来的深刻变革，亦是早就了一批如今的巨头；16 年之后的人工智能，被鼓吹为下一个十年里的电力。我不知道下一个十年会发生什么，但作为一个渺小的个人，其实能做的不多：</p><ul><li>保持学习、阅读：接下来的日子里，不再有老师追着你让你写作业了，你得自己 push 自己，保持自律，去学习能够为自己带来长半衰期的知识。奶头乐的事物会越来越多，诱惑很大，但要学会啃硬骨头，坐得住冷板凳，人丑，就多看看书。</li><li>多走走，多看看：读万卷书，行万里路。生命的长度有限，努力拓展厚度是我们能做的。除了读书以外，我觉得到世界各个地方体验不一样的生活是触及更大的世界的又一方式。总而言之，<strong>不要让自己后悔走这一遭</strong>。</li><li>养成良好的理财观念：钱是我们无法回避的话题，掌握基本的理财知识，例如区分资产和负债，对于财富的积累有着重大的帮助。断断续续也在股市上已经交了不少的学费，有机会能够系统地梳理一遍，形成自己的投资框架，早日获得”睡后收入“。</li></ul><p>希望接下来的日子里，自己能够保持健康，抓住能够抓住的机会，见识更大的世界。感谢这一路以来遇到的所有人，祝大家新年快乐！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;明天就是 2020 年了，其实 11 月份的时候就想起笔把今年的总结写了，但可能时间没到，没有那种氛围，写了一点就丢到了回收站里。到了 12 月 31 日，在陆续看了网易云等软件以及关注的人们的诸多的年度总结之后，总算积攒够了情绪。人是需要仪式感的动物，所以，开始动笔写这篇
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Understanding and Improving Layer Normalization 阅读笔记</title>
    <link href="https://tobiaslee.top/2019/11/21/understanding-layernorm/"/>
    <id>https://tobiaslee.top/2019/11/21/understanding-layernorm/</id>
    <published>2019-11-21T02:54:40.000Z</published>
    <updated>2019-11-20T13:48:17.050Z</updated>
    
    <content type="html"><![CDATA[<p>LayerNorm 是 Transformer 中的一个重要组件，其放置的位置（Pre-Norm or Post-Norm），对实验结果会有着较大的影响，之前 ICLR <a href="https://openreview.net/forum?id=B1x8anVFPr" target="_blank" rel="noopener">投稿</a>中就提到 Pre-Norm 即使不使用 warm-up 的情况也能够在翻译任务上也能够收敛。所以，理解 LayerNorm 的原理对于优化诸如 Transformer 这样的模型有着重大的意义。先来简单地复习一下 LayerNorm，类似 BatchNorm，其核心思想就是对同一层内的神经元的值进行 Normalization 操作。具体地就是计算输入向量 $\mathbf{x}$ 内各个单元的均值 $\mu$ 以及方差 $\sigma$，然后通过一个学习一个 gain $\mathbf{g}$ 和 bias $\mathbf{b}$ 来 re-scale 回去：</p><p>$$ \mathbf{h} = \mathbf{g} \odot N(\mathbf{x}) + \mathbf{b} $$</p><p>$$ N(\mathbf{x}) = \frac { \mathbf{x} - \mu}{\sigma}$$</p><p>那么，为什么 LayerNorm 能帮助模型收敛呢，一个比较普遍的看法就是 LayerNorm 能够使得每一层的输入分布变得更加的稳定（因为进行了 norm 操作），但这是从神经网络前向的角度出发考虑的，LayerNorm 是否从梯度上对网络的训练有所帮助呢？接下来的讨论，将会基于一篇 NeurIPS 2019 上的<a href="https://papers.nips.cc/paper/8689-understanding-and-improving-layer-normalization.pdf" target="_blank" rel="noopener">文章</a>，这篇文章的作者发现了两个有趣的结论：</p><ul><li>LayerNorm 中引入的 gain 和 bias，可能会导致 overfitting，去掉他们能够在很多情况下提升性能</li><li>和前向的 normalization 相比，norm 操作之中因为均值和方差而引入的梯度在稳定训练中起到了更大的作用</li></ul><p>基于此，作者也提出了一种提出自适应学习 gain 和 bias方法，称之为 AdaNorm。文章的贡献就如标题所说，探究了一下 LayerNorm 的原理，然后提出了一种改进的方法。<strong>比起方法本身，作者做这个研究的思路更让我喜欢：</strong>从现象入手，分析结果并且设计合理的实验来验证想法，最后基于此设计新方法并且辅以适当的理论证明。接下来，就 follow 作者的思路，来一起探究一下 LayerNorm。</p><a id="more"></a><h2 id="Understanding-Layer-Normalization"><a href="#Understanding-Layer-Normalization" class="headerlink" title="Understanding Layer Normalization"></a>Understanding Layer Normalization</h2><p>LayerNorm 有可能从两个方面起作用</p><ul><li>正向的 normalization，让输入分布稳定，这里还有一个比较值得研究的就是 rescale 的两个参数 bias 和 gain；</li><li>在 norm 的计算过程之中，引入的反向的梯度</li></ul><p>因此，作者先对比了一下把 gain 和 bias 去掉的实验结果：</p><p><img src="/img/simple-ln.png" alt="Simple-Norm"></p><p>这里有两点发现：LayerNorm 能 work （废话，不然为什么大家都用它）；去掉 re-scale 的两个参数（LayerNorm-simple）在很多数据集上都有提点，甚至在 En-Vi 上取得了 sota 的效果。这点有些奇怪，因为原先的设计是很符合直觉的，让神经网络学到这个 re-scale 的参数，但是为什么去掉这个之后反而效果变好了呢？作者把 training loss 曲线打出来一看，诶，simple version 抗过拟合的效果更好。再联想到 <strong>gain 和 bias 都是模型在 training set 上直接学出来的，而没有考虑到具体的 input</strong>，是非常有可能出现过拟合（即， training 和 test 不匹配）。这个想法很自然，也非常 make sense。我其实很好奇为什么作者会想到把这俩参数去掉，不过我猜测可能是就随手一试？<strong>这也告诉我们一点：很多 idea 就是试出来的，勤动手跑跑实验改改 model，说不一定 NeurIPS 就在等着你了</strong>。</p><h3 id="DetachNorm"><a href="#DetachNorm" class="headerlink" title="DetachNorm"></a>DetachNorm</h3><p>接下来我们 focus 到前向 norm 的效果，但是原先的 LayerNorm 中，前向传播中计算 mean 和 variance 的过程会引入梯度的计算，这样就无法分离两个部分的效果。为此，作者提出了 DetachNorm，操作也很简单，就是把梯度给截断掉，用一个 mean 和 variance 的 copy 来进行 norm，用 PyTorch 实现的话就是 Tensor 的一个 <code>detach</code> 方法就能搞定。作者称之为 DetachNorm，<strong>用这个方法分别和不用 LayerNorm 以及  LayerNorm-simple 对比，可以对比前向 normalization 以及后向梯度的效果</strong></p><p><img src="/img/detach-norm.png" alt="DetachNorm"></p><p>可以看到，仅仅考虑前向 norm 的效果，只在 SST 和 PTB 上取得了性能提升，而引入后向梯度的 LayerNorm-simple，相比 DetachNorm，在多个数据集上都有提升。基于此，作者得出了之前提到的结论：</p><blockquote><p>和前向的 normalization 相比，norm 操作之中因为均值和方差而引入的梯度在稳定训练中起到了更大的作用</p></blockquote><p>那么接下来，很自然的，我们就会关注到后向的梯度之上，这三种方法（LayerNorm / LayerNorm-simple / DetachNorm）上 gain 和 bias 带来的梯度有什么不同呢？作者从理论上证明了：</p><ul><li>$\mu$ ：能够让梯度回到 0 附近</li><li>$\sigma$：能够降低梯度的 variance</li></ul><p>这二者，被作者称为  gradient normalization。所以总结一下，LayerNorm 起作用的原因：一方面通过使得前向传播的输入分布变得稳定；另外一方面，使得后向的梯度更加稳定。二者相比，梯度带来的效果更加明显一些。</p><h2 id="AdaNorm"><a href="#AdaNorm" class="headerlink" title="AdaNorm"></a>AdaNorm</h2><p>之前提到，原先的直接学习 gain 和 bias 有可能导致 overfitting 的问题，作者提出的解决方法就是，gain 和 bias 需要将 input 考虑进来，讲新的 Norm 函数设计为：</p><p>$$\mathbf{z} = \phi (\mathbf{y}) \odot \mathbf{y}=\phi( N(\mathbf{x}))\odot N(\mathbf{x})$$</p><p>这里 $\phi(\cdot)$ 的设计需要满足几个要求：</p><ul><li>可微分，不然梯度没法传递</li><li>类似原先的 rescale 操作中 $\sigma$，我们希望$\phi(\mathbf{y})$ 需要是一个常量，并且大于零</li><li>得到的输出 $\mathbf{z}$  的均值应该是有界的，来避免 loss 爆炸</li></ul><p>最终，作者将 $\phi(\cdot)$ 定义为，并且证明其满足以上的三个要求:</p><p>$$ \phi(y_i) = C(1 - k y_i)$$</p><p>实验的结果也令人满意，在多个数据集上都超过了 LayerNorm-simple：</p><p><img src="/img/adanorm.png" alt="AdaNorm"></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>如我开头所说，AdaNorm 方法本身并不是这篇文章的主要亮点，我更为欣赏的而在于作者整个探究的思路，希望能够学习这种做研究的方法：</p><ul><li>切入的角度：选取一个大家都在用，但是又不怎么清楚的 block 来进行探究。我觉得今后做研究的思路就是要考虑<strong>实际的价值和应用的场景</strong>，而不单单是为了创新而创新，思考不考虑能否对整个社区做出贡献。</li><li>由表及里，从现象到本质：在选定好方向之后，多多尝试，多跑几组实验，改改这里改改那里，从<strong>现象中总结出规律</strong>，尝试用基础的原理来解释现象，而不是对着某个指标一顿调参而不深究。</li><li>有的放矢：发现问题之后，针对性地提出解决方案，做到有的放矢。</li></ul><p>希望大家也能早日做出让自己满意的工作！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LayerNorm 是 Transformer 中的一个重要组件，其放置的位置（Pre-Norm or Post-Norm），对实验结果会有着较大的影响，之前 ICLR &lt;a href=&quot;https://openreview.net/forum?id=B1x8anVFPr&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;投稿&lt;/a&gt;中就提到 Pre-Norm 即使不使用 warm-up 的情况也能够在翻译任务上也能够收敛。所以，理解 LayerNorm 的原理对于优化诸如 Transformer 这样的模型有着重大的意义。先来简单地复习一下 LayerNorm，类似 BatchNorm，其核心思想就是对同一层内的神经元的值进行 Normalization 操作。具体地就是计算输入向量 $\mathbf{x}$ 内各个单元的均值 $\mu$ 以及方差 $\sigma$，然后通过一个学习一个 gain $\mathbf{g}$ 和 bias $\mathbf{b}$ 来 re-scale 回去：&lt;/p&gt;
&lt;p&gt;$$ \mathbf{h} = \mathbf{g} \odot N(\mathbf{x}) + \mathbf{b} $$&lt;/p&gt;
&lt;p&gt;$$ N(\mathbf{x}) = \frac { \mathbf{x} - \mu}{\sigma}$$&lt;/p&gt;
&lt;p&gt;那么，为什么 LayerNorm 能帮助模型收敛呢，一个比较普遍的看法就是 LayerNorm 能够使得每一层的输入分布变得更加的稳定（因为进行了 norm 操作），但这是从神经网络前向的角度出发考虑的，LayerNorm 是否从梯度上对网络的训练有所帮助呢？接下来的讨论，将会基于一篇 NeurIPS 2019 上的&lt;a href=&quot;https://papers.nips.cc/paper/8689-understanding-and-improving-layer-normalization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文章&lt;/a&gt;，这篇文章的作者发现了两个有趣的结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LayerNorm 中引入的 gain 和 bias，可能会导致 overfitting，去掉他们能够在很多情况下提升性能&lt;/li&gt;
&lt;li&gt;和前向的 normalization 相比，norm 操作之中因为均值和方差而引入的梯度在稳定训练中起到了更大的作用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于此，作者也提出了一种提出自适应学习 gain 和 bias方法，称之为 AdaNorm。文章的贡献就如标题所说，探究了一下 LayerNorm 的原理，然后提出了一种改进的方法。&lt;strong&gt;比起方法本身，作者做这个研究的思路更让我喜欢：&lt;/strong&gt;从现象入手，分析结果并且设计合理的实验来验证想法，最后基于此设计新方法并且辅以适当的理论证明。接下来，就 follow 作者的思路，来一起探究一下 LayerNorm。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Transformer" scheme="https://tobiaslee.top/tags/Transformer/"/>
    
      <category term="Layer Normalization" scheme="https://tobiaslee.top/tags/Layer-Normalization/"/>
    
  </entry>
  
  <entry>
    <title>Meta-Learning：Learning to Learn and Applications</title>
    <link href="https://tobiaslee.top/2019/10/02/meta-learning-notes/"/>
    <id>https://tobiaslee.top/2019/10/02/meta-learning-notes/</id>
    <published>2019-10-02T12:00:37.000Z</published>
    <updated>2019-10-31T04:27:44.045Z</updated>
    
    <content type="html"><![CDATA[<p>Meta-Learning 是一个最近非常 promising 的方向，背后的 Learning to Learn 的思想在解决一些 Low-Resource / Few-shot 场景下的问题是非常有帮助的。因此，这篇文章就是 Meta-Learning 的学习笔记，同时因为我个人的研究方向，会讨论一些在 NLP 问题上的应用。</p><h2 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta-Learning"></a>Meta-Learning</h2><p>首先，在了解 Meta-Learning 具体的方法之前，一个很重要的问题就是，Meta-Learning 有什么用？为什么要用 Meta-Learning 呢？先来看看传统监督学习的 paradigm：在 training set 上利用 examples 来 training (update model parameter)，然后在 test set 上做 testing (only inference)。</p><p>如果我们希望我们的 model 有足够的泛化能力，那么对于 training set 的大小是有一定要求的，一般来说， the bigger the better，这也是现在预训练模型动辄 billion-level 的 Corpus 的一个很重要的原因。并且很多场景下的数据分布，是服从一个<strong>长尾分布</strong>的，即有一些 class 的样本很多，而有一些 class 的样本很少，<strong>典型的就是，类别不均衡问题</strong>，在这样的限制之下，怎么学习一个良好的分类器，特别是在那些只有极少数样例的 class 上。</p><p>另一个很明显的对比就是人类学习的时候，能够基于过往的经验快速地学习，即使新的任务只有 few examples，比如下面这个图，你觉得是哪个画家的风格呢？:</p><p><img src="/img/human_test.png" alt="Human Test"></p><p>正确答案是 Braque（我在这两者之间摇摆不定了很久hhh），Chelsea 在 Tutorial 现场是多数人答对了，看来我可能真的没什么艺术天赋？这里每个风格只有 3 个样例，我们答对的一个很大的原因就是来自于<strong>过往的经验</strong>，如果我们的 Machine Learning Method 也能具备这样的能力，那么他是不是就能离真正的智能更近一步？Meta-Learning 就是用来做这个的，Learning to Learn！</p><a id="more"></a><p>在我看来，Meta-Learning 或许称为 <strong>task-level learning</strong> 更为贴切：原先我们希望模型通过学习训练集中的样本，具备对新的样本进行决策的能力；而 Meta-Learning希望我们的模型能够从以前的任务（task，可以视作是某一类 example 的集合，例如图片分类中所有的狮子图片就可以视作是一种 task）中学到知识，来快速的解决新的任务，这就可以看做是我们把<strong>原先的训练集中的 example 变成了 high-level 的 task</strong>。</p><p>Meta-Learning 的具体定义这里就不再展开，有兴趣的可以参考 Tutorial 中的符号。倒是有一些术语例如 k-shot 值得一提，这里的 k 指的是 example 的数量，而 k-way classification 则是指的是类别的数量。</p><p>接下来就是 Meta-Learning 的具体算法，主流算法可以分成三大类：</p><ul><li>Black-box Adaption：核心的思想就是学习一个神经网络，来从过去的任务中学习模型的参数，新的任务则会根据之前学到的参数进行预测。因为要不断地 consuming tasks，所以一般这个学习模型参数的模型一般会采用 RNN-based Model。带来的一个问题就是：<strong>Outputing all neural net parameters does not seem scalable</strong>，要预测一个神经网络的所有参数，是不具备可拓展性的，特别是现在动辄几个 M 的参数的网络。一个直接的解决方案就是输出一个 low-dimension 的 vector 来代表之前 task 的 context，再过一个 NN 来习得模型的参数。</li><li>Non-parametric methods：或者说是 Metric-based，通过学一个度量 examples 之间的 metric，来利用之前的任务的信息，比如判断两张图片中的物体是不是同一个类别，一个很常规的方法就是训练一个 Siamese Network，得到各自的表示之后过一个 Distance NN 再判断是否是同类。测试的时候就是利用 test example 和其对应的 metric 进行比对，得到一个预测结果。主要的问题就是：一般受限与分类问题，以及对于 K 的泛化能力不是很好（比对所有的训练集，那么就是和 K 的大小成 linear 复杂度的）</li><li>Optimization-based inference：主要就是 Chelsea Finn 提出的 Model Agnostic Meta-Learning （MAML），下一节就详细地讲一讲 MAML。</li></ul><h2 id="MAML"><a href="#MAML" class="headerlink" title="MAML"></a>MAML</h2><p>之前已经讲了 MAML 提出的动机，就是希望 Model 能够具备<strong>快速学习</strong>的能力，甚至是在没有见过的样本类别、并且只有少数几个样例的情况下。Chelsea Finn 给出的解决方案就是：</p><ul><li>一方面，学习一个好的初始化参数，如果背离目标太远那么快速 adapt 也就无从谈起，当然，这也就变相地要求我们测试的 task 和训练的用的 task 时候要是同一个分布</li><li>除了一个好的参数以外，在 training 过程中，我们去模拟 train-test 的过程，即提高模型参数对于 loss 的敏感性，这样才能只用几个样本就能够在未见过的 task 上取得一个比较好的效果</li></ul><p>下面这一张图很直观地说明了 MAML 在做的事情：</p><p><img src="/img/maml.png" alt="MAML"></p><p>具体的算法框架如下：</p><p><img src="/img/maml-algorithm.png" alt="Algorithm-MAML"></p><p>可以看到有两个循环：</p><ul><li>内循环：采样一个 Task 之后，从这个 Task 中选取一些 examples 作为 training set，并且留一部分作为 validation set，根据这些样本上的 loss 对模型参数进行更新，得到的更新后的参数叫做 $\theta’$</li><li>外循环：我们用内循环得到的 $\theta’$ 在之前留出的 validation set 上计算 Loss，再用这个 Loss 来更新原来的模型参数 $\theta$</li></ul><p>值得注意的是这里的 $\theta’$ 只是一个临时的模型参数，最后使用的还是更新过的 $\theta$ 。因为外循环通过对模型内循环得到的更新后的参数求导，所以这里隐含着二阶导数，当然，对于有着 Autograd 框架的我们来说只要 <code>loss.backward()</code> + <code>optimizer.step()</code> 就好。</p><p>我对于这个算法的理解是这样：首先我们内循环就是一般的梯度更新，但我们把传统的测试阶段也加入到了训练过程之中，即外循环的时候在 validation 集上计算 loss。这一步起到的作用就是引入训练过程中未见过的样例，再利用这上面的 loss 来更新梯度，从而使得模型更新后的参数具备快速 adapt 的能力。直觉上，这样的方法是 maske sense 的，数学上能够证明 MAML 最后能够收敛到 optima 吗，Chelsea 也给出了答案，更多的证明就请参考 refences 中的文章。</p><p>当然，也有研究者认为这样的二阶导数计算复杂，所以也提出了 REPTILE，一个一阶的 Meta-Learning 算法，框架如下：</p><p><img src="/img/reptile.png" alt="REPTILE"></p><p>和 MAML 的不同之处在于，这里的外循环被简化成更新梯度时，取前面更新 k 步后与原模型的改变量的 $\epsilon$ 倍。特别地，当 $k=1$ 的时候，这个式子就变成了 Multi-Task Learning 的更新公式：每次选取一个 task，都更新一步，那么结果就是和对所有 task 的 loss 的期望求导是一样的。作者 argue 在 $k&gt;1$ 的情况下</p><blockquote><p>Specifically, it maximizes the inner product between the gradients on different minibatches from the same task. If gradients from different batches have positive inner product, then taking a gradient step on one batch improves performance on the other batch.</p></blockquote><p>也就是会有个同一个 task 内，maximize 多个 mini-batch 梯度的内积。Why？我的理解是，<strong>内积为正说明梯度的夹角小于九十度，那么两次更新的方向可以认为是近似的，所以一个 batch 上的更新能够提升在另外一个 batch 上的性能，从而提升了模型的泛化能力。</strong></p><h3 id="Pre-train-amp-Multi-Task-Learning"><a href="#Pre-train-amp-Multi-Task-Learning" class="headerlink" title="Pre-train &amp; Multi-Task Learning"></a>Pre-train &amp; Multi-Task Learning</h3><p>在学习的 MAML 过程中，会发现它背后的 idea 和目前盛行的 pre-train 很接近，他们都是去学一个初始化的参数，然后在下游的任务上进行 fine-tune；和 multi-task learning 也有些类似，那么它们的区别在哪呢？</p><ul><li>和 pre-train 的一个最大的区别在于：MAML 对于新 task 的 fit 能力更强，几步 gradient update 就能够获取一个比较好的性能 适合 few-shot learning 这样的 task，因为在一开始选取参数的时候，pre-train model 侧重的是 model 的表达能力，能够很好的</li><li>而 Multi-Task Learning 可以看做是一个 zero-shot meta-learning problem，也就是说，经过 multi-task 的 pre-training 之后，不再会有 N-way k-shot 的训练数据给你去 fine-tune 了</li></ul><h2 id="Applications-in-NLP"><a href="#Applications-in-NLP" class="headerlink" title="Applications in NLP"></a>Applications in NLP</h2><p>MAML 在 RL 中四处开花之后，很自然就会想到，把这个 framework 运用到 NLP 中，特别是那些天然存在 few-shot 设置的场景，接下来就介绍几篇 MAML 在具体的一些 NLP 任务上的应用文章（所有的文章都在<a href="#References">参考文献</a>里可以找到链接）。</p><p><strong>Meta-Learning for Low-Resource Neural Machine Translation</strong>:  EMNLP 2018 的文章，是 Meta-Learning 在 NMT 上的一次尝试，思路也很直接，机器翻译任务中存在着大量的语言，他们的语料因为使用范围、人数等原因本身就少，规模不大。那么，<strong>能不能利用富语料 e.g. En-De / Ch-En 做预训练，从而快速地 fit 小语料的翻译任务呢</strong>？套用 MAML 的框架，如果我们把每一个语言对视作是一种 task，那么只有少量数据的语言对的翻译就是 MAML 对应的 k-shot learning 问题，文章也给出了一个很直观的示意图：</p><p><img src="/img/maml-nmt.png" alt="MAML for NMT"></p><p>实验的结果就是 MAML 能够在目标的四个语言对上打败 Multi-task Learning 和 Transfer Learning。作者在 training 的时候设计了一项正则项以及使用 ealy-stopping 来避免 meta-update 造成的偏移太大，并且提出了一种 fine-tune 只更新部分 Module 的思路，<strong>怀疑这个方法面临的一个主要问题就是在 Meta-Learning 阶段，模型参数容易产生比较大的偏移，从而倾向于某一种 source task</strong>，进而在下游任务取得比较差的表现。</p><p><strong>Personalizing Dialogue Agents via Meta-Learning</strong>: EMNLP 19, 短文。在 PersonaChat(ConvAI2) 上做的 persona-chat 的实验，这篇文章把 Persona 看做是不同的 task，<strong>旨在不使用 Persona 的信息</strong>，仅仅通过用 MAML 来根据 dialog history，学习生产 personalized 的对话。生成的实验结果很有意思，在 BLEU 和 PPL 都没能超过普通的 Seq2Seq 方法，但是在 persona 的一致性和 Human-Evaluation 上超过了 Baseline。persona 一致性是通过一个预训练的分类器，判断对话是否和 persona 对应训练出来的。有点奇怪的是，PPL 和人类对 fluency 的评估结果是相反的。</p><p><strong>Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks</strong>：EMNLP 19 的一篇短文，如标题所说，主要是探究 MAML 在 NLU 任务上的应用。直接套框架，主要贡献在于比较了 MAML、FOMAML 以及 REPTILE 三种 MAML 算法的效果，并且对采样任务的方法（uniform/根据 dataset size 采样）做了探究。</p><p><strong>Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification</strong>：EMNLP 19， 长文，这篇文章利用 Meta-Learning 的方式不再是学习初始化，而是之前的方式一：输出模型的参数。具体来说，是输出一个多标签分类器的 threshold 以及各个标签类别对于 loss function 的权重，motivation 在于：</p><ul><li>多标签分类问题，存在标签之间的 dependency，以及不同标签之间的区分度是不同的（区别 organization 和 person 很简单，但是区别 broadcast 和 news 就不容易了），因此需要一个 label-specific 的 classifier</li><li>用 Meta-Learning 的思路，我们可以学习一个 classifier，而全部输出参数的话不太现实，这篇文章采用了输出两个值：分类的 threshold 以及不同 label 对于 loss function 的贡献度（权重）</li></ul><p><img src="/img/william-mlc.png" alt="Meta-Learning Multi-Label Classifier"></p><p>需要解决的一个不可微分的问题（threshold 这里有个对梯度的截断），所以文章提出用 Policy gradient 的方法来训练这个 classifier，设计了一个跟预测的 threshold 以及 prediction label prob 的相关的 reward，具体请参考文章中的公式，一句话说就是分类正确并且和预测的 label 概率值和 prob threshold 差距越大，则 reward 越高，反之则会收到惩罚。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总结一下：</p><ul><li>MAML 或者说 Meta-Learning 还是有很大的应用空间的，因为具体的问题存在着很多 few-shot 的场景，如果真的有这样的需求，那么可以尝试一下 MAML</li><li>简单的套用 MAML 框架，对于 NLP community 来说其实是没有太大增益的，所以也只能发发 short paper，想要有更大的 Contribution，还是需要结合具体的问题，针对问题提出基于 MAML 相应的解决方案。</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://sites.google.com/view/icml19metalearning" target="_blank" rel="noopener">ICML 2019 Meta-Learning Tutorial</a></p><p><a href="http://ai.stanford.edu/~cbfinn/_files/dissertation.pdf" target="_blank" rel="noopener">PhD Thesis of Chelsea Finn - Learning to Learn with Gradient</a></p><p><a href="https://arxiv.org/abs/1703.03400" target="_blank" rel="noopener">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></p><p><a href="http://arxiv.org/abs/1710.11622" target="_blank" rel="noopener">Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm</a></p><p><a href="http://arxiv.org/abs/1803.02999" target="_blank" rel="noopener">Reptile: On First-Order Meta-Learning Algorithms</a></p><p><a href="https://arxiv.org/abs/1808.08437" target="_blank" rel="noopener">Meta-Learning for Low-Resource Neural Machine Translation</a></p><p><a href="http://arxiv.org/abs/1909.04176" target="_blank" rel="noopener">Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification</a></p><p><a href="http://arxiv.org/abs/1905.10033" target="_blank" rel="noopener">Personalizing Dialogue Agents via Meta-Learning</a></p><p><a href="http://arxiv.org/abs/1909.04176" target="_blank" rel="noopener">Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification</a></p><p><a href="http://arxiv.org/abs/1908.10423" target="_blank" rel="noopener">Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Meta-Learning 是一个最近非常 promising 的方向，背后的 Learning to Learn 的思想在解决一些 Low-Resource / Few-shot 场景下的问题是非常有帮助的。因此，这篇文章就是 Meta-Learning 的学习笔记，同时因为我个人的研究方向，会讨论一些在 NLP 问题上的应用。&lt;/p&gt;
&lt;h2 id=&quot;Meta-Learning&quot;&gt;&lt;a href=&quot;#Meta-Learning&quot; class=&quot;headerlink&quot; title=&quot;Meta-Learning&quot;&gt;&lt;/a&gt;Meta-Learning&lt;/h2&gt;&lt;p&gt;首先，在了解 Meta-Learning 具体的方法之前，一个很重要的问题就是，Meta-Learning 有什么用？为什么要用 Meta-Learning 呢？先来看看传统监督学习的 paradigm：在 training set 上利用 examples 来 training (update model parameter)，然后在 test set 上做 testing (only inference)。&lt;/p&gt;
&lt;p&gt;如果我们希望我们的 model 有足够的泛化能力，那么对于 training set 的大小是有一定要求的，一般来说， the bigger the better，这也是现在预训练模型动辄 billion-level 的 Corpus 的一个很重要的原因。并且很多场景下的数据分布，是服从一个&lt;strong&gt;长尾分布&lt;/strong&gt;的，即有一些 class 的样本很多，而有一些 class 的样本很少，&lt;strong&gt;典型的就是，类别不均衡问题&lt;/strong&gt;，在这样的限制之下，怎么学习一个良好的分类器，特别是在那些只有极少数样例的 class 上。&lt;/p&gt;
&lt;p&gt;另一个很明显的对比就是人类学习的时候，能够基于过往的经验快速地学习，即使新的任务只有 few examples，比如下面这个图，你觉得是哪个画家的风格呢？:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/human_test.png&quot; alt=&quot;Human Test&quot;&gt;&lt;/p&gt;
&lt;p&gt;正确答案是 Braque（我在这两者之间摇摆不定了很久hhh），Chelsea 在 Tutorial 现场是多数人答对了，看来我可能真的没什么艺术天赋？这里每个风格只有 3 个样例，我们答对的一个很大的原因就是来自于&lt;strong&gt;过往的经验&lt;/strong&gt;，如果我们的 Machine Learning Method 也能具备这样的能力，那么他是不是就能离真正的智能更近一步？Meta-Learning 就是用来做这个的，Learning to Learn！&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="ML" scheme="https://tobiaslee.top/tags/ML/"/>
    
      <category term="RL" scheme="https://tobiaslee.top/tags/RL/"/>
    
      <category term="DL" scheme="https://tobiaslee.top/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>文本生成框架体验记录</title>
    <link href="https://tobiaslee.top/2019/08/31/TG-framework-notes/"/>
    <id>https://tobiaslee.top/2019/08/31/TG-framework-notes/</id>
    <published>2019-08-31T15:07:05.000Z</published>
    <updated>2019-08-31T15:08:21.635Z</updated>
    
    <content type="html"><![CDATA[<p>我之前主要在做的工作像嘻哈歌词、主题写作都是是文本生成任务，而大部分的生成任务都可以归结到 Seq2Seq 框架下来，最近也接触了利用框架跑了一些 Machine Translation 的数据集。在实现 idea 的时候，我喜欢的一种方式是快速实现一个简单的 Baseline，跑通模型看看主要的指标例如 BLEU 的效果怎么样，然后进一步地做模型的设计和修改。要快速迭代，一方面需要对基本框架 TF、PyTorch 的 API 足够熟悉，另一方面也需要有一个趁手的 toolkit，来避免做一些重复的工作（例如，Tokenization,  Load Pre-trained Word Embedding）等。<strong>这些 pipeline 在学习初期很有必要自己亲手实现以了解细节，但是在需要快速迭代的时候，就不要重复造轮子了</strong>。这里，我对使用过（大部分是浅尝辄止）的框架写一下主观上的感受，主要考虑几个方面：</p><ul><li>能否开箱即用</li><li>定制 Model 的难易程度</li><li>API 是否易于理解</li></ul><p>希望对选择困难的朋友们有所帮助。</p><a id="more"></a><h2 id="AllenNLP"><a href="#AllenNLP" class="headerlink" title="AllenNLP"></a>AllenNLP</h2><p><a href="https://allennlp.org/" target="_blank" rel="noopener">AllenNLP</a> 是 Allen 人工智能研究所的产品，这个组最近也产生了一系列的很不错的文章，大牛 Noah A Smith 也在这里，牛组出品的质量一般都还不错。</p><blockquote><p>GitHub 地址： <a href="https://github.com/allenai/allennlp" target="_blank" rel="noopener">https://github.com/allenai/allennlp</a></p><p>文档地址：<a href="https://allenai.github.io/allennlp-docs/" target="_blank" rel="noopener">https://allenai.github.io/allennlp-docs/</a></p><p>Tutorial：<a href="https://allennlp.org/tutorials" target="_blank" rel="noopener">https://allennlp.org/tutorials</a></p></blockquote><p>Tutorial 给的比较齐全，基本照着抄一遍就能明白是怎么玩的了，<strong>源码也很值得学习</strong>。安装方面强烈建议使用虚拟环境 Anaconda，因为在很多服务器上我们并没有 root 权限，虽说可以通过 <code>pip -u</code> 来指定只给当前用户安装，但是不同人物需要的库版本可能冲突，<strong>会导致新/旧代码出现不兼容无法运行的情况</strong>，所以，conda 大法好（尖叫</p><p>用 AllenNLP 我的遇到的一个坑的就是：它要求 <strong>PyTorch 的版本 &lt; 1.2.0</strong>，并且目前用 conda 安装的版本只支持到 CUDA 9，对于需要使用 FP16 提升速度的 Model 不是很友好。如果卸掉 conda 带的 PyTorch 版本，从官网安装 CUDA 10版本，请注意：<strong>指定 PyTorch 版本 = 1.1.0</strong>。还有一个坑点就是<strong>小版本之间的兼容性</strong>不好，有些 API 在你升级之后会发生路径的移动，所以，建议<strong>固定一个版本进行使用</strong>。</p><p>目前使用下来，<strong>AllenNLP 是唯一一个支持在训练过程中利用 BLEU 做评测的框架</strong>，冲着这一点，加分！</p><p>定义模型，很轻松，因为整个 AllenNLP 的设计 Model、Trainer 以及 Predictor 这样的一个思路，各个模块划分的很清楚，也能够间接地敦促我们据此来写代码，比面条一样的一坨不知道高到哪里去了。如果需要对于 Metric 定制，也可以通过自己魔改 Trainer 来实现相应的计算和日志功能。</p><h2 id="FairSeq"><a href="#FairSeq" class="headerlink" title="FairSeq"></a>FairSeq</h2><p>Facebook 团队出品的 Seq2Seq 框架，<strong>机器翻译首选</strong>，配合 CUDA 10 使用，速度和效果都是顶呱呱。</p><blockquote><p>文档地址：<a href="https://fairseq.readthedocs.io/" target="_blank" rel="noopener">https://fairseq.readthedocs.io/</a></p><p>GitHub 地址：<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p><p>Example Translation: <a href="https://github.com/pytorch/fairseq/tree/master/examples/translation" target="_blank" rel="noopener">https://github.com/pytorch/fairseq/tree/master/examples/translation</a></p></blockquote><p>安装方面就按文档来就可以了，丝滑无坑，基本做到开箱即用，参数的说明文档也给的很明白了，不行的话查看一下源码也就知道一系列的配置是干嘛的。</p><p>FairSeq 的一个很大的优势就是更新速度很快，基本上一有新的模型都能在 repo 里找到，像最新的 ReBERTA，也已经在库里等着你跑了（毕竟是自家的 model）。<strong>大公司出品的好处，就是自家的代码基本都用这个框架，follow 起来比较方便</strong>。</p><p>模型定制方面，我最近在机器翻译任务上对 Transformer 进行了一些改进，<strong>小的改动还是比较轻松的，但是要从头定制，不方便也没必要</strong>，适合快速地验证一下 idea。</p><h2 id="OpenNMT"><a href="#OpenNMT" class="headerlink" title="OpenNMT"></a>OpenNMT</h2><p>看名字就知道是做机器翻译的一个框架，它的一个很大的好处就是同时提供了 <strong>PyTorch 和 TensorFlow 两个版本</strong>，工业界很多时候部署都是要用到 TensorFlow 的，所以这是他的一个优势所在。</p><blockquote><p>GitHub: <a href="https://github.com/OpenNMT/OpenNMT-tf" target="_blank" rel="noopener">https://github.com/OpenNMT/OpenNMT-tf</a></p><p>Documentation: <a href="http://opennmt.net/OpenNMT-tf/installation.html" target="_blank" rel="noopener">http://opennmt.net/OpenNMT-tf/installation.html</a></p><p>QuickStart: <a href="http://opennmt.net/OpenNMT-tf/quickstart.html" target="_blank" rel="noopener">http://opennmt.net/OpenNMT-tf/quickstart.html</a></p></blockquote><p>相比 fairseq，OpenNMT 的一个缺点就是<strong>更新不够及时</strong>，很多新的东西（例如预训练大礼包），目前来看 OpenNMT 还是没有相应的支持。</p><p>开箱即用方面，对于一般的翻译任务都是能够胜任的，但是仔细对比一下可以发现，TF 版本对于训练参数的配置的支持是要比 PyTorch 版本好一些，所以大家可以在阅读文档对两个版本有个基本的了解之后，根据自己需要来进行选择。</p><p>定制方面没有尝试，不多说。</p><h2 id="Texar"><a href="#Texar" class="headerlink" title="Texar"></a>Texar</h2><p>Texar 是 CMU 邢波老师组出品的框架，一作 Zhiting Hu 在生成领域也有很多代表作。</p><blockquote><p>GitHub: <a href="https://github.com/asyml/texar" target="_blank" rel="noopener">https://github.com/asyml/texar</a></p><p>Document: <a href="https://texar.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://texar.readthedocs.io/en/latest/</a></p><p>Examples: <a href="https://github.com/asyml/texar/tree/master/examples" target="_blank" rel="noopener">https://github.com/asyml/texar/tree/master/examples</a></p></blockquote><p>我个人比较喜欢的是里面一些关于 RL 的例子，因为 RL 做 NLP 大部分时候需要用到 Policy Gradient 这一方法，所以其内置的 <code>PGAgent</code> 对于<strong>实现关于 RL 的想法是很方便的</strong>。类似有着 RL 相关 NLP代码的还有上交出品的 <a href="https://github.com/geek-ai/Texygen" target="_blank" rel="noopener">Texygen</a>，但似乎最近已经不再更新，因此如果需要复现上交组 RL 的 Paper 相关代码可以参考一下，使用的话还是推荐 Texar。</p><p>API 方面的设计其实官方提供的图就很不错，大家可以参考一下：</p><p><img src="/img/texar-stack.png" alt="Texar-Stack"></p><h2 id="HuggingFace"><a href="#HuggingFace" class="headerlink" title="HuggingFace"></a>HuggingFace</h2><p>HuggingFace 我没用过，但是组里师兄有不少用它的，因为它对<strong>新模型的支持非常及时</strong>。<a href="https://github.com/huggingface/pytorch-transformers" target="_blank" rel="noopener">PyTorch-Transformer</a> 这个 repo 集预训练模型之大成，像 GPT-2、XLNet 和 RoBerta 它都有收录。最重要的一点在于，原先像 Google 官方提供的 BERT 代码只能够跑分类任务，但是 HuggingFace 可以跑生成！</p><blockquote><p> <code>run_generation.py</code>: GPT, GPT-2, Transformer-XL and XLNet for conditional language generation</p></blockquote><p>之前看到 ConvAI 这个对话挑战上的第一名就是 HuggingFace，所以说有刷榜需求的同学们，可以尽早把它添加到你的武器库里，相信能够让你如虎添翼。</p><h2 id="Misc"><a href="#Misc" class="headerlink" title="Misc"></a>Misc</h2><p>最近没怎么更新 Blog，一方面是因为实验室的砖确实比较多，搬不怎么过来，另外一方面，也是这段时间<strong>忙于 coding，没有时间整理</strong>。但还是要保持读文章的习惯，做好笔记！</p><p>然后是清华的 NLP 小组收集整理了一波 <a href="https://github.com/THUNLP-MT/TG-Reading-List" target="_blank" rel="noopener">Text Generation</a> 的文章，大家可以配合着这些框架，把一些有意思的文章复现一下，说不定，一篇顶会就出来啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我之前主要在做的工作像嘻哈歌词、主题写作都是是文本生成任务，而大部分的生成任务都可以归结到 Seq2Seq 框架下来，最近也接触了利用框架跑了一些 Machine Translation 的数据集。在实现 idea 的时候，我喜欢的一种方式是快速实现一个简单的 Baseline，跑通模型看看主要的指标例如 BLEU 的效果怎么样，然后进一步地做模型的设计和修改。要快速迭代，一方面需要对基本框架 TF、PyTorch 的 API 足够熟悉，另一方面也需要有一个趁手的 toolkit，来避免做一些重复的工作（例如，Tokenization,  Load Pre-trained Word Embedding）等。&lt;strong&gt;这些 pipeline 在学习初期很有必要自己亲手实现以了解细节，但是在需要快速迭代的时候，就不要重复造轮子了&lt;/strong&gt;。这里，我对使用过（大部分是浅尝辄止）的框架写一下主观上的感受，主要考虑几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能否开箱即用&lt;/li&gt;
&lt;li&gt;定制 Model 的难易程度&lt;/li&gt;
&lt;li&gt;API 是否易于理解&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;希望对选择困难的朋友们有所帮助。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="Text Generation" scheme="https://tobiaslee.top/tags/Text-Generation/"/>
    
  </entry>
  
  <entry>
    <title>一切才刚刚开始</title>
    <link href="https://tobiaslee.top/2019/07/14/A-Long-Way-To-Go/"/>
    <id>https://tobiaslee.top/2019/07/14/A-Long-Way-To-Go/</id>
    <published>2019-07-14T15:04:09.000Z</published>
    <updated>2019-07-15T02:35:24.325Z</updated>
    
    <content type="html"><![CDATA[<p>最近主要做的事情就是过了信科的夏令营，拿到优秀营员资格，之后又出了 EMNLP 的 review 意见。我大约在大二的时候下定决心要努力成为一名 NLPer，如今应该算是走在了正确的道路上，最近也算是取得了一些成果，所以过程记下来，希望以后自己丧气的时候，再看看，找回自己的初心。</p><h2 id="信科夏令营游记"><a href="#信科夏令营游记" class="headerlink" title="信科夏令营游记"></a>信科夏令营游记</h2><h3 id="Day-1"><a href="#Day-1" class="headerlink" title="Day 1"></a>Day 1</h3><p>第一天就是报道，并且办理入住手续，因为我在中关村提前租了房子，所以没有去安排的酒店 check in，听去的同学说条件还不错，北大还是很厚道的，<del>相比什么都不给报的复旦</del>，发了 100 块饭卡，应该够吃。</p><p>上午有个简短的开营仪式，听老师吹北大，焦点就在和隔壁较劲，想必这就是爱情吧。</p><p>下午就是各个方向老师来介绍实验室，晚上有 POJ 的模拟机试，提前适应环境，需要联系的朋友可以在<a href="bailian.openjudge.cn">OpenJudge</a>上找往年题。模拟做的是去年的题，A 了三题，rank 30% 左右，我以为这样就很稳了，结果第二天果然被打脸 2333。</p><p>模拟上机之前先溜到孙栩老师办公室，找他聊天，发现正好在，就和他进行了亲切的交谈，并嘱咐我第二天机试好好表现。<strong>提醒同学们，能尽早勾搭老师就勾搭老师!</strong></p><a id="more"></a><h3 id="Day-2"><a href="#Day-2" class="headerlink" title="Day 2"></a>Day 2</h3><p>第二天上午依旧是各个组老师介绍方向，我感兴趣的主要是计算语言所，是王厚峰老师来做介绍，但王老师和其他实验室不一样，不是在宣传组里的工作，更多好像是在科普自然语言处理，让我有一种<strong>自然语言处理没人做</strong>的错觉。</p><p>下午就是各个组的座谈，可以提前和老师们聊聊，毛遂自荐一下。我去到的就是计算语言所，穗志方老师主持了这个环节。收获还是很大的，一方面是能和老师们面对面的聊天，问一些可能很蠢的问题，但老师也都会很耐心地解答。我问了一个问题：在算力为王的时代，学校里怎么做研究？<del>有个似乎是来自所外的老师</del>胡俊峰，回答说学校里做的应该是类似公理一样的工作，让业界去 cite；侧重的是<strong>提出想法，系统地验证 idea</strong>，至于好不好用，交给时间去评判。这个老师还有个观点很有意思：如果我们和同学讨论的时间有 50% 是花在讨论上，例如 <code>LSTM 的 gate 到底是怎么其作用的？</code> ，而不是 focus 在调参上，才是真正的 deep learning。穗志方老师则说，NLP 是可上可下的，能做的很深，也能做一些 incremental 的工作，其中需要我们好好地取舍。之后，各个老师介绍了一下自己做的方向。总而言之， 收获还是很大的。</p><p>晚上就是机试了，难度相比去年来说应该是小了一些，但是有一题月历打印题卡了我很久最后还是没过，心态有点崩。最后过了三题，rank 50%，算是中规中矩吧。第三天果然老师问了这个问题，我就把这个原因说了下，老师也没怎么深究。</p><h3 id="Day-3"><a href="#Day-3" class="headerlink" title="Day 3"></a>Day 3</h3><p>先是出乎意料的笔试题，第一题是古文数学题，断句后回答：</p><blockquote><p>三角几何共计九角三角三角几何几何</p></blockquote><p>一开始看了半天没看懂，还以为是和180°有关，后来突然灵光一闪，感觉这就是两个东西卖9毛钱，一个三毛问你剩下的多少钱，我断成了 <code>三角几何/共计九角/三角三角/几何几何</code>。后来知道是<code>三角</code>和<code>几何</code>是两本书名，应该要用书名号框起来，算是对了一半吧。</p><p>还有一道题考察对信息熵的理解，以及一道简单的几何题（我似乎做错了？）还有英译汉，一篇关于检索式QA的文章，难度不大，时间 40 分钟，翻译的篇幅还是挺长的，我平时基本有看 Paper，所以没怎么难住我。</p><p>主要就是面试了，第二天早上到计算语言所门口看面试顺序名单，按姓氏拼音排序的，我在第四个。等待的时候碰到了 ypc 学长，他给我打了气，让我好好表现。问了下前面面完的同学说没有用英文自我介绍，结果我一进去穗老师就让英文自我介绍并且介绍工作。当时一下子脑子有点懵，把准备的自我介绍说完之后开始介绍工作，一开始有点卡，后来说着说着稍微好了点。</p><p>介绍完了之后还问了一下为什么读研究生，以后的安排，英语怎么样，数学课最喜欢哪个，有没有打过 ACM 为什么没有打？看得出来，老师还是比较侧重<strong>编程能力、英语和数学</strong>，我觉得没毛病，这就是一个信息学科的基本三要素。</p><h3 id="优秀营员"><a href="#优秀营员" class="headerlink" title="优秀营员"></a>优秀营员</h3><p>等了几天，9 号出了优秀营员名单，等待时候还是满煎熬的。看到自己拿到 offer 之后就给复旦和南大的招生办发了邮件，一方面是他们不报车票我懒得动，还有就是也许这个机会能顺延给后面的同学呢？算是攒点 RP。之后有一点恍惚，啊，也许曾经的那个梦想，真的要实现了？</p><h2 id="燕园一梦"><a href="#燕园一梦" class="headerlink" title="燕园一梦"></a>燕园一梦</h2><blockquote><p>只有实现了的梦想，才配拿出来和人说：我曾经有过这样一个目标。否则，那都是痴心妄想。</p></blockquote><p>六年前的夏天，我从初中保送杭高，在一次新生英语 xdf 讲座上，有个互动环节问大家以后想去哪所大学，我接过话筒，说 Peking University，然后主持人问我叫什么名字，我说：Li Lei。全场爆发一阵笑声，笑的是我的名字和英语书中著名的人物李雷重名，亦或是我的无知，我无从得知。后来某人说她对我的第一印象就是讲座上我不知天高地厚的梦想，倒也挺好。在我们高中，一年也只有个位数的同学能够进入清华和北大这样所有中国人都梦寐以求的学府。我是一个普通人，普通到班主任入学的时候就跟我说：班里比你优秀的人多得多，你以后能够考一个重点大学就不错了。是啊，我们还有着地区生，其他市的前几名，清华和北大哪里轮得到我呢？</p><p>好在我们高中并不只有学习，我的高一高二都是非常快乐的，在杭高足球队和自己创建的吉他社中度过，成绩维持在中游，够上一个 211 的分，努努力也许还能上个 985。我上了大学之后时常会想，如果再过一次高中，你会有不一样的选择吗？<strong>我说不准，毕竟谁也不能回到过去，没有人能经历两次人生，我只能说不后悔。</strong></p><p>高二下学期，我意识到了高考的重要性，于是开始勤奋地刷题，成绩也有了些许进步，大约能冲冲浙大了，并且把上海交大作为了心中的目标，只是那个人依旧不相信。我想，也许后来我的努力，很大一部分都是为了证明给她看，告诉她：你看，其实我也很棒的！<strong>其实，是想要证明给自己看，告诉那个不甘心的自己，You are more than that</strong>！</p><p>高考的前一天晚上，辗转反侧地无法入睡。我想很多人都会这样吧，毕竟一考定终身。一考定终身吗？也不一定。考完数学的晚上我知道已经血崩，但依旧怀着些微的上浙大的希望考完了所有科目。</p><p>结果出来的时候我内心毫无波澜，有一种”这就是命”的感觉。是啊，一个普通人，上一个不错的 211，你还想怎么样？毕业旅行我们去了北京，我也是第一次进到北大，在燕园里转了一圈。说实话，没什么感觉，未名湖感觉和浙大华家池差不多，甚至还小一些。像我这样的游客很多，留下来的人很少，是啊，这是属于天之骄子的地方，没有归属感是很正常的。</p><p>大二的时候到北大参加 DeeCamp，住了一个月觉得，<code>诶，好像研究生有机会来？</code>。然后就开始联系老师，在实习日记里写下：<code>争取留在组里</code>。之后也是比较的幸运，在学长指导之下，发了一篇文章，然后通过夏令营，拿到了预录取的 offer。明年九月，就可以成为一名 PKUer 了！算是圆了这个梦吧。</p><h2 id="Still-A-Long-Way-To-Go"><a href="#Still-A-Long-Way-To-Go" class="headerlink" title="Still A Long Way To Go"></a>Still A Long Way To Go</h2><p><strong>进入北大，只是一个新的开始，而并非终点</strong>。今天 EMNLP 的 review 意见出来，我自己的惨不忍睹，而师姐的审稿意见里作者基本都是赞美之词，甚至有<code>我没有发现这篇文章有什么明显的缺点</code>这样的评价。同样是短文，差距就这么大。不过一个客观事实是，那篇文章投的时候比较仓促，基本是我自己独立完成的。但这样让我意识到，我距离一个能够独立开展研究的 NLPer，还有不少距离。目前来看，我希望自己在研究生的阶段能够具备以下这几个方面的能力：</p><ol><li>能够感知方向，知道要做什么，能够提出问题来：简而言之就是挖坑的能力，说实话挺难，大约要博士才能够具备，希望能够多读 Paper，多和师兄师姐交流，希望自己毕业之后至少不至于两眼一抹黑。</li><li>有了问题之后，解决问题，验证想法：就深度学习来说，就是要利用框架能够快速 coding，并且设计实验对比出结果。这里我觉得很重要的一点是实验的设计，<strong>作者必须对提出的所有 claim 负责，并且通过理论或者实验来证明</strong>，换我做审稿人，也会是这样一个要求。所以，我觉得以后自己过不了自己 bar 的 Paper 就不要再投了，避免浪费大家时间。</li><li>产出东西的能力：这里会包括所有投稿需要的例如数据收集、写 Paper、画图、做 PPT。拿写 Paper 要举例子，除了语言上一些低级错误不能犯以外，文章的结构和故事都是很重要的环节。怎么把故事讲得 vivid 和 attractive，能够让读者信服，是很难也是非常重要的，而要把故事讲好，就和文章的结构安排密不可分，希望自己以后能多得到 <code>The paper is well-written and well-organized</code>  这样的评价。</li><li>Social：出去开会，做 Oral，但是我没有体验过，这条先放着，但总归学术圈子就这么大，与大家多多交流，保持良好的合作关系，没毛病。</li></ol><p>这几天和组里的师兄相处下来，觉得他们都非常厉害，不仅文献的阅读量很大、覆盖面广（有个师兄甚至3天看完 NAACL 的所有文章），并且对于文章、模型也都理解的很深，我觉得我简直是个<strong>弟中弟</strong>。</p><p>今天是周日，我早上 10 点到了实验室，晚上十点多走（还是第一个走的，师兄们还在实验室讨论），回到家洗漱完就只剩下敲下这些字的时间（从<code>这几天和组里的师兄相处</code> 开始），真切的感受到自己的菜以及还有很多时间没有利用好。我逐渐意识到，<strong>读一个研究生，并不是为了逃避工作，而是能够沉下心去做一些自己想做的事情，为此，你所付出的精力可能比工作还要多得多</strong>。并且，相比于工作能够带来的金钱上的正反馈，读研更需要强大的自驱力来推着自己向前，如果想要做出一番成就而不是浑浑噩噩地度过研究生的几年，真的，要做好心理准备。</p><p>真心希望自己毕业的时候，能够实现自己的目标，不要虚度这宝贵的三年（actually，我觉得其实是五年了）。</p><p>鸡汤还是要熬给自己喝的：<strong>既然选择了远方，便只顾风雨兼程</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近主要做的事情就是过了信科的夏令营，拿到优秀营员资格，之后又出了 EMNLP 的 review 意见。我大约在大二的时候下定决心要努力成为一名 NLPer，如今应该算是走在了正确的道路上，最近也算是取得了一些成果，所以过程记下来，希望以后自己丧气的时候，再看看，找回自己的初心。&lt;/p&gt;
&lt;h2 id=&quot;信科夏令营游记&quot;&gt;&lt;a href=&quot;#信科夏令营游记&quot; class=&quot;headerlink&quot; title=&quot;信科夏令营游记&quot;&gt;&lt;/a&gt;信科夏令营游记&lt;/h2&gt;&lt;h3 id=&quot;Day-1&quot;&gt;&lt;a href=&quot;#Day-1&quot; class=&quot;headerlink&quot; title=&quot;Day 1&quot;&gt;&lt;/a&gt;Day 1&lt;/h3&gt;&lt;p&gt;第一天就是报道，并且办理入住手续，因为我在中关村提前租了房子，所以没有去安排的酒店 check in，听去的同学说条件还不错，北大还是很厚道的，&lt;del&gt;相比什么都不给报的复旦&lt;/del&gt;，发了 100 块饭卡，应该够吃。&lt;/p&gt;
&lt;p&gt;上午有个简短的开营仪式，听老师吹北大，焦点就在和隔壁较劲，想必这就是爱情吧。&lt;/p&gt;
&lt;p&gt;下午就是各个方向老师来介绍实验室，晚上有 POJ 的模拟机试，提前适应环境，需要联系的朋友可以在&lt;a href=&quot;bailian.openjudge.cn&quot;&gt;OpenJudge&lt;/a&gt;上找往年题。模拟做的是去年的题，A 了三题，rank 30% 左右，我以为这样就很稳了，结果第二天果然被打脸 2333。&lt;/p&gt;
&lt;p&gt;模拟上机之前先溜到孙栩老师办公室，找他聊天，发现正好在，就和他进行了亲切的交谈，并嘱咐我第二天机试好好表现。&lt;strong&gt;提醒同学们，能尽早勾搭老师就勾搭老师!&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>BERT 的演进和应用</title>
    <link href="https://tobiaslee.top/2019/07/08/evolution-and-application-of-BERT/"/>
    <id>https://tobiaslee.top/2019/07/08/evolution-and-application-of-BERT/</id>
    <published>2019-07-08T14:00:05.000Z</published>
    <updated>2019-07-09T03:27:44.685Z</updated>
    
    <content type="html"><![CDATA[<p>Pre-train language model 风头正盛，以 BERT 为代表的模型也在各个任务上屠榜，有一统天下的趋势。知乎上也有不少文章对 BERT 的原理、应用做分析和总结的，例如张俊林老师的一系列<a href="https://zhuanlan.zhihu.com/p/68446772" target="_blank" rel="noopener">文章</a>对 BERT 和 Transformer 的解读就很有深度。但<strong>看别人写和自己读文章梳理一遍的效果是天差地别的</strong>，因此借着最近难得的闲暇，来对 Pre-train Language Model 的文章做一次整理。</p><a id="more"></a><h2 id="PLM-的演进"><a href="#PLM-的演进" class="headerlink" title="PLM 的演进"></a>PLM 的演进</h2><p>陆续有不少工作在原先的 Pre-train Language Model 的结构上做修改，如果读者对于 GPT、BERT 还不了解，可以看之前的 Blog。这里主要是对其变种做一个梳理和对比：</p><p><strong>XL-Net</strong>：把 Pre-train Model 划分为两类</p><ol><li>AR Autoregressive: 从某个方向递归地建模 language model，缺点是不能建模双向的 context information，GPT 的做法就是这样。</li><li>AE Autoencoding：recover sentence from corrupted input，比如根据 masked input 来预测完整的句子，如 bert 所做。有个缺点就是， MASK token 会有一个训练阶段和 fine-tune 阶段的 mismatch，还有个更为严重问题根据 masked sequence 与测试，被 mask 的 token 之间的是相互独立的。例如，原句为 ”New York is a city”，masked 之后“[Masked] [Masked] is a city”，那么预测时候是预测<code>P(New | is a city)</code>以及 <code>P(York | is a city)</code>，这样就无法捕获 <code>New</code> 和 <code>York</code> 之间的关联。</li></ol><p>为了解决 AE 的这个问题，作者枚举输入序列的全排列（permutation），例如输入序列 <code>1 2 3 4</code>，那么可能的全排列就有 <code>2 1 4 3</code> 又或是 <code>3 2 4 1</code>，作者希望根据排列中的序列中靠前的 Token 来预测后面的 Token，拿之前的那个例子，假如说排列是 <code>is a city New York</code>，那么预测的就是 <code>P(New | is a city)</code> 以及 <code>P(York|is a city New)</code> （利用排列中靠前的 token 预测后面）：</p><p><img src="/img/XLNet.png" alt="XLNet"></p><p>具体的实现，其实就是在 attention 的 mask 上做手脚。但在这之前，还有一个问题要解决，就是可能会有多个排列，利用相同的前缀去预测不同的词，所以需要在预测的时候把位置信息加进去。加进位置信息的情况分两种：</p><ol><li>预测当前这个词 $x_t$ 的时候，要知道这个词在句子中的位置 $z_t$，但是不能知道它的内容信息，如上图中的 b 所示</li><li>利用这个词预测排列中后面的词时候，要知道他的内容信息，如上图中的 a 所示。</li></ol><p>这两种方式，就被称为 two-stream attention（1 是 query stream， 2 是 content stream），实现上就是两个 mask 矩阵，如上图中 (c) 的右边，上方的 content stream，主对角线是不做 mask 的，意味着能看到自己；相反地，对于 query stream，就不能看到自己，而只能看到前面的 token 的信息。</p><p>文章的核心贡献就在于<strong>提出 permutation language modeling objective，不再引入额外的 MASK 标记，利用 two-stream attention 替换掉 BERT 中常规的 Attention，并且在很多任务上刷新了 sota</strong>。、想法很好，但是想到长度为 n  的序列有 n! 种情况，即使作者只选择后面一部分的 token 进行 prediction，这个预训练过程的花费也是巨大的，机器之心给出的估计是 6万刀，好吧穷也是为什么限制我们发paper 的原因吧。</p><hr><p><strong>XLM</strong>：把预训练的想法拓展到跨语言，做法也很简单，构建一个 shared sub-word （文章中使用的是 BPE）vocabulary 来对多种语言使用同一个词表，然后通过 Masked LM 做 Pre-train；对于有并行语料的场景，可以直接把 source  和 target 拼接起来然后中间加分隔符，不区分 source 和 target 地进行 mask，还是做 language modeling。作者发现这样的做法能够带来以下的好处</p><ol><li>a better initialization of sentence encoders for zero-shot cross-lingual classification </li><li>a better initialization of supervised and unsupervised neural machine translation systems  </li><li>language models for low-resource languages</li><li>unsupervised cross-lingual word embeddings</li></ol><p>前面两个都比较直观，因为本身 BERT 的一个用法就是在 large scale 上的预料上 pre-train 然后到下游任务上 fine-tune。对于第三点，作者是通过利用多个语言来辅助少资源的语言建模任务，例如利用英语和印地语来帮助建模尼泊尔语，实验发现，相比英语，同为梵语后代的印地语能够更大幅地降低 PPL ，并且二者的结合能够带来更好的效果；对于第四点，因为训练的时候用的是 shared vocab，对于不同于语言的 word，只要在词表里查找训练之后的 embedding 即可。</p><hr><p><strong>ERNIE（THU）</strong>：把知识图谱的信息整合进 pre-train 的过程中，具体地，对于文本中的实体，在知识库中找到对应的 entity，利用 transE 来进行表示之后，在 BERT 的 text encoder 之上再加一层 knowledge encoder：</p><p><img src="/img/ERNIE-thu.png" alt="ERNIE"></p><p>就是利用经过 multi-head attention 得到 的 token representation 和 entity representation 过一个 FFW 得到 combined representation，然后再根据此过 FFW 得到 token 和 entity 的表示，是一个融合 -&gt; 拆分的过程。为了适应这样的结构，文章也添加了一个 entity alignment 任务，根据给定的 toke 预测对应的 entity。模型在关系分类上的结果也证明了其确实有效。<strong>主要的 contribution 就在于想到把 KG 给融合进去，剩下的实验和设计就非常水到渠成了</strong>。</p><p><strong>ERNIE（Baidu）</strong>：名字和清华的撞车了，但是 motivation 上还是不一样的。THU 的主要 argue 外部知识的重要性，重点在于融合；而百度这边的，则是在 mask 的 level 上对中文文本做了调整，英文 mask 单词是很直接的想法，而中文的处理一般是以词为单位，因此会有 phrase mask 以及 entity mask，例如人名、地名的  mask。通过添加两种 mask 机制，来让 language model 隐式地学习到 entity 之间的关联。 比较有趣的一点是，作者还在 Pre-train 中加入了 Dialogue 语料，结果显示效果也会有一些提高。这指明了一个方向，在预训练阶段，除了修改模型、目标函数以外，选择高质量、特定 domain 的语料也是可行的方向。</p><h2 id="PLM-的应用"><a href="#PLM-的应用" class="headerlink" title="PLM 的应用"></a>PLM 的应用</h2><p>PLM 的应用，也是最近顶会产出 Paper 很多并且刷新很多 state-of-the-art 的一个方向。除了已经在原 Paper 中展现的对于简单的分类任务、MRC 等，这方面的工作现在我觉得比较有意思的是：</p><ol><li>把一些问题改造成能够用 BERT 去解决的形式，然后在具体的数据集上 Fine-tune</li><li>利用 BERT 做一些数据集的扩充和增广</li></ol><h2 id="改造任务"><a href="#改造任务" class="headerlink" title="改造任务"></a>改造任务</h2><p><strong>BERTSum</strong>：利用 BERT来做摘要生成任务，抽取式的摘要。对于每个句子，前面设置一个 CLS 在此基础之上判断是否选取这个句子；进一步地为了整合 Document-Level 的信息，再得到句子表示之后（即 CLS token），可以再做一次 self-attention 或者是过一层 RNN。此外，除了 BERT 原有的 Positional Encoding，文章为了区别句子（某些词属于某个句子），额外增加了一个 Segment Encoding，对句子进行交错编码。</p><p><img src="/img/BERTSum-Ret.png" alt="BERTSum Result"></p><p>不过从结果上来看，再加一层对于模型的提升不是特别大（对比 BERT + Classifier 和 BERT + Transformer），也似乎说明<strong>BERT 本身其实就能够考虑到比较远距离的语义关联信息</strong>。</p><p><strong>ASBA</strong>：Aspect-level Sentiment Analysis 是即情感分类问题的一个细粒度版本，给定句子判定某个方面的情感极性，具体地又有两种形式：</p><ol><li>给定 target entity $t \in T$，以及 aspect $a \in A$ ，询问对于特定对象 $t$ 在 $a$ 在方面的情感极性</li><li>给定某个 aspect $a$，询问这个层面的情感极性</li></ol><p>后者可以看做是前者的简化版本，因此文章的讨论也主要是基于第一种形式。作者通过构建辅助句子（Auxiliary Sentence）来组成问答对，从而利用 BERT 中的句子对分类范式来解决 APSA 问题。比如，对于评论：</p><blockquote><p>杭州的房价很贵，而安吉的放假很便宜并且气候很适宜</p></blockquote><p>可以构建以下几种形式的辅助句子：</p><ol><li>你觉得杭州的房价怎么样？（t = <code>杭州</code>，a = <code>房价</code>）</li><li>杭州 - 房价</li><li>杭州的房价高 / 杭州房价低 / 杭州的房价不知道 （分别对应 negative / positive / None）</li><li>杭州- 房价-高 / 杭州-房价-低 / 杭州-房价-不知道</li></ol><p>前两周，将句子和评论拼接起来，在 label 中给出结果，BERT 预测的是这个 label；后面两种形式拿 BERT 的 NSP 任务来套，对于三种结果每个都和评论计算一个 score，取最高的作为分类结果即可。</p><h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><p>BERT 在各个数据集上屠榜之后，甚至超过人类表现之后，一个很自然的问题，还有没有能难倒 BERT 的数据集？另外，一个很重要的事实是，<strong>DL 技术的进步是随着数据集的发展而不断向前的</strong>，李飞飞老师做的 ImageNet 带来了神经网络的繁荣，而像 NLP 领域的 WMT 机器翻译数据集也是推动机器翻译技术不断进步的原因之一。道理也很简单，对于深度学习这样的实验科学，必然需要 benchmark 来做 Evaluation，只有找到了靶子，才能更好地练习射箭。因此，找到能够难倒 BERT 的数据集，不能说咱们有了 BERT 就一把梭，要找到够难的数据集，倒逼技术进步。这方面，还是有不少有趣的工作的</p><p><strong>HellaSwag</strong>：Yejin Choi 组的工作，其核心想法就是上面说的那段，<strong>数据集应该和模型一起进化</strong>。SWAG 是 18 年提出的一个推理数据集（给定上文，判断一个句子是否是对应的结尾），人类能够达到 88% 的准确率，BERT之前的 state-of-the-art 是 60% 不到，而 BERT 则能达到 86%  的准确率。很自然地，会问，为什么 BERT 效果这么好？实验证明，BERT 并不具备很强的常识推理能力，而是通过 fine-tune 阶段习得的数据分布的归纳偏好(dataset-specific distribution biases），实现了接近人类的性能。下一个问题就是，如何难倒 BERT 呢？解铃还须系铃人，文章使用 adversarial filtering 技术，随机将数据集分成训练集和测试集，然后在训练集上训练分类器，利用 Pre-train Language Model 来生成假的 candidate，并且不断替换能够被分类器轻松识别的候选句子，直到在这些具有对抗性的候选答案上的准确率收敛为止。文章有意思的是对 BERT 在 SWAG 取得较好性能的探究，首先是对 fine-tune 数据集的 size 做了探究，发现只要十几个样本 BERT 就能达到 76% 的准确率，当然这并<strong>不能得出是来对 data set 的 fit 所致</strong>，文章还做了一个实验，发现即使是不给上文，也能达到 75% 的准确率，<strong>说明 fit 故事结尾就能够学习到很多的 bias</strong>，此外，即使是打乱结尾的句子词序，带来的性能降低也不足 10%，因此得出了 BERT 在 SWAG 上的出色表现来自于对于 surface 的学习，学习到合理 ending 的某些 Realization Pattern 的结论。</p><p><strong>COMET</strong>：同样出自 Yejin Choi（哈哈哈最近看了好多他们组的文章），idea 也很有意思，利用 Pre-train language model，来进行常识 knowledge triplet 的生成。方法也很简单，对于 KB 的三元组，$(s, r, o)$ 分别对应主语、关系和对象，像 ConceptNet 里的 “taking a nap” 就可以写成：</p><blockquote><p>(s = take a nap, r = Causes, o = have energy) 小憩一下能够恢复能量</p></blockquote><p>我们的任务就是给定 $s$，和 $r$，来预测出 $o$。有了这样的想法之后，就要把任务改造一下来适应 Pre-train Model：</p><p><img src="/img/comet.png" alt="Comet"></p><p>就是把 $r$，$s$， $o$ 看成句子，然后用 MASK 隔开，训练的时候利用 MLE，预测的时候就可以把 $o$ 交给 Model 来进行预测。得到的结果非常有趣：</p><p><img src="/img/comet_ret.png" alt="Comet-Result"></p><p>这篇文章除了能够在 ConceptNet 上做一些 completion 以外，也告诉我们，pre-train language model 还是能学到一些常识信息的，但这是不是还是 Surface Realization 呢？有待探究。</p><p>所以，相比较拿 BERT 这类模型去套一些已有的任务，如何另辟蹊径地找一些类似数据增强、数据集生成的任务，也许更有意义。</p><h2 id="PLM-的分析"><a href="#PLM-的分析" class="headerlink" title="PLM 的分析"></a>PLM 的分析</h2><p>对于 BERT 为什么表现出色，除了根据任务来探究这个问题以外，也有不少工作在一些基础的语言任务上做研究：</p><p><a href="##Reference">Syntactic Ability</a>：探究 BERT 的句法能力，Yoav Goldberg 的一篇类似实验报告的文章，写的比较随意，通过主谓一致任务来探究 BERT 的对于句法结构的捕获能力。文章主要的发现有：</p><ol><li>一般我们会认为像 RNN 这样循环的结构对于句法（尤其是主谓一致任务非常重要），但是结果表明，purely attention-based model 也能够做的很好，至少和 LSTM 的表现是在同一个 level 上的</li><li>BERT-base 的表现会比 BERT-large 好，这是不是意味着对于句法任务而言， model capacity 并不是一个主要的因素。自然而然地，我们想到，那么多大的 capacity 能够刚好 cover 住这个任务？</li></ol><p><a href="##Reference">Attention</a>：BERT 主要组件是 Transformer，而 Transformer 的就是 Self-Attention + Multi-head Attention，Attention 权重是可以可视化出来看一看的，虽然最近有一些工作认为 Attention 不能解释模型行为的一个，但看看无妨：</p><p><img src="/img/vis_bert.png" alt="Visualization-BERT"></p><p>比较有趣的现象是对 <code>[SEP]</code> 的 attention weight 大多很大，文章认为这可以看作是一个<strong>空操作</strong>，当不知道 attention 谁的时候就会 attend 到 <code>[SEP]</code> 上，为了证明这一点，文章可视化了 loss 对 attention 权重梯度，发现这些权重的梯度的大小很小，意味着其对于最终结果不会有太大的影响。另外，文章还发现在 self-supervised 的过程中，能够学习到一些句法知识，而这是通过 attention weight 来实现的，文章把 BERT 中 head 的 attention weight 拿出来对 pre-trained word embedding（Glove） 做一个加权，来预测一个词是否是另外一个词的 head，结果能取得 77 的 UAS，说明 attention weight 之中也包含了很多信息。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如张俊林老师所说，BERT 大有一统  NLP 局面的趋势，但是学术上依旧有很多坑可以填，虽然我们可能没有那么多的机器，但是，搞清楚 BERT 的原理、适用的场合，提出 idea 并且用实验去验证，这就是学术的做法，至于效用，就交给工业界去 judge 就行；反过来，工业界，也不是每家每户都有那么多卡的能用上 TPU 的，那部署的时候怎么用这么大的模型，是不是要蒸馏、压缩一下，也是很值得考虑的问题。<strong>总而言之，NLP 不会因为 BERT 而失去活力，反倒是，焕发新春</strong>。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://arxiv.org/abs/1901.05287" target="_blank" rel="noopener">Assessing BERT’s Syntactic Abilities</a></li><li><a href="http://arxiv.org/abs/1901.07291" target="_blank" rel="noopener">Cross-lingual Language Model Pretraining</a></li><li><a href="http://arxiv.org/abs/1906.05317" target="_blank" rel="noopener">COMET: Commonsense Transformers for Automatic Knowledge Graph Construction</a></li><li><a href="http://arxiv.org/abs/1905.07129" target="_blank" rel="noopener">ERNIE: Enhanced Language Representation with Informative Entities</a></li><li><a href="http://arxiv.org/abs/1904.09223" target="_blank" rel="noopener">ERNIE: Enhanced Representation through Knowledge Integration</a></li><li><a href="http://arxiv.org/abs/1905.07830" target="_blank" rel="noopener">HellaSwag: Can a Machine Really Finish Your Sentence?</a></li><li><a href="http://arxiv.org/abs/1906.04341" target="_blank" rel="noopener">What Does BERT Look At? An Analysis of BERT’s Attention</a></li><li><a href="http://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pre-train language model 风头正盛，以 BERT 为代表的模型也在各个任务上屠榜，有一统天下的趋势。知乎上也有不少文章对 BERT 的原理、应用做分析和总结的，例如张俊林老师的一系列&lt;a href=&quot;https://zhuanlan.zhihu.com/p/68446772&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文章&lt;/a&gt;对 BERT 和 Transformer 的解读就很有深度。但&lt;strong&gt;看别人写和自己读文章梳理一遍的效果是天差地别的&lt;/strong&gt;，因此借着最近难得的闲暇，来对 Pre-train Language Model 的文章做一次整理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>概率图模型学习笔记</title>
    <link href="https://tobiaslee.top/2019/05/07/PGM-Notes/"/>
    <id>https://tobiaslee.top/2019/05/07/PGM-Notes/</id>
    <published>2019-05-07T13:37:50.000Z</published>
    <updated>2019-07-12T02:06:22.816Z</updated>
    
    <content type="html"><![CDATA[<p>概率图模型（PGM）是很重要的一类模型，听闻很久以前的”概率图模型加边”和现在的”神经网络加层”是同样的地位，但随着 DL 的发展，PGM 已经有一些不吃香了。但我<strong>依旧觉得这个一个很重要的知识点</strong>，对于 NLP 而言，还有很多序列标注的问题的 sota 还是基于 CRF（条件随机场，概率图模型的一种）取得的，所以还是很有必要学习一下，并且和之前的一些知识给串起来看，会有更多的收获。这篇文章就我记录《统计学习方法》和《统计自然语言处理》两本书中相关章节的学习笔记。</p><h2 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h2><p>概率图模型，就是在概率模型的基础之上，用基于图的方式来表示概率分布，其中，图中的节点表示变量，节点之间边表示相应的概率关系。例如，我们把分词任务用概率图模型建模，$S$ 是一个汉语句子，$X$ 是其切分出来的词序列，那么，分词过程就可以看成是推断使 $P(X | S) $ 最大的 $X$ 分布；而如果是词性标注任务，那么就是给定序列 $X$，寻找词性标签序列 $T$ 使得 $P(T|X)$ 最大。我们在做的还是建模概率模型，<strong>特别之处就是变量之间的关系由一张图来确定，而通过这张图，我们可以引入一些外部的不确定性知识</strong>。</p><p>书中对常用的概率图模型的演变过程做了一个梳理：</p><p><img src="/img/pgm-evolve.png" alt="PGM-Evolve"></p><p>上面一行的是生成式模型，在一定条件下，能够转变成判别式模型。而这生成式模型和判别式模型又是什么东西呢？生成式模型假设观察序列（常常就是 label $y$）决定状态序列(常常是 input $x$)，因此建模的是 $P(X | Y)$ ，比如我们之前讨论的产生<strong>正向情感极性 $y_p$ 的文本 $s$ </strong>，就是建模 $P( s |y_p )$；判别式模型的假设恰恰相反，认为 $x$ 决定 $y$，上面的问题翻过来，给定一个文本 $s$，来判断其携带正向情感极性的概率，就是建模 $P(y_p | s)$ ，这就是一个判别式模型了。粗浅地理解的话，生成式模型就是一个生成器，典型如 n-gram 语言模型；判别式模型则可以看做是一个分类器，例如 SVM（支持向量机）。</p><p>接下来，我们就着重学习自然语言处理中比较常用的两类线性结构的概率图模型，正好一个是生成式一个判别式：HMM (隐马尔科夫模型)和 CRF (条件随机场)。</p><h2 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov  Model"></a>Hidden Markov  Model</h2><h3 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h3><p>先介绍马尔科夫模型，其描述的是一类重要的随机过程（随时间而随机变化的过程），而这种随机过程可以由一个随机变量序列来刻画。假设一个系统有限个状态 $S = (s_1, \dots, s_n)$，随着时间推移，系统的状态从一个状态转到另外一个状态，我们在每个时刻观测系统，则可以得到其状态序列 $Q = (q_1, \dots, q_T)$，一般来说，$t$ 时刻的状态会依赖之前时刻的状态，所以我们描述状态转移的时候，会刻画如下概率：</p><p>$$ P(q_t = s_j | q_{t-1}=s_i, q_{t-2} = s_k, \dots )$$</p><p>如果 $t$ 时刻状态只和 $t-1$的状态相关，则上面的式子可以改写成</p><p>$$P(q_t = s_j | q_{t-1} = s_i)$$</p><p>这个系统就被称为一阶马尔科夫链，进一步地，如果状态转移的概率 $P$ <strong>与时刻$t$无关</strong>，是一个常数 $a_{ij}$ ，则这个随机过程就可以称为马尔科夫模型，不过 $a_{ij}$ 需要满足：</p><ul><li>大于等于 0 ，很直接，这就是概率的约束</li><li>以及 $\sum_j a_{ij} = 1$ ，从一个状态转向其他状态概率之和为 1 </li></ul><p>在给定状态转移矩阵 $A$ 的情况下，我们可以利用简单的概率公式来计算给定某个状态序列的概率，这里就不再展开。一个很有趣的地方是，马尔科夫模型和 n-gram 模型是很类似的。在 n = 2 的时候，实际上就是一个一阶的马尔科夫模型；而当 $n &gt;= 3 $ 的时候，如果我们把模型表示为概率乘积 $P(w_n | w_{n-1}, \dots, w_{n-N+1})$时，那么 n-gram model 就可以视为是 n -1 阶马尔科夫模型（language 的窗口大小为 n，我们就需要看前面 n -1 个）。</p><a id="more"></a><h3 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h3><p>隐马尔科夫模型重在一个<strong>隐</strong>字，隐藏的是<strong>状态序列</strong> $Q$，而每个状态 $q_t$ 对应会有一个输出 $o_t$，我们能够观察到的只有观测序列 $O$，如果假设可取的输出集合为 $V = (v_1, \dots, v_k)$ ，那么我们就会有一个观测概率 $P(o_t = v_k | s_t = q_j)$，类似状态转移矩阵，我们可以得到一个观测概率矩阵 $B$。再加上一个初始状态概率分布 $\pi$ (一开始系统处于每个状态的概率 $P(q_0 = s_i)$)，我们就可以刻画隐马尔可夫模型 $\lambda = (A, B, \pi)$。一堆公式还是容易让人迷惑，《统计学习方法中》举了一个例子：</p><blockquote><p>假设我们有几个盒子，每个盒子里装着红白黑三种颜色的小球，每次有个人会悄悄地选择一个盒子，然后从这个盒子中选择一个球并且展示。</p></blockquote><p>在这个例子里面，盒子就是状态，每次选择盒子，就是一个状态间的转移，但是这个选择的过程我们是不知道的；从盒子中选择球并展示，就是由状态产生观测结果，我们是能够看到选了什么颜色的球的。</p><p>基于此，就会有三个基本的问题：</p><ol><li>给定 HMM 模型 $\lambda = (A, B, \pi)$ 和观测序列 $O(o_1, \dots, o_T)$ ，计算 $P(O | \lambda)$</li><li>已知观测序列 $O$，估计模型参数 $\lambda$，使得 $P(O | \lambda)$ 最大，这就是我们常用的极大似然方法</li><li>已知模型 $\lambda$ 和观测序列 $O$，求可能性最大（即 $P(Q| O, \lambda)$）的的状态转移路径 $Q = (q_1, \dots, q_T)$</li></ol><p>下面把求解这三个问题方法的核心思想做一个简要的介绍：</p><p><strong>问题1</strong>：给定 $\lambda$ 和 $O$， 求 $O$ 的概率</p><p>显然，<strong>我们需要借助状态序列来完成对观测序列概率的估计</strong>，即</p><p>$P(O | Q, \lambda) = \Pi_{t = 1}^T P(O_t | q_t, \lambda) = b_{q_1}(O_1)<em>b_{q_2}(O_2)\dots</em>b_{q_T}(O_T)$</p><p>就是一系列发射概率的乘积，而注意到 </p><p>$$P(O | \lambda) = \sum_Q P(O, Q|\lambda) = \sum_QP(O|Q, \lambda)P(Q|\lambda)$$</p><p>式子就变成了枚举所有可能的状态序列，计算其出现 $O$ 的概率，如果状态集合的数量为 $N$ ，长度为 $T$，则枚举的时间复杂度是 $O(N^T)$ ，是无法高效地计算的。为什么计算量这么大？因为枚举过程中会存在多次重复计算已经算过的部分状态序列，为了解决这个问题，可以通过使用动态规划来限制搜索的范围，从而提升效率，而 DP 的计算方式有两种：</p><ol><li><p>前向算法：从前向后计算，设 $\alpha_t(i)$ 为 $t$ 时刻观测序列为 $(o_1, \dots, o_t)$ ，且状态为 $s_i$ 的概率</p><p>$$\alpha_t(i) = P(o_1, \dots, o_t, q_t = s_i| \lambda)$$</p><p>则可以得到递推公式：</p><p>$$\alpha_{t+1}(j) = \sum_i a_{ij}\alpha_t(i) $$</p><p>而原先观测序列 $O$ 的概率由下面的式子计算：</p><p>$$P(O| \lambda) = P(o_1, \dots, o_T, q_T = s_t | \lambda ) = \sum_i P(o_1, \dots, o_T, q_T = s_i | \lambda) = \sum_i \alpha_T(i)$$</p><p>我们可以通过初始的状态分布和发射概率轻松得到 $\alpha_1(s_i)$ ，然后快乐地向后递推即可，下面这张图很好地说明了这个推断的过程</p><p><img src="/img/forward-algorithm.png" alt="前向算法"></p></li><li><p>后向算法：和前向相反，是从后往前推的。定义 $\beta_t(i)$ 为给定模型 $\lambda$ ，并且在时间 $t$ 时状态为 $s_i$，模型输出 $(o_{t+1}, \dots, o_T)$ 的概率：</p><p>$$ \beta_t(i) = P(o_{t+1}, \dots, o_T|q_t = s_i, \lambda)$$</p><p>则我们可以得到递推公式:</p><p>$$\beta_t(i) = \sum_j \alpha_{ij} b_j(O_{t+1}) \beta_{t+1}(j) $$</p><p>初看可能比较难以理解，我们可以在 $t$ 时刻状态为 $s_i$ 产生输出 $(o_{t+1}, \dots, o_T)$ 分解为两个过程：</p><ol><li>$t$ -&gt; $t+1$，状态从 $s_i$ 到 $s_j$，从 $s_j$ 产生 $O_{t+1}$，即 $a_{ij}b_j(O_{t+1})$</li><li>在 $t+1$ 时刻状态为 $s_j$，产生输出  $(o_{t+2}, \dots, o_T)$，即 $\beta_{t+1}(j)$</li></ol><p>而我们需要枚举可能的状态转移，所以有一个求和的操作，再看下面这张图</p><p><img src="/img/backward-algorithm.png" alt="后向算法"></p><p>是不是就豁然开朗了。</p></li></ol><p>上述两个算法的代码就不放了，但我们可以简单分析一下时间复杂度，外层是一个从前向后(或者从后向前)对 $t$ 的循环（$T$ 次），内层则是对每个状态计算 $\alpha_t(i)$（$N$ 次)，计算的时候需要枚举状态转移($N $次)，所以复杂度为$O(N^2T)$.另外，结合前向和后向公式，我们可以获得计算给定观测序列 $t$ 时刻状态为 $s_i$ 的概率：</p><p>$$ P(O, q_t=s_i | \lambda) = \alpha_t(i) \beta_t(i)$$</p><p><strong>问题2：给定观测序列 $O$ 和模型 $\lambda$，求最优的状态转移序列 $Q$</strong></p><p>一种很贪心的想法就是，选择每个时刻概率最大的状态作为该时刻的状态，但是显然这是行不通的，因为可能两个”最佳”状态之间的转移概率为 0，所以得到的状态转移序列是非法的。因此，我们实际上需要求的是：</p><p>$$\hat Q = \text{argmax}_Q P(Q| O , \lambda)$$</p><p>我们需要优化的是<strong>状态序列</strong>的概率，而非单个概率。为此，我们同样借助动态规划，定义 $\delta_t(i)$ 为在时刻 $t$ 时 HMM 沿着某一条路径到达状态 $s_i$ 且输出 $(o_1, \dots, o_t)$ 的最大概率：</p><p>$$\delta_t(i) = \text{max}_{q_1, \dots, q_{t-1}} P(q_1, \dots, q_t = s_i, o_1, \dots, o_t  | \lambda)$$</p><p>类似前向方法，我们可以得到如下递推式：</p><p>$$\delta_{t+1}(i) = \text{max}_j [\delta_t(j) <em> \alpha_{ji}] </em>b_i(O_{t+1}) $$</p><p>这里比较特殊的是我们需要记录一下得到最大概率所经过的路径，学过 DP 的话就知道里可以利用一个同构的数组来保存路径即可，这里就不赘述。</p><p><strong>问题3：参数估计，给定 $O$ ，找到使其出现概率最大的参数 $\lambda$</strong></p><p>很直接地，上最大似然估计即可：</p><p><img src="/img/hmm-mle.png" alt="MLE"></p><p>但是因为 $Q$ 是观察不到的的，所以这个方法是不能用。如果我们有神经网络的话，就可以很轻松地利用梯度下降来更新模型的参数(可以把转移概率、初始分布、发射概率都视作模型的参数)，从而得到一个近似解。所幸我们还有 EM 算法，其思想和神经网络有一些类似，同样是通过迭代来更新参数，只不过这里不再是拟合 groud-truth，而是通过上一步地参数来估计上图中的参数，得到新的参数来更新自身。</p><p>具体的实现就是 Baum-Welch 算法，其核心思想就是在状态序列中打开一段，计算给定 $O$ 和 $\lambda$  $\epsilon_t(i, j)$ 在 $t$ 时刻状态为 $s_i$ 且 $t+1$ 时刻为 $s_{j}$ 的概率：</p><p><img src="/img/bw-al.png" alt="BW-Algorithm"></p><p>从而，我们可以得到给定 $O$ 和 $\lambda$ 情况下，在时间 $t$ 时状态为 $s_i$ 的概率 $\gamma_t(i)$</p><p>$$\gamma_t(i) = \sum_j \epsilon_t (i, j ) $$</p><p>则我们可以用下列式子来估计模型的参数：</p><p><img src="/img/bw-al2.png" alt="BW-Algorithm Continue"></p><p>看到这里的人我想不多了吧，所以我也懒得手打公式了(逃</p><h3 id="条件随机场-CRF"><a href="#条件随机场-CRF" class="headerlink" title="条件随机场 CRF"></a>条件随机场 CRF</h3><p>之前说过条件随机场就是在意一定条件下，将 HMM 的生成式模型改成判别式模型，换而言之就是我们不再根据输出序列 $Y$(之前的观测序列) 来推输入序列 $X$（状态序列），而是建模 $P(Y|X)$。进一步，条件随机场的定义为：无向图 $G = (V, E)$ ，$V$ 中每个节点都对应于一个随机变量 $Y_v$，如果以观察序列 $X$ 为条件，则 $Y_v$ 满足：</p><p>$$ P(Y_v| X, Y_w, w \sim v)$$</p><p>其中 $w \sim v$ 表示与 $v$ 相连的节点。说人话就是，随机变量 $Y_v$ 和 $X$ 有关，并且也和与其相连的随机变量有关。举个例子就是，词性标注任务，一个词的词性，除了与这个词 ($X$) 有关以外，也和其相邻的词性标签有关。拿图说话就是：</p><p><img src="/img/crf.png" alt="CRF"></p><p>对于 CRF，到这里就已经足够我们将其应用到一些序列标注问题上了，所以就不再放公式了。接下来就来看两篇 Paper 来缓解一下被公式统治的恐惧。</p><h2 id="LSTM-CRF-for-Sequence-Labeling"><a href="#LSTM-CRF-for-Sequence-Labeling" class="headerlink" title="LSTM-CRF for Sequence Labeling"></a>LSTM-CRF for Sequence Labeling</h2><p>首先我在学习 CRF 之前，一直不知道为什么词性标注任务需要用 LSTM+CRF 这样的结构，我理解为词性标注的输入和输出都是序列，那么用 Seq2Seq 的方法做不就可以了吗？《Bidirectional LSTM-CRF Models for Sequence Tagging》 这篇文章给了我答案，看下面两张图：</p><p><img src="/img/lstm-pos.png" alt="LSTM for POS"></p><p><img src="/img/lstm-crf.png" alt="LSTM for POS"></p><p>看到区别了吗？区别就是<strong>前者 Label 之前是没有边相连的，而后者边之前是有关联的</strong>，这能解决什么问题呢？比如说，在英文中形容词后面跟的往往是名词而不是动词，这种标签之间的关系，就是 CRF 所考虑的。另外，实体识别中，也会有类似的情况出现，一个人的名字还没结束的情况下一般不会出现另外一个地名。因此，CRF 就显得非常重要了。</p><p>上面的文章提出，<strong>我们把 LSTM 的输出看做是状态到 label 的发射概率，而 CRF 层其实就是一个 label x label 大小的矩阵</strong>，建模的就是 label 之间的转移概率。有了这些概率和权重之后，就相当于有了之前 HMM 中的 $\lambda$，然后我们通过问题 2 的解法求出最优的词性标注序列，然后再通过 MLE 来更新参数即可(因为这个问题还是一个 Seq2Seq，所以 loss 还是 label-wise negative log-likelihood)。其实可以看到，神经网络在这里面扮演了两个角色：</p><ol><li>特征抽取，通过 LSTM 和 CRF 层来学习发射概率和转移概率，而不用手工的去定义规则，做特征工程</li><li>通过 SGD 等梯度下降算法可以很好地优化参数，避免使用 EM 算法的繁琐，交给 GPU 去算就是了</li></ol><p>另外一篇《End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF》和这篇文章相比，<del>没有什么东西</del>就是在 $X$ 的表示上增加了 Character-level 的 CNN，并且把实验做的更加丰富，从而<del>水了一篇文章</del>发表在顶会上。不过，<strong>很多现在看来很直接的想法，在以前依旧是很不容易的</strong>，因为我们依旧是站在了巨人的肩膀上。</p><p>最后，附上 PyTorch 的 LSTM-CRF <a href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html" target="_blank" rel="noopener">教程</a>，有兴趣的同学可以手搓一下，加深印象。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这篇文章主要是把两本书中的相关内容过了一篇，权当笔记，虽然有着大量的公式，但其实根源还是贝叶斯概率公式，所以也没有想象中那么复杂。之前看不下去就是因为心不静，抽出一点时间，手推一遍，总是能看懂的。另外，写的比较仓促，如有疏漏，敬请指出~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;概率图模型（PGM）是很重要的一类模型，听闻很久以前的”概率图模型加边”和现在的”神经网络加层”是同样的地位，但随着 DL 的发展，PGM 已经有一些不吃香了。但我&lt;strong&gt;依旧觉得这个一个很重要的知识点&lt;/strong&gt;，对于 NLP 而言，还有很多序列标注的问题的 sota 还是基于 CRF（条件随机场，概率图模型的一种）取得的，所以还是很有必要学习一下，并且和之前的一些知识给串起来看，会有更多的收获。这篇文章就我记录《统计学习方法》和《统计自然语言处理》两本书中相关章节的学习笔记。&lt;/p&gt;
&lt;h2 id=&quot;概率图模型&quot;&gt;&lt;a href=&quot;#概率图模型&quot; class=&quot;headerlink&quot; title=&quot;概率图模型&quot;&gt;&lt;/a&gt;概率图模型&lt;/h2&gt;&lt;p&gt;概率图模型，就是在概率模型的基础之上，用基于图的方式来表示概率分布，其中，图中的节点表示变量，节点之间边表示相应的概率关系。例如，我们把分词任务用概率图模型建模，$S$ 是一个汉语句子，$X$ 是其切分出来的词序列，那么，分词过程就可以看成是推断使 $P(X | S) $ 最大的 $X$ 分布；而如果是词性标注任务，那么就是给定序列 $X$，寻找词性标签序列 $T$ 使得 $P(T|X)$ 最大。我们在做的还是建模概率模型，&lt;strong&gt;特别之处就是变量之间的关系由一张图来确定，而通过这张图，我们可以引入一些外部的不确定性知识&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;书中对常用的概率图模型的演变过程做了一个梳理：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/pgm-evolve.png&quot; alt=&quot;PGM-Evolve&quot;&gt;&lt;/p&gt;
&lt;p&gt;上面一行的是生成式模型，在一定条件下，能够转变成判别式模型。而这生成式模型和判别式模型又是什么东西呢？生成式模型假设观察序列（常常就是 label $y$）决定状态序列(常常是 input $x$)，因此建模的是 $P(X | Y)$ ，比如我们之前讨论的产生&lt;strong&gt;正向情感极性 $y_p$ 的文本 $s$ &lt;/strong&gt;，就是建模 $P( s |y_p )$；判别式模型的假设恰恰相反，认为 $x$ 决定 $y$，上面的问题翻过来，给定一个文本 $s$，来判断其携带正向情感极性的概率，就是建模 $P(y_p | s)$ ，这就是一个判别式模型了。粗浅地理解的话，生成式模型就是一个生成器，典型如 n-gram 语言模型；判别式模型则可以看做是一个分类器，例如 SVM（支持向量机）。&lt;/p&gt;
&lt;p&gt;接下来，我们就着重学习自然语言处理中比较常用的两类线性结构的概率图模型，正好一个是生成式一个判别式：HMM (隐马尔科夫模型)和 CRF (条件随机场)。&lt;/p&gt;
&lt;h2 id=&quot;Hidden-Markov-Model&quot;&gt;&lt;a href=&quot;#Hidden-Markov-Model&quot; class=&quot;headerlink&quot; title=&quot;Hidden Markov  Model&quot;&gt;&lt;/a&gt;Hidden Markov  Model&lt;/h2&gt;&lt;h3 id=&quot;马尔可夫模型&quot;&gt;&lt;a href=&quot;#马尔可夫模型&quot; class=&quot;headerlink&quot; title=&quot;马尔可夫模型&quot;&gt;&lt;/a&gt;马尔可夫模型&lt;/h3&gt;&lt;p&gt;先介绍马尔科夫模型，其描述的是一类重要的随机过程（随时间而随机变化的过程），而这种随机过程可以由一个随机变量序列来刻画。假设一个系统有限个状态 $S = (s_1, \dots, s_n)$，随着时间推移，系统的状态从一个状态转到另外一个状态，我们在每个时刻观测系统，则可以得到其状态序列 $Q = (q_1, \dots, q_T)$，一般来说，$t$ 时刻的状态会依赖之前时刻的状态，所以我们描述状态转移的时候，会刻画如下概率：&lt;/p&gt;
&lt;p&gt;$$ P(q_t = s_j | q_{t-1}=s_i, q_{t-2} = s_k, \dots )$$&lt;/p&gt;
&lt;p&gt;如果 $t$ 时刻状态只和 $t-1$的状态相关，则上面的式子可以改写成&lt;/p&gt;
&lt;p&gt;$$P(q_t = s_j | q_{t-1} = s_i)$$&lt;/p&gt;
&lt;p&gt;这个系统就被称为一阶马尔科夫链，进一步地，如果状态转移的概率 $P$ &lt;strong&gt;与时刻$t$无关&lt;/strong&gt;，是一个常数 $a_{ij}$ ，则这个随机过程就可以称为马尔科夫模型，不过 $a_{ij}$ 需要满足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大于等于 0 ，很直接，这就是概率的约束&lt;/li&gt;
&lt;li&gt;以及 $\sum_j a_{ij} = 1$ ，从一个状态转向其他状态概率之和为 1 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在给定状态转移矩阵 $A$ 的情况下，我们可以利用简单的概率公式来计算给定某个状态序列的概率，这里就不再展开。一个很有趣的地方是，马尔科夫模型和 n-gram 模型是很类似的。在 n = 2 的时候，实际上就是一个一阶的马尔科夫模型；而当 $n &amp;gt;= 3 $ 的时候，如果我们把模型表示为概率乘积 $P(w_n | w_{n-1}, \dots, w_{n-N+1})$时，那么 n-gram model 就可以视为是 n -1 阶马尔科夫模型（language 的窗口大小为 n，我们就需要看前面 n -1 个）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="ML" scheme="https://tobiaslee.top/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>图神经网络笔记</title>
    <link href="https://tobiaslee.top/2019/04/12/gnn-notes/"/>
    <id>https://tobiaslee.top/2019/04/12/gnn-notes/</id>
    <published>2019-04-12T13:56:25.000Z</published>
    <updated>2019-04-12T14:14:53.765Z</updated>
    
    <content type="html"><![CDATA[<p>图神经网络 (Graph Neural Networks, GNNs) 是最近很火热的一个 Topic，最近读了不少这方面的文章，梳理一下相关的内容。</p><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><p>符号表示：$G = ( \mathcal{V},\mathcal{E}) $，图 $\mathcal{G}$ 由节点集合 $V$ 和边集合 $\mathcal{E}$ 组成。对于某些区分边类型的图(不仅仅表示有连接，而是更细致的某种连接)，还有一个关系集合 $\mathcal{R}$。GNN 的目标就是要对于图中的每一个节点 $v \in \mathcal{V}$ ，学习到它的一个表示 $h_v$ 。</p><a id="more"></a><h3 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h3><p><a href="#Reference">Kipf</a> 提出了图卷积网络用来在图上对节点进行分类。文章中的数学公式对于新人还是不怎么友好的，可以参考<a href="https://zhuanlan.zhihu.com/p/54505069" target="_blank" rel="noopener">图卷积网络(GCN)新手村完全指南</a>来阅读，这里省略了推导的过程，就从最后的前向传播公式和 Model 说起：</p><p><img src="/img/gcn_formula.png" alt="GCN-Formula"></p><p>公式中的 $X$ 对应的节点的 feature，$\hat A$ 则是经过处理的邻接矩阵，通过出度矩阵和原始的邻接矩阵计算得到。在文章中实验用的 Cora 数据集上，$X$ 用的是论文的 BOW feature，而邻接矩阵则是通过论文之间的引用关系得到（两篇文章有引用则说明有边）。模型的示意图如下：</p><p><img src="/img/gcn_model.png" alt="GCN-Model"></p><p>经过多层 GCN 的叠加，非直接连接的节点的信息也能传递到相应的节点（相当于 multi-hop）。最后每个节点的表示 $Z_v$ 可以用在下游任务中。对于不想深究数学细节的读者，一句话概括 GCN 就是：<strong>通过将邻接节点的表示做一个求和，然后做一个线性变换，再接上激活函数(ReLU)，就是一次 GCN 得到某个节点表示的过程</strong></p><h3 id="R-GCN"><a href="#R-GCN" class="headerlink" title="R-GCN"></a>R-GCN</h3><p>刚才的 GCN 没有考虑<strong>边的类型</strong>的信息，这对于一些复杂的图是一个比较大的缺点，例如下面的一个知识库，实体之间的联系也是包含着大量的信息：</p><p><img src="/img/rgcn_kb.png" alt="R-GCN KnowledgeBase"></p><p>作者举了一个例子：</p><p>知道 Mikhail Baryshnikov 在 Vaganova Academy 上过学说明 Mikhail Baryshnikov 是一个人，并且也以为着他应该住在 Russia。因此，为了将 Relation 的信息整合到 GCN 之中，作者提出了 R-GCN，核心的公式如下：</p><p><img src="/img/rgcn.png" alt="R-GCN"></p><p>后面一项是一个 self-connection，来保证自己的信息不在传播过程之中有太大的损失。和 GCN 的主要区别就在于：GCN 是简单的加和平均，而这里在每一个邻接节点的表示之前都做了一个 relation-specific 的线性变换，也就是 $W_r h_j$ 。而为了避免因为 relation 太多带来的过高矩阵维度以及对稀有 relation 的过拟合，作者对 relation 矩阵 $W_r$ 做了分解，利用小矩阵的线性组合来实现。模型的示意图如下：</p><p><img src="/img/rgcn_model.png" alt="R-GCN Model"></p><p>值得注意的一点就是，对于同一类型的关系，也分 in 和 out 的两种情况，所以在实际的代码中，我们需要对 relation * 2 来实现。</p><h3 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h3><p>前面两种 GNN 都可以看做是最邻接节点做一个加和得到的节点表示（GCN 是平均，R-GCN则是根据不同的 relation 有些许调整），想到加权和，很自然的想法就是用 self-attention，于是 Graph Attention Networks 应运而生：</p><p><img src="/img/gat.png" alt="GAT"></p><p>和之前的比较一下，最大的不同就是在<strong>邻接节点表示之前有一个权重</strong> $\alpha_{ij}​$，作者说这样能够:</p><blockquote><p>our model allows for (implicitly) assigning different importances to nodes of a same neighborhood</p></blockquote><p>当然还有不少和前作对比的好处，不然也不会发到 ICLR 上了。但是，把 self-attention 和 GCN 的结合是一个非常自然的想法，从这一点上来说，有一点灌水的嫌疑。后续作者还把 multi-head 也加了进来：</p><p><img src="/img/gat-mh.png" alt="GAT-Multi-Head"></p><p>做法就是把邻接节点的 representation 给 transform 到其他的 subspace 里，然后分别计算 weighted-sum 最后拼接起来。</p><h3 id="Uniform-Framework"><a href="#Uniform-Framework" class="headerlink" title="Uniform Framework"></a>Uniform Framework</h3><p>看了前面三个 model，有没有一种其实都差不多的感觉？没错，其实他们都可以看做是 Message Passing Networks 的一种：</p><p><img src="/img/mpn.png" alt="Message Passing Neural Network"></p><p>其中，$M_t(\cdot)$  是消息传递函数，描述邻接节点如何向被连接的节点传递 message；$U_t(\cdot)$ 则是一个更新函数，描述当前的节点如何根据周围节点传递来的消息来更新自己的表示。拿 GAT(self-attention 版本) 来说，$M_t$ 就是一个加权和，而 $U_t$ 就可以看做是一个 <code>sigmoid</code> 激活函数。<strong>设计 GNN，就是设计消息传递函数以及表示更新函数</strong>。最后，我们如何利用图中节点的表示来进行下游的任务，则是 task-specific 的了，被作为称为 readout (读出) 阶段。</p><p>最近有一篇<a href="#Reference">工作</a>探究的是 GNNs 的表达能力，作者是把 GNNs 的传播过程分成 Aggregate 和 Combine 两个阶段，其实也就是换了个说法，但是作者对于 GNNs 表达能力的证明（借用了一个图同构的测试方法来作为标准）还是很值得探索的一个方向。</p><p>总结一下：GNNs 就是将图中节点的信息，通过邻接这一种关系来进行传递，从而捕获到图整体的结构信息；通过设计合理的消息传递和更新函数（可能还包括图的构建，例如节点的 feature），我们可以得到一个适合下游任务的表示。进一步地，设计一个合理的读出函数，来在下游的任务上做诸如分类、聚类等操作。</p><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><p>利用 GNNs 获得图中节点的表示之后，下一步就是应用到各类问题上了，而 GNNs 特别是 GCN 目前在 NLP 上的应用还不是特别多，似乎现有的工作还 focus 在如何改进结构上，因此这里就简单的举两个例子。</p><h3 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h3><p>如果图中每个节点是一个 document，那么我们在得到节点的表示之后就相当于获得了一个 document vector，过一个全连接层然后做分类是很自然的想法。提出 Model 的 Paper 基本都是在 Citation Dataset 上做的测试，即把每一篇 Paper 作为节点，Paper 之间的引用关系作为边，把 Paper 的BOW 作为节点的 feature (GCN 中的 $X​$ )，最后在节点之上做分类(判断 Paper 是属于哪个领域的)。而泛化到 Text Classification 上，<strong>核心问题就是如何构建 doument 之间的 edge</strong>，和 Paper 不同的是，一般的 Document 没有直接的联系。<a href="#Reference">Graph Convolutional Networks for Text Classification</a> 就提出了一种解决的方法：把 word 看成是一种极端的 <code>document</code>，去做 word-word 和 word-document 之间的边：</p><p><img src="/img/gcn_tc.png" alt="GCN for Text Classification"></p><p>这里的邻接矩阵进一步泛化，不再是单纯的 1/0 indicator，而是更细致的权重。对于 word-word，是两个词之间的互信息，互信息衡量的是一个词之间的语义关联程度；而对于 word-document，则是对应的 TF-IDF 值，衡量一个词对于一个 document 的重要程度。注意到，word 和 document 是两个 level 的 <code>document</code> ，所以这是一张异质图，并且作者提到：</p><blockquote><p>although there is no direct document-document edges in the graph, the two-layer GCN allows the information exchange be- tween pairs of documents. </p></blockquote><p>通过 GCN 的信息传递机制，来实现 document 级别的信息传递。最后在一系列 dataset 上，GCN Model 都能够超过 LSTM 和 CNN 的 baseline。</p><h3 id="Text-Generation"><a href="#Text-Generation" class="headerlink" title="Text Generation"></a>Text Generation</h3><p>另外一个很有意思的应用是文本生成，现在有很多 Data-to-Text 的问题的 Source 是类似 Wikipedia 的一张表，而这其中其实也就一张很大的知识图的一部分，<a href="#Reference">Deep Graph Convolutional Encoders for Structured Data to Text Generation</a> 把 GCN 用到了生成上。样例数据如下：</p><p><img src="/img/rdf.png" alt="RDF"></p><p>根据上方的三元组来生成下面的描述。主要的思路就是把原先 Seq2Seq 中的 Encoder 替换成一个 GCN 来编码 Source 端的三元组。相比于 RNN 类的 Encoder，因为三元组之间并不存在时序关系，而更多的是图的结构和节点之间的边，所以用 GCN 来得到节点的表示作为 hidden state，然后在 decode 的时候 attention，也是一个比较自然的想法。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>从在 Text Classification 上的结果来看，GCN 对图的信息表示能力还是比较强的。我个人认为，未来的探索方向大约是两个：</p><ul><li>现有的机制还是对邻接节点的整合，依旧在消息传递这样一个框架下，如何设计更 expressive 和 power 的 framework 是很有意义的；</li><li>和下游任务的结合，目前还只有少量的工作在做 GCN 的应用，因此可能也是很好发文章的一个方向，我相信不久以后会有更多关于将 GCN 和特定的问题结合的工作</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">Semi-Supervised Classification With Graph Convolutional Networks</a></li><li><a href="http://arxiv.org/abs/1704.01212" target="_blank" rel="noopener">Neural Message Passing for Quantum Chemistry</a></li><li><a href="">Graph Attention Networks</a></li><li><a href="https://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">Modeling Relational Data with Graph Convolutional Networks</a></li><li><a href="https://arxiv.org/abs/1809.05679" target="_blank" rel="noopener">Graph Convolutional Networks for Text Classification</a></li><li><a href="http://arxiv.org/abs/1810.09995" target="_blank" rel="noopener">Deep Graph Convolutional Encoders for Structured Data to Text Generation</a></li><li><a href="http://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">How Powerful are Graph Neural Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图神经网络 (Graph Neural Networks, GNNs) 是最近很火热的一个 Topic，最近读了不少这方面的文章，梳理一下相关的内容。&lt;/p&gt;
&lt;h2 id=&quot;Models&quot;&gt;&lt;a href=&quot;#Models&quot; class=&quot;headerlink&quot; title=&quot;Models&quot;&gt;&lt;/a&gt;Models&lt;/h2&gt;&lt;p&gt;符号表示：$G = ( \mathcal{V},\mathcal{E}) $，图 $\mathcal{G}$ 由节点集合 $V$ 和边集合 $\mathcal{E}$ 组成。对于某些区分边类型的图(不仅仅表示有连接，而是更细致的某种连接)，还有一个关系集合 $\mathcal{R}$。GNN 的目标就是要对于图中的每一个节点 $v \in \mathcal{V}$ ，学习到它的一个表示 $h_v$ 。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Graph Neural Networks" scheme="https://tobiaslee.top/tags/Graph-Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Knowledge in Neural Language Models</title>
    <link href="https://tobiaslee.top/2019/03/09/knowledge-paper-notes/"/>
    <id>https://tobiaslee.top/2019/03/09/knowledge-paper-notes/</id>
    <published>2019-03-09T08:29:37.000Z</published>
    <updated>2019-03-09T08:32:42.622Z</updated>
    
    <content type="html"><![CDATA[<p>先验知识和当前盛行的 Neural NLP Model 的结合是一条必由之路，最近接触到 <a href="#Reference">Yejin Choi</a> 的一系列工作，并且看了她的一些 Talk（PS：真的是又有才又好看！），觉得非常有意思，将一些内容和 Paper 的阅读感悟整理如下。</p><h2 id="The-Missing-Component-in-NLM-Models"><a href="#The-Missing-Component-in-NLM-Models" class="headerlink" title="The Missing Component in NLM Models"></a>The Missing Component in NLM Models</h2><p>这几年来 NLP 的进展很快，包括在机器翻译、阅读理解等领域的 SOTA 都是能够达到甚至是超过人类水平的，绝大多数的 Model 都是基于 Seq2Seq 的 Framework，即使是说把传统的 RNN 替换成 Transformer，也依旧是在 Seq2Seq 这样一个框架之下，而对于像机器翻译这样的问题，我们喂给 Transformer 大量的数据之后，是能够习得一个非常出色的翻译模型的。但是，在 Neural Language Model 领域，典型如 Data-to-Text 任务， 目前的模型性能还是<strong>差强人意</strong>。例如根据一系列的 Keyword 来生成篇章级别的作文这个问题，模型产出的文章别说人类水平了，就连基本的逻辑、连贯性都很难达到合格的水平。即使是 GPT-2 这样 Huge Amount Data 喂出来的模型，前后文的关联性依旧是一个严重的问题，拿独角兽那个例子来说，后面一句就说这个物种有四个角，这是一个明显的逻辑错误。为什么这些生成任务和之前的 NMT 和 QA 差距如何之大？ Yejin Choi 提到有两个主要的问题：</p><ol><li><p>Information Gap：拿 NMT 来说，我们可以认为 source 和 target 两端文本所携带的信息是大致相当的，对于 Summarization 来说，则可以认为 target 所携带的信息是要少于 source 的，在这两个任务上，source 的信息是足够的；而对于像主题词写作这样的任务来说，几个 keywords 所包含的信息寥寥无几，和 target 端所需要携带的信息相比，是远远不足的。这就是 Information Gap，<strong>target 端所携带的信息大于甚至是远远大于 source 端</strong>。而如何填补这个 Gap 呢？思考一下我们在写命题作文中的时候，都是要从脑海中疯狂搜刮库存，才能写出像样的文章，那么 Neural Model 是不是也需要这样的一个知识的库存？</p></li><li><p>Alignment Between Input and Output：这个其实是 Information Gap 的另外一个视角，依旧拿 NMT 的中英翻译来说来说，”香蕉是绿的“ 对应的英文翻译是 ”Bananas are green”，这种 Alignment（对应关系）往往会通过 Attention Mechanism 来建模，所以这也是 Attention 在 NLP 中盛行的一个原因。而如果是一个 Dialogue 任务，对于同样的输入“香蕉是绿的“，那么正常的人类回答可能会是“不，香蕉是黄的”或者“它还没熟吧”。这个时候输入和输出就不存在这样一种强烈的对应关系，并且背后隐含着的是 Commonsense：“香蕉一般来说是黄的”。这种非对应，和欠缺知识，为我们构建输入和输出的 mapping 带来了困难。</p><p>进一步地，我们可以将问题分成两类：</p></li></ol><ul><li>Shallow NLU，主要就是指的是类似 NMT 这些有着强 Alignment，Mainly Focus on Surface Pattern；</li><li>Deep NLU：类似 Data-to-Text 问题，弱对应关系，并且需要额外的知识的补充才能更好地解决这个问题</li></ul><p>所以，为了更好地解决 Deep NLU 问题，我们需要在现有的 Model（主要是 Language Model） 中加入 Knowledge，来缩小 Information Gap。然而，这就又有很多问题摆在眼前，知识从哪来？如何表示知识？如何建模利用知识的过程？</p><h2 id="How-to-Obtain-Knowledge"><a href="#How-to-Obtain-Knowledge" class="headerlink" title="How to Obtain Knowledge"></a>How to Obtain Knowledge</h2><p>首先，先来大致给知识分个类：</p><ol><li>Encyclopedic Knowledge：百科式的知识，当我们询问美国的总统是谁的时候，维基百科显示一个词条就是一个例子。</li><li>Commonsense Knowledge：常识，这些知识是隐含在我们的日常对话之中的；另外还有像社会规约等等的知识在其中。</li></ol><p>对于第一种百科式的知识，有很多工作通过人工构建知识库来获取这一部分的知识，WordNet 也是一个很好的例子，其中包含了很多语言的知识；而对于第二种知识，也有类似的知识库，比如 ConceptNet，是通过搜集网络上人们的看法来对某些概念做一些常识性的推断。但是，人工搜集构建的方式总归是比较麻烦的，而<strong>常识信息隐含在对话之中</strong>：例如，我们会说“我把石头扔出去了”而不会说“我把大象扔出去了”，这里就包含着着石头比我轻，以及大象比我重这样的物理知识在里面。能不能从语言中抽取出类似这样的物理知识？<a href="#Reference">Verb Physics</a> 就做了这样一个工作。</p><p><img src="/img/verb_physics.png" alt="Verb Physics"></p><p>文章主要关注动词以及其相关词组所隐含的物理信息，包括大小(size）、速度(speed)、硬度(rigidness)等，这样我们就可以把这个任务视作是一个分类任务，根据给定的 <code>(x, y, verb)</code> 这样一个三元组，来选择在某个物理属性上 <code>x</code> 和 <code>y</code> 的关系（大于、小于或者约等于）。</p><p>另外有两点值得一提，</p><ul><li>数据集的构建：包括了动词的选取，动词词组（frame）的构建以及跨知识源的知识收集（主要是人类知识作为辅助）</li><li>Model：文章对三种 node 进行建模 frame / verb / object，而其中又因各个 node 之间的联系存在很多的转移边，模型整体比较复杂，建议可以阅读原文来深入地了解。另外，很多的信息他的数据对是比较稀疏的，因此也就需要借助类似的 Pair 来辅助传递，所以还需要考虑到各个 node 之间的联系（相似度）</li></ul><p>虽然看起来文章只是在动词所包含的物理信息上做了一些探索，但毫无疑问，如何从非结构化的自然语言中获取知识，并且用于辅助接下来的推理、分析是非常有趣也很有意义的一个方向。</p><h2 id="Representation-of-Knowledge"><a href="#Representation-of-Knowledge" class="headerlink" title="Representation of Knowledge"></a>Representation of Knowledge</h2><p>在有了外部的知识之后，如何来表示这些知识，又是一个值得思考的问题。外部的知识往往可以表示为知识图谱中 <code>(head, relation, tail)</code> 这样的三元组，表明 <code>head 和 tail 之间有着 relation</code> 这样的一条信息，有些时候我们可以忽略掉 <code>relation</code>，比如说某些词的近义词，就可以构成一个 Bag-of-Words 的词袋模型。针对这两种（后一种可以视作是三元组的退化），我所了解到的表示方式有以下几种。</p><p><strong>针对 Bag-of-Words</strong></p><p>拿着一堆词，我们能够做的事情其实并不多，离散的 word index 并不能携带什么信息，常用的方法有</p><ul><li>Embedding：借鉴 word embedding，我们可以把这些辅助的信息映射到某个低维空间，让 model 自己习得其中的语义关系。之前在一篇 aspect-level review generation 中，作者就尝试将 aspect 信息 map 到一个 embedding 空间，从而让生成评论是关于某个商品的某个层面。<strong>这种方法最大的优点就是简单</strong>，但是我个人实验的过程中会出现<strong>输出的评论欠缺多样性的</strong>问题，还有待深入研究。</li><li>Transformation：常用的将词语转换成隐表示的手段在前几年是 RNN，特别是对有时序关系的一组词，丢进 RNN 里把每个 time-step 的 hidden state 作为词语的表示即可，后续再使用 attention mechanism 来利用这些信息。而最近很火热的 Transformer 也完全能够起到这样的作用，如果词之间没有明显的时序关系，也许利用 Self-Attention 能更好地捕获词之间的关联信息，从而得到一个更好的表示。</li><li>Memory Network：记忆网络也是</li></ul><p><strong>针对知识图谱</strong></p><p>之前提到很多知识库都是基于图来进行表示的，但是直接把知识图谱给塞进神经网络还是比较困难的。我目前接触到的主要手段就是 Graph Convoluctional Network (GCN，图卷积网络)，在知识图谱上做卷积操作，得到其表示，从而辅助接下来的任务。Actually，这块我并不是很了解，国内知识图谱以及其表示工作做的比较出色的就是<a href="http://nlp.csai.tsinghua.edu.cn/~lzy/" target="_blank" rel="noopener">刘知远</a>老师了，感兴趣的读者可以到他的主页了解更多的相关工作。</p><h2 id="Integrating-Knowledge-Into-Neural-Models"><a href="#Integrating-Knowledge-Into-Neural-Models" class="headerlink" title="Integrating Knowledge Into Neural Models"></a>Integrating Knowledge Into Neural Models</h2><p>终于，历经千辛万难之后，我们到了最后一个阶段，如何把抽取得到并且有着较好表达能力的知识表示整合进神经网络模型。这里还是围绕 Choi 的两篇工作讨论，而这有两篇工作又都是围绕着菜谱。Choi 在一个 Talk 上表示她非常希望以后早上起床有机器人能给她做早饭，可能她不怎么会做菜吧哈哈哈。</p><p>第一篇文章 <a href="#Reference">Checklist Models</a> 说的是菜谱生成，给定原料列表和目标，比如我们要做一道 <code>西红柿炒蛋</code>，原料列表就是 <code>西红柿、鸡蛋</code> 等，生成的菜谱可能就是下面这种：</p><blockquote><p>1、将<strong>西红柿</strong>洗净后用沸水烫一下，去皮、去蒂，切片待用。</p><p> 2、将<strong>鸡蛋</strong>打入碗中，加盐，用筷子充分搅打均匀待用。 </p><p>3、<strong>炒</strong>锅放油3汤匙烧热，将<strong>鸡蛋</strong>放入锅中<strong>炒</strong>熟盛出待用。 </p><p>4、将剩余的油烧热，下<strong>西红柿</strong>片煸<strong>炒</strong>，放盐、糖<strong>炒</strong>片刻，倒入<strong>鸡蛋</strong>翻<strong>炒</strong>几下出锅即成。</p></blockquote><p>主要的难点在于前后文的连贯性，我们要把西红柿和番茄都得说到，并且可能要重复说，重复说的时候就要考虑到之前对番茄已经处理过了，它的位置都可能发生了变化。模型利用了和类似 <a href="#Reference">MTA-LSTM</a>  中 Coverage 机制，维护一个为使用的原料列表和使用过的原料列表，每次生成菜谱的词的时候会根据当前的 hidden state 来生成一个概率分布，是要利用原料中的词，还是使用过的词还是直接生成词：</p><p><img src="/img/ot_cl.png" alt="Output at T"></p><p>模型示意图如下：</p><p><img src="/img/checklist.png" alt="Check List"></p><p>总体来说模型不复杂，核心就是一个 Checklist，这种融合方式是<strong>启发式</strong>的，模拟人类写作的过程，而这个 Checklist 估计也是从此而来。</p><p>另外一篇文章则更多地是围绕着推理，我们在看菜谱的时候，实际上是<strong>理解一种过程语言</strong>，而过程语言的难点就是会有一些省略的指代，以及实体状态的变化。比如我们不能把鸡蛋直接搅拌，而是要打碎得到蛋液之后才能进行这样的操作。为了模拟阅读菜谱的过程，Choi 等人提出了 <a href="#Reference">Neural Process Model(NPM)</a>，模型示意图如下：</p><p><img src="/img/npm.png" alt="Neural Process Model"></p><p>菜谱的两个核心要素是 action（动作）以及 entity（原料实体），那么理解菜谱就可以认为是根据动作来对实体的状态进行改变的一个过程。基于这样一个想法，NPM 从菜谱的每一句中选择需要执行的动作，以及对应的 entity，然后交给一个 applicator 在 entity 上执行操作，从而带来相应的 state embeddings 改变。而这里的实体的状态，作者选取了一些具有代表性的比如：温度（加热之后温度会上升）、位置（把鸡蛋打碎后鸡蛋应该在碗里）。</p><p>这几步中最困难的就是选取正确的实体，而仅仅通过 sentence encoding + attention 机制是不够，因为有些时候我们会选择上一句甚至更之前出现的实体，因此，文章借鉴了之前菜谱生成的思路，每次在 hidden state 上做一个三类的分布， 分别是当前的 attention weight / 上一步的 attention weight / 以及不采用，从而将历史的 attention 信息考虑进来：</p><p><img src="/img/npm.png" alt="NPM Recurrent Attention"></p><p>模型其他的细节就请读者参考原文。文章利用在实体选择上的 F1/ ACC以及状态改变上的 F1/ACC 指标对 model 做了评估，结果也都能超过 baseline。但这篇文章的亮点并不在于取得了 SOTA 的效果，而是在于<strong>一种尝试，对于理解过程式语言的一种探索</strong>。事实上，现在 NLP Community 的关注点已经逐渐从诸如 NMT 的几个点的 BLEU 值转向让 NLP 系统更加智能的方向。这是一个好事，也许未来不久的某一天，我们就能吃上机器人做的菜了！</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://homes.cs.washington.edu/~yejin/" target="_blank" rel="noopener">Homepage of Yejin Choi</a></li><li><a href="https://www.dropbox.com/s/it1e4ndrcuevl04/Repl4NLP.pdf?dl=0" target="_blank" rel="noopener">The Missing Representation in Neural (Language) Models-Yejin Chor</a></li><li><a href="https://arxiv.org/abs/1706.03799" target="_blank" rel="noopener">VERB PHYSICS: Relative Physical Knowledge of Actions and Objects</a></li><li><a href="https://homes.cs.washington.edu/~yejin/Papers/emnlp16_neuralchecklist.pdf" target="_blank" rel="noopener">Neural Checklist Models</a></li><li><a href="https://arxiv.org/abs/1711.05313" target="_blank" rel="noopener">Simulating Action Dynamics With Neural Process Model</a></li><li><a href="https://github.com/TobiasLee/MTA-LSTM-TensorFlow" target="_blank" rel="noopener">MTA-LSTM</a></li><li><a href="http://nlp.csai.tsinghua.edu.cn/~lzy/" target="_blank" rel="noopener">Homepage of Zhiyuan Liu</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;先验知识和当前盛行的 Neural NLP Model 的结合是一条必由之路，最近接触到 &lt;a href=&quot;#Reference&quot;&gt;Yejin Choi&lt;/a&gt; 的一系列工作，并且看了她的一些 Talk（PS：真的是又有才又好看！），觉得非常有意思，将一些内容和 Paper
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>革命尚未成功，同志仍需努力</title>
    <link href="https://tobiaslee.top/2019/02/23/what-I-learned-from-intership/"/>
    <id>https://tobiaslee.top/2019/02/23/what-I-learned-from-intership/</id>
    <published>2019-02-23T15:16:38.000Z</published>
    <updated>2019-03-13T04:05:18.672Z</updated>
    
    <content type="html"><![CDATA[<p>一眨眼，35 天的寒假就过去了，我也即将踏上返校的旅途，简单地总结一下这段时间的工作和感悟。</p><h2 id="走出象牙塔"><a href="#走出象牙塔" class="headerlink" title="走出象牙塔"></a>走出象牙塔</h2><p>寒假刚开始两天，就去到之前联系的艾耕实习，继续我之前在文本生成方向的探索。</p><p>这是我第一次真正到一家公司去实习，收到第一个月工资的时候还是很激动的哈哈哈哈毕竟人生第一笔真正意义上的工资。另外，艾耕作为一家创业公司公司的氛围很不错，没有非常的 push；Mentor 们也很 nice，我也经常会和带我的组长以及 CTO 讨论。</p><p>除了工资、实习内容以外，我觉得这段时间并不长的工作经历带给的思考和感悟有：</p><ul><li>不管做什么，一定要找到有<strong>反馈</strong>的工作去做。人其实是一个很懒惰的动物，如果没有一些外界的激励，是很难让我们去做一些比较辛苦的事情。但是很多时候在学校里，是没有这种 feedback 的，或者说 feedback 要很久之后才能获得，比如学习和期末考试，中间就隔着一个学期，这似乎和<strong>延迟满足</strong>有些类似，但我觉得一种可行的方式可能是给自己设定一些 milestone，比如完成某个小目标就给自己一些 reward；在公司这种情况会稍好一些，毕竟有一份工资的激励，作为一个爱岗敬业的人自然是要努力工作的，但我也怀疑久了以后这份激励也会变成懒惰的动力，如果没有有效的考核机制的话，摸鱼也能拿钱那人人肯定都去摸鱼了，我觉得工作之后的 motivation 应该会变成<strong>算法落地所带来的成就感</strong>。</li><li>敢于尝试。其实一开始投实习我是没什么底气的，毕竟寒假时间很短，很难有什么产出。但最终还是获得了这个机会，感谢艾耕的同时我也觉得，真的还是<strong>要迈出那一步</strong>，当然了，到了岗位还是要努力工作，尽自己最大努力去探索。我也了解到，公司招实习生一方面是培养后备力量，另外一方面就是让实习生去做一些探索，能做出产品自然是最好的，没做出来也没什么大不了的，在过程中能够留下一些经验，记录下踩过的坑，也算是很重要的技术储备。</li><li>感受生活的不易。最直观的就是物价，平时吃一顿饭就是 20 起步，和学校里 8 块撑死十五的可谓天壤之别；另外，每天上下班的路程也不算近，通勤时间有 3h，早上七点起床晚上九点到家，加上这几天杭州一直下雨，高峰期甚至有过连着两班公交车都挤不上的情况，上班族真不容易啊。</li></ul><p>从象牙塔走向社会之前，能够切身地在实习生活中体会一次做社会人的滋味，还是很有意义的。</p><h2 id="仍需努力"><a href="#仍需努力" class="headerlink" title="仍需努力"></a>仍需努力</h2><p>刚拿到之前 NAACL 的投稿结果，又是被拒，好在之前两次 Journal 的被拒经历让我心理承受能力强大了很多，442 的结果对于一个第一次投顶会的菜鸡来说还是可以接受的，看了下 Review 的意见，4 分的两个说的也都很中肯；2 分居然喷 motivation 不足，好吧我服了。</p><p>Anyway，革命尚未成功，还需加把劲。这个社会还是很残酷的，我个人的看法是没有实质性的产出例如 Paper，那么很多付出不是说没有意义，而是说很难量化。希望能在不久的将来有一些成果吧。</p><p>希望新年的大家都能够年少有为！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一眨眼，35 天的寒假就过去了，我也即将踏上返校的旅途，简单地总结一下这段时间的工作和感悟。&lt;/p&gt;
&lt;h2 id=&quot;走出象牙塔&quot;&gt;&lt;a href=&quot;#走出象牙塔&quot; class=&quot;headerlink&quot; title=&quot;走出象牙塔&quot;&gt;&lt;/a&gt;走出象牙塔&lt;/h2&gt;&lt;p&gt;寒假刚开
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>编译原理课程总结</title>
    <link href="https://tobiaslee.top/2019/01/15/compile-principle-notes/"/>
    <id>https://tobiaslee.top/2019/01/15/compile-principle-notes/</id>
    <published>2019-01-15T09:13:14.000Z</published>
    <updated>2020-05-10T03:16:51.028Z</updated>
    
    <content type="html"><![CDATA[<p>本学期学习了编译原理，轮子哥口中的三大浪漫之一，复习期间写这篇文章作为梳理，并且把大作业的 Code 放到了 GitHub 上，希望对学弟学妹有所帮助（逃。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>编译说白了就是翻译，把高级语言代码翻译成的机器能够执行的机器代码，主流有两种形式：编译器和解释器。前者将源程序翻译成目标程序后再进行运行，后者则是一遍翻译一遍执行。前者典型如 C++，执行效率高，后者如 Python，灵活性好但是效率有所逊色。但实际上，现在的大趋势就是编译和解释的融合，例如 Java 就是编译成字节码后进行由 JVM 来解释执行，带来良好的跨平台性，同时也引入 JIT（just in time）技术在 JVM 执行时提高程序运行的效率。</p><p>编译大致可以分成如下几个阶段：词法分析、语法分析、语义分析、中间代码生成、代码优化以及目标代码生成，符号表管理和出错处理贯穿整个过程。如果把中间代码以前的阶段称之为前端，而其后的成为后端，便是另外一种角度下的编译器（分析/综合模式），而课程所涉及的也主要是前端部分，后端部分的优化事实上对程序有着重要的作用，希望以后有机会能进一步的探索。接下来就以课程所学的各个阶段为主线来梳理一下内容。</p><a id="more"></a><h2 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h2><p>词法分析要做的就是把文本序列转换为我们所规定的记号（token），因此其主要工作如下：</p><ol><li>处理与平台相关的输入</li><li>过滤程序中无用的部分，比如注释</li><li>识别输入序列中的记号</li><li>调用符号表管理程序和出错管理程序进行相关处理</li></ol><p>核心是第三点，而要识别文本序列中的记号，首先我们要定义规则，什么是合法的记号，然后想办法来识别它，这也就是老师常说的立法和执法。</p><h3 id="立法：记号的组成"><a href="#立法：记号的组成" class="headerlink" title="立法：记号的组成"></a>立法：记号的组成</h3><p>规则：因为记号的组成一般是字符的线性组合，因此可以使用正则表达式来进行描述，这里就不对正则表达式做过多的展开。而这种规则又可以称为模式（pattern）</p><p>记号：按照某个模式识别出的元素，一般是指一类，一般我们需要识别出记号的类别和其属性，常见的类别包括关键字、标识符、字面量以及特殊符号（+, - 等）</p><p>单词（lexeme）：识别出元素自身的值</p><p>举个例子，对于 C++ 中的关键字 <code>const</code> ，我们就需要识别出他的类别是关键字，而对于某个变量名 <code>count</code> 则要识别出类别为 <code>id</code> 并且其 lexeme 为 <code>count</code> 以便后续的符号表登记操作。</p><h3 id="执法：识别记号"><a href="#执法：识别记号" class="headerlink" title="执法：识别记号"></a>执法：识别记号</h3><p>我们使用有限自动机来识别记号，Why？先来看看自动机 M 的组成：$M = (S, \Sigma, move, s_0, F) $，这个五元组的组成：</p><ol><li>$S$：有限个状态的集合</li><li>$\Sigma$：有限个输入字符的集合</li><li>$move$：状态转移函数，表明某个状态在接收到某个字符会转向下一个状态</li><li>$s_0$：唯一的初始状态</li><li>$F$：终态集，代表了可以接受的状态的集合</li></ol><p>自动机是一种很好用的工具，根据状态转移来一步一步地建模问题。记号识别正好契合了这种方法，比如你要识别一个邮件的地址，你拿着对应的正则表达式，先识别一系列的字符组合，关键是其中的<code>@</code> 符号，最后再来形如 <code>qq.com</code> 的后缀完成识别。</p><p>有限自动机根据其状态转移函数是否是一对多的，也即同在某个状态下，对于同一个输入字符是否有多个可能的下一状态，分成不确定的有限自动机（NFA）和确定的有限自动机（DFA）。而自动机的编程实现是很简单的，每次读一个字符进行，根据状态转移函数进行状态转移，循环往复执行到最后一个输入即可。而 NFA 因其的不确定性会带来大量的回溯，一般我们都会将其确定化后形成一个 DFA 来进行实现，确定化的方法不展开，核心就是一句话：<strong>把状态转移拓展为状态集转移，把状态集之间的转移作为 DFA</strong>。</p><p>有了正则表达式和自动机，怎么把他们连起来做记号的识别呢？Thompson 可以完成正规式到 NFA 的转换，而再通过确定化 NFA 得到 DFA 即可完成记号的识别。</p><h2 id="语法分析"><a href="#语法分析" class="headerlink" title="语法分析"></a>语法分析</h2><p>有了词法分析的记号流之后，我们便要从记号流之中提取程序的语言结构，而一般会通过一颗树来表示语言的结构（在 NLP 中一般也是这么干的），这棵树我们称之为语法树；除了提供语法树给下一阶段语义分析以外，语法分析还需要检查一下语法错误，比如我们经常会漏的括号等。对于错误处理，这里对可能出现的错误简单做一下分类：</p><ol><li>词法错误：不符合构词规则，一般在词法分析阶段发现</li><li>语法错误：语法结构出错，不成句，例如缺少括号等</li><li>静态语义错误：类型不一致，参数不匹配等</li><li>动态语义错误：死循环，除数为 0 是常见的动态语义错误</li></ol><p>从错误中恢复是比较困难的，因此这里不再展开，而实现的时候一般直接报错，并且指出哪里出错即可。回到语法分析，我们还是需要关注亮点：如何描述语言的结构（立法）和有了规则之后如何识别。</p><h3 id="立法：上下文无关文法"><a href="#立法：上下文无关文法" class="headerlink" title="立法：上下文无关文法"></a>立法：上下文无关文法</h3><p>语言是非常复杂的结构，否则自然语言处理也不会被称为人工智能皇冠上的明珠了，其中包含很多的非线性结构，好在程序语言相对来说更加的精确，我们能够通过形式化的方式来描述它。常用的便是上下文无关文法（Context Free Grammar，CFG），它是一个四元组 $G = (S, N, T, P)$：</p><ol><li>S：开始符号，所有的句子都从这里开始（Start）</li><li>N：非终结符的有限集合</li><li>T：终结符的有限集合</li><li>P：产生式的有限集合，其形式为 $A\rightarrow\alpha$，$A \in N$ 以及 $\alpha \in N \cup T \cup {\epsilon} $</li></ol><p>其中对于 N 和 T 我的理解是：T 是最终我们能在记号流中见到的所有记号的类别，而 N 是描述语言产生时的中间结构，比如常见的运算就可以表示成下面这张图：</p><p><img src="/img/cfg.png" alt="CFG Example"></p><p>有了 CFG 以后，语言是怎么产生的呢？推导，也即利用产生式的右部替换产生式的左部，最终将所有的 N 都换成了 T，得到一个<strong>句子</strong>，其中推导的时候如果每次都替换最左边的非终结符，就成为最左推导，同理有最右推导，也被称之为规范推导；推导各个步骤中的中间结果称为句型。To be honest，我觉得<strong>这些概念没什么记忆的必要，纯粹是为了应付考试，了解大概意思即可</strong>。</p><p>推导中对于非终结符的替换的选择不是唯一的，因此同一个句子可能得到两颗不一样的语法树，这就是文法的二义性。<strong>其本质是文法对优先级和结合性缺少定义</strong>。如之前的 NFA 一样，我们不喜欢二义性，消除二义性的方法就是加入优先级和结合性的信息来改写文法。但消除二义性后的文化书写起来比较麻烦，并且因为引入了额外的信息，树的高度变高，也给后续的带来额外的负担。</p><p>此外，再对文法做一些展开。和前面的正则表达式相比，CFG 的表达能力是更强的，也即能够描述更丰富的结构，比 CFG 更强的有 CSG（上下文有关文法），一个很明显的例子就是计数问题，正规表达式无法表示有两个相同数量的字母句子，而 CFG 可以，CSG 进一步地可以表示三个相同数量的字母的句子。</p><h3 id="执法：识别语言结构"><a href="#执法：识别语言结构" class="headerlink" title="执法：识别语言结构"></a>执法：识别语言结构</h3><p>有了规则，下一步就是怎么做，方法大致分成两类：自上而下和自下而上。</p><h4 id="自上而下"><a href="#自上而下" class="headerlink" title="自上而下"></a>自上而下</h4><p>自上而下分析的核心就是模拟推导的过程，对于任何一个输入序列 $\omega$，从开始符号 $S$  进行推导，试探着对 $\omega$ 进行匹配。在模拟这一过程之前，我们需要对文法做一些改动，主要包括：消除左递归和消除左因子，避免带来的递归调用爆栈以及回溯。</p><p>修改好文法之后，我们就可以来编程了，利用简化的文法写出 DFA，来暴力实现。这种方法称之为递归下降子程序法，适合手动编写简单的语言解释器，我们的大作业便是采用这种方法，但不适合大型的项目和自动生成。因此我们还有另外一种方法：预测分析器。</p><p>预测分析器使用一个下推自动机，就是带有数据栈的有限自动机来完成推导的过程。工作方式就是从初始的某个格局开始，经过一系列变化达到最终格局，如果是接收格局那么就 ok 否则报错。格局是一个三元组：（栈内容，当前剩余输入，改变格局的动作）。改变格局的动作分为三类：</p><ol><li>匹配终结符：从当前剩余输入中拿出一个终结符吃掉</li><li>展开非终结符：模拟推导的过程</li><li>成功接收 / 出错</li></ol><p>这里的核心是一张预测分析表，来指导下推自动机的工作。预测分析表说的就是：当你栈顶的非终结符看到一个终结符的时候你应该转向哪一个状态（或者说进行哪一步推导）。而如何确定，则是通过计算非终结符的 FIRST 和 FOLLOW 集合来完成这张表的填写，简单来说，FIRST 集合中是该非终结符推导出的所有句子的起始终结符，而 FOLLOW 集合中则是紧跟着该非终结符的终结符，具体的细节也不再赘述。</p><h4 id="自下而上"><a href="#自下而上" class="headerlink" title="自下而上"></a>自下而上</h4><p>自上而下的思路是模拟推导，从开始符号开始，把非终结符展开，最终得到一个完整的句子；那么反过来，从句子出发，不断地进行推导的逆过程——归约（将终结符变为非终结符），最终得到一个开始符号，也是一种可行的方法。因此，移进-归约分析应运而生。</p><p>还是借助一个下推自动机，运用格局和一张分析表来驱动语言结构的识别，关键就在于我怎么知道什么时候该进行归约？又该归约成什么非终结符？</p><p><strong>归约</strong>（reduce）是推导的逆过程，而如果将某个句型（或则是句子）的分析树画出来，我们可以发现：</p><p><img src="/img/fxs.png" alt="分析树"></p><p>每次归约，就是把一个子树的叶子节点减掉，而为了规范，我们一般从最左边的最小子树（高度为2）的开始，这个操作就是<strong>剪句柄</strong>。想想一下最右推导，我们最后才会把最左边的非终结符展开成终结符，而剪句柄，则是每次都把句子的最左边归约成一个非终结符。<strong>最右推导和最左归约互为逆过程，也正好是自上而下和自下而上的体现</strong>。</p><p>回到归约的下推自动机，其格局类似之前的预测分析器，只不过改变格局的动作由匹配终结符和展开非终结符换成对应的移进和归约。但是还是那两个问题，我怎么知道我现在该移进还是规约？这张分析表的构造的核心在于<strong>识别活前缀</strong>，即某个句型的开头序列是什么。而要识别活前缀，还是要需要我们的老朋友 DFA 以及项目（<strong>带有已经见到部分标记的产生式</strong>）来完成。这里不再展开，因为我觉得这个属于技术细节，核心的东西在于前面的思想，搞清楚了思想剩下的就是模拟机器执行的过程，没必要过于纠结。</p><h2 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h2><p>拿到了语法分析产生的语法树，到了语义分析阶段我们就可以<del>为所欲为</del>做一些跟最终结果相关的工作了。而对于一个解释器来说，在拿到语法树之后，就可以执行相应的语义动作了，这也是我们在最终大作业<strong><a href="https://github.com/TobiasLee/Draw-Language-Interpreter" target="_blank" rel="noopener">一个简单绘图语言的解释器</a></strong>中所采用的做法，就是把对应的图给画出来。</p><p>一般的编译器在语义分析阶段需要完成的工作是：检测表达的语义是否合法以及执行相应的语义动作——生成中间代码。</p><p>中间代码，前面也提到了，是连接前端和后端的桥梁。主要的形式有树、后缀式以及三地址码。树和后缀式之前在数据结构都有接触过，而三地址码则是形如 <code>a := 1</code> 这样形式的代码，和汇编非常的接近，一般可以用一个四元组 <code>(op, arg1, arg2, result)</code> 来表示，拿 <code>a = 1 + 2</code> 来说，就会生成 <code>(+, 1, 2, T1)</code> 以及 <code>(:=, , T1, T2)</code>  这样两条代码。同样的还有三元组，把存放结果的第四个 <code>result</code> 参数省略而通过三元式的位置来确定存储位置，毫无疑问，这样做会给后面的代码优化带来很大的麻烦，因此多用四元式。</p><p>执行语义动作除了生成中间代码以外，一般还会在对应的文法符号（比如非终结符上）记录一些信息，称作属性。属性可以分为继承属性和综合属性，前者需要父结构的信息，而后者需要子结构的信息。属性的计算方案根据抽象程度不同可以分为<strong>语法制导定义</strong>和<strong>翻译方案</strong>，这二者就类似算法和具体的代码，前者是一个框架流程，后者则是关系到具体的平台和语言实现。</p><p>总的来说，语义分析做的事情就是拿着语法树执行动作，而还有一些内容比较零散，我就挑一些我觉得比较重要的来说。</p><h3 id="嵌套"><a href="#嵌套" class="headerlink" title="嵌套"></a>嵌套</h3><p>变量或者过程嵌套的原则：静态作用域原则和就近嵌套原则。这两个原则其实我们已经很习惯了，但单独拿出来说是想说明一下实现的思路。<strong>栈</strong>，后进先出，和这个嵌套的关系刚好契合，每次我们遇到一个作用域和其中的变量（过程同理），就可以把它压栈，查找的时候沿着栈顶向里查找即可。</p><h3 id="过程调用方式"><a href="#过程调用方式" class="headerlink" title="过程调用方式"></a>过程调用方式</h3><p>这就是我们之前经常遇到的 C 语言中的 <code>swap</code> 函数遇到的问题：值传递还是引用传递。在编译原理中，我们强调了<strong>左值</strong>和<strong>右值</strong>的区别，左值是一个有存储空间的变量，是一个容器，而右值则是内容，比如常见的字面量 1、123。值传递是右值的传递（将实参的内容交给形参），引用传递则是左值（将实参的地址填到形参的地址中），所以后者在函数/过程内部修改形参也会对实参产生影响。此外复写-调用方式是二者的一个结合，在函数体内部是值传递，而在函数返回的时候会把形参的值传回的实参。</p><h3 id="拉链-回填技术"><a href="#拉链-回填技术" class="headerlink" title="拉链-回填技术"></a>拉链-回填技术</h3><p>在翻译一些控制语句的时候，例如 if / while 等，我们需要考虑的一个核心问题就是：下一条在哪里？例如，对于 <code>if x &gt; 0 then x := a + 1 else x := 2</code> 这样一条语句，在一遍自上而下的扫描的过程中，我们看到当前的语句 <code>if x &gt; 0</code> 能确定的是条件为真时要转向 <code>then x:= a + 1</code>  （可以认为是紧接着的下一条指令），但是为假的出口并不能马上确定，因为<strong>then 语句中可能做很多操作，不是简单的下一条</strong>，必须要到我们看到 <code>else</code> 的时候才能确定。而延迟这种条件转向语句的填写的技术就是拉链回填技术。说白了，就是我现在不知道，先记着，到了合适的时候再回过头来的填之前的漏。而<strong>如何设计回填的时机</strong>是比较 tricky 的，比如前面的看到 <code>else</code> 我们就知道这里可能需要回填一波之前的条件为假的出口，设计的时候需要仔细的考虑。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>一开始学编译原理，觉得是非常高大上的一门课，而随着抽丝剥茧把前端各个阶段分开来，一点一点学起来，还是非常有趣的。并且亲手写了一个简单的解释器，成就感还是爆棚的。此外，也切实感受到软件工程的重要性，如果不能够把任务分解成可实现的小块，而是一上来丢给你一个<strong>写一个解释器</strong>的大目标，是很难有信心完成它的。</p><p>另外，明天就考试了，祝自己好运！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本学期学习了编译原理，轮子哥口中的三大浪漫之一，复习期间写这篇文章作为梳理，并且把大作业的 Code 放到了 GitHub 上，希望对学弟学妹有所帮助（逃。&lt;/p&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;编译说白了就是翻译，把高级语言代码翻译成的机器能够执行的机器代码，主流有两种形式：编译器和解释器。前者将源程序翻译成目标程序后再进行运行，后者则是一遍翻译一遍执行。前者典型如 C++，执行效率高，后者如 Python，灵活性好但是效率有所逊色。但实际上，现在的大趋势就是编译和解释的融合，例如 Java 就是编译成字节码后进行由 JVM 来解释执行，带来良好的跨平台性，同时也引入 JIT（just in time）技术在 JVM 执行时提高程序运行的效率。&lt;/p&gt;
&lt;p&gt;编译大致可以分成如下几个阶段：词法分析、语法分析、语义分析、中间代码生成、代码优化以及目标代码生成，符号表管理和出错处理贯穿整个过程。如果把中间代码以前的阶段称之为前端，而其后的成为后端，便是另外一种角度下的编译器（分析/综合模式），而课程所涉及的也主要是前端部分，后端部分的优化事实上对程序有着重要的作用，希望以后有机会能进一步的探索。接下来就以课程所学的各个阶段为主线来梳理一下内容。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Compile Principle" scheme="https://tobiaslee.top/tags/Compile-Principle/"/>
    
      <category term="Course Notes" scheme="https://tobiaslee.top/tags/Course-Notes/"/>
    
  </entry>
  
  <entry>
    <title>Goodbye 2018，Hello 2019</title>
    <link href="https://tobiaslee.top/2018/12/31/2018-summary/"/>
    <id>https://tobiaslee.top/2018/12/31/2018-summary/</id>
    <published>2018-12-31T01:50:44.000Z</published>
    <updated>2018-12-31T02:09:41.678Z</updated>
    
    <content type="html"><![CDATA[<p>这是年终总结系列的第三篇了，翻看之前写的总结，发现所有对未来的规划都赶不上变化。但总结还是要做，因为<strong>生活总是需要一些仪式感的东西来记录你曾经来过</strong>。</p><p>看 17 年的总结，发现 18 年的主旋律就是向成为一名 NLPer 转变：</p><ul><li>Motivation 的转变：之前支持着我的驱动力大约是高考失利的不甘，而上次和 AiTing 姐聊人生吹牛的时候我说：”我们很有幸生活在这样一个时代，一个也许是属于 AI 的时代；也很有幸能够从事这样一个行业，一个可能能够真的让世界变好一点的行业。“虽然有些鸡汤，但是我觉得为什么不试试呢？</li><li>读了大概 200 + 的 Paper，养成定期刷 arixv 以及扫货各大顶会感兴趣的文章的习惯，投了两篇论文出去。算是明确以后自己要做什么，也大致知道该怎么做。</li><li>DeeCamp：结识了很多朋友以及学到很多东西，并且成功勾搭到老师，进组实习。</li><li>维护自己的 GitHub 和 Blog：博客的访问量破 8k，之前的文本分类项目 Star 数快破 300 了（窃喜），以及开源了一些项目和论文复现的代码，认真回复每一个 Issue。</li><li>读书和观影：看的不少，但自己没有写读书笔记/观后管的习惯，只大概觉得不少小说很不错以及今年的好电影不少，特别是国产片。</li></ul><p>16 年刚入学的我只想在毕业之后赶紧就业，<del>找个女朋友</del>，而 18 年快要过去，谁能想到现在的我居然想用学的知识努力让世界变好一点点？大约是 Andrew Ng 太过洗脑了吧（逃</p><p>总之，18 年还算是收获蛮多的一年，满分 100 分的话，我给 80！</p><p>按照惯例，规划一下未来：</p><ul><li>做好实习的工作，争取能够再投一篇文章</li><li>能够在 10 月份去到想去的学校</li><li><strong>锻炼身体</strong>，坐的太久身体真的快要不行了，各位小伙伴也一定要保重身体</li><li>学习一下语言学的知识</li><li>说话做事的时候多思考一下，考虑他人的感受</li></ul><p>以上，感谢身边所有的同学、朋友的陪伴，以及看到这里的你，祝大家新年快乐，万事顺意！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是年终总结系列的第三篇了，翻看之前写的总结，发现所有对未来的规划都赶不上变化。但总结还是要做，因为&lt;strong&gt;生活总是需要一些仪式感的东西来记录你曾经来过&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;看 17 年的总结，发现 18 年的主旋律就是向成为一名 NLPer 转变：&lt;
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>从 Transformer 说起</title>
    <link href="https://tobiaslee.top/2018/12/13/Start-from-Transformer/"/>
    <id>https://tobiaslee.top/2018/12/13/Start-from-Transformer/</id>
    <published>2018-12-13T01:36:43.000Z</published>
    <updated>2018-12-21T02:56:09.922Z</updated>
    
    <content type="html"><![CDATA[<p>从我开始学习 NLP 以来，带有轰动效应的 Paper 大概就是 Transformer 和 BERT，前者是在机器翻译领域全面超过原先的 Seq2Seq，而后者则是继 Word Embedding 作为 representation 之后的又一突破。而实际上 BERT 的提出也是建立在 Transformer 的基础上。这篇文章就是围绕这两篇 Paper 的笔记以及一些思考。就从 Transformer 说起吧！</p><a id="more"></a><h2 id="Transformer-的细节"><a href="#Transformer-的细节" class="headerlink" title="Transformer 的细节"></a>Transformer 的细节</h2><p>Transformer 的 model，主要就在于 Multi-Head Attention：</p><p><img src="/img/mh-attn.png" alt="Multi-head Attention"></p><p>而在这之前文章先向我们介绍了 attention weight 的两种计算方法：</p><ol><li>最初的注意力机制中，利用一个 feed-forward network，也即 Bahdanau 提出的 alignment function：$a(s_{i-1}, h_j)$</li><li>点乘式计算：通过 Query 和 Key 计算一个 Dot-product（可以认为是计算关联度），然后 Softmax 得到权重</li></ol><p>二者方式对比：</p><ol><li>二者复杂度接近，点乘方法可以通过矩阵计算的优化来实现快的速度</li><li>点乘可能因为某个位置的元素过大而在 softmax 之后进入梯度的死区，即没有梯度，因此作者通过一个 $\sqrt{d_k} $ 做了 scale 来避免这样的情况</li></ol><p>回到正主 Multi-head attention，先是 single head 的计算方式：</p><p>$ Attention(Q, K ,V) = softmax( \frac{QK^T}{\sqrt{d_k}} )V$</p><p>Multi-head 就是重复以上过程多次，但是每次先对 $Q$，$K$ 和 $V$ 做一个线性变换，也就是乘个矩阵：</p><p><img src="/img/mh-attn2.png" alt="Multi-Head Attention Formula"></p><p>美名其曰：</p><blockquote><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</p></blockquote><p>在我看来，上面的解释有点牵强，但是也勉强说服我了，反正你算的快多算几次总是有好处的…</p><p>而在应用过程中，有下面一些细节：</p><ul><li>Decoder 可以对 Encoder 的所有输出进行 Attention，和 Seq2Seq 中的做法一样</li><li>Encoder 内部有 Self-Attention，即能够 Attend 到之前 layer 所有的输出，并且没有<strong>位置的限制</strong>，即某个位置可以 attend 所有位置</li><li>Decoder 内部也有 Self-Attention，即能够 Attend 到之前 layer的输出，但是只能是当前位置以及之前的输出，这很好理解，翻译的时候后面的还没翻译出来呢。怎么实现，在 softmax 的输入上做一个 mask，把不能 attend 位置都 mask 成 $-\infty$ 就 ok</li></ul><p>另外一个比较有趣和难理解的点就是 Positional Encoding，有趣是在于因为之前的 Attention 机制都没有考虑到序列的时序信息，因此作者在 Embedding 上额外增加了两个 sin 和 cos 信号：</p><p>$PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})$</p><p>$PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})$</p><p>我第一遍读的时候也是很困惑，为什么这样能够增加时序的信息？在学习过信号与系统之后，尝试解释一下：首先，PE 的变量有两个，分别是 pos 以及维度 i，对于某一个位置的某一个维度，其三角函数信号的 $\omega$ 是唯一的，而利用傅里叶变换将频率转到时域，其就是一个冲击信号，冲击信号的位置也因 $\omega$ 的唯一而是唯一的，所以每个位置每个维度都有自己的独特的值，这样就能够引入时序的信息（不同pos冲击信号的位置不同，加的不一样）。</p><p>机器之心上也有对此的讨论，摘取一个理解以供参考：</p><blockquote><p>这个 position embedding 设置得很有意思，我尝试解释一下：每两个维度构成一个二维的单位向量，总共有 d_model / 2 组。每一组单位向量会随着 pos 的增大而旋转，但是旋转周期不同，按照论文里面的设置，最小的旋转周期是 2pi，最大的旋转周期是 10000 x 2pi。至于为什么说相邻 k 步的 position embedding 可以用一个线性变换对应上，是因为上述每组单位向量的旋转操作可以用表示为乘以一个 2 x 2 的旋转矩阵。 </p></blockquote><h2 id="Transformer-的能与不能"><a href="#Transformer-的能与不能" class="headerlink" title="Transformer 的能与不能"></a>Transformer 的能与不能</h2><p>前面讨论了 Transformer 的模型上的一些，事实上还有很多训练的 trick，就不再一一赘述了。进一步地，来看看 Transformer 给我们带来了什么，首先是计算效率的大大提高：</p><p><img src="/img/comparison_transformer.png" alt="Comparison"></p><p>上面这张表给出了几种 Model 的对比，序列操作数量 RNN 毫无疑问是 O(n)，再来看看剩下两种：</p><ol><li><p>Maximum Path Length：这一点是序列中两个元素交互（interaction）的路径最长长度，对于 RNN 来说，句首的信息要传递到句尾，需要经过 n 次 RNN 的计算；而 Self-Attention 可以直接连接任意两个节点；CNN 的交互则类似一颗 k 叉树（卷积核大小 k）。下面这张图更为形象：</p><p><img src="/img/comparison2.png" alt="Comparison2"></p></li><li><p>每层的复杂度：为什么 Self-Attention 是 $O(n^2<em>d)$ 呢，借助上面的图：每个位置的 token 都需要对其他位置的 token 计算一次 d 维的 representation，而为什么这里的 RNN 是$O(n</em>d^2)$ 呢，因为这里考虑的是多层 RNN 堆叠的情况，那么对于一层中的一个 token，其接收到本层的某个位置信息的操作是 $O(n*d)$，而对于其之前层产生的信息则相当于再进行了一次 hidden state 的计算，需要额外乘上 d，因此是 $d^2$</p></li></ol><p>这张表的告诉我们：Transformer 因为其可以并行，并且在限制 attention 的范围之后计算效率很高。除了计算效率高以外，Transformer 因其 node 之间交互的路径较短，长距离的依赖信息丢失的问题相比 RNN 会好很多，因此可以通过很多手段把 Transformer 做的很<strong>深</strong>，从而带来在 NMT 问题上 state-of-the-art 的效果。网络深了带来的好处就是模型容量变大，可以捕获到更多的细节，文本的表示也能够因此变得丰富，这一点也就是后续 BERT 工作的基础；但是一个很深的网络的 training 除了计算量以外，也是很难调的，所以文章也有非常复杂的 learning rate 调整公式。</p><p>总结一下：Transformer 可以提供高效率的计算，更好地捕获文本的表示。</p><p>以上这两点应该就是 Transformer 带来的主要好处了，然后有人就会问，那么 RNN 是不是就可以被取代了呢？有研究者就开始思考 RNN，或者说这种循环结构能够带来什么好处？<a href="#Reference">[1]</a> 中就对比了 RNN 结构和 Transformer 捕获文本层次结构的能力。文章用了两个问题来研究所谓的文本层次结构（Hierarchical 这个东西在文本中到底指什么？有待进一步的补一些语言学的知识）：主谓一致和逻辑推理。主谓一致任务就是我们经常做的语法题，让你选一个词来使其的单复数形式和主语相符；而逻辑推断则是使用简单的一些人工定义语言来进行一些关系的推断，感兴趣的可以参考原 Paper。来看看在主谓一致任务上二者的表现：</p><p><img src="/img/fan_rnn.png" alt="Transformer v.s RNN"></p><p>从图中可以看出，在这个任务上 RNN 可以说是完胜 Transformer 的，尤其是当任务变得困难的时候：主语和谓词的距离变长，attractor 的数量增加（可以认为是用于混淆的主语）等都会拉大二者的差距。文章的解释是 LSTM 被强迫压缩句子的信息到一个 vector 中，因此能够更好地捕获结构信息；而 Transformer 能够获取所有的历史信息，所以可能在这方面有所忽视。但是，这里的 head 数目最大为 4，不知道增加 multi-head 的数量会不会对 Transformer 的性能有所帮助。</p><p>在逻辑推断上二者的表现也同样是 RNN 优于 Transformer，作者也基于此提出了循环结构可能是捕获层次化信息的一个关键，但是不要笃信这个结论，说不定调调 Transformer 的参数性能就上去了。但这依旧是一个值得探索的方向，探索 RNN 相比 Transformer 更擅长哪些任务。</p><p>另外，<a href="##Reference">[2]</a> 中对 Transformer 中用到的很多 trick 做了详尽的 study，把其中的各个部件进行任意组合，弥补了 Transformer 缺少 Ablation 实验的遗憾。文章也提出了三个 Key Point：</p><ol><li>Source 端对 Encoder 最下面的几层做 Attention 没什么用：可能因为下层的表示还都太浅，无法捕获高层语义信息</li><li>多头 Attention 和 Residual Connection 是 Transformer Work 的关键</li><li>相比 Decoder 端，Source 端的 Self-Attention 起到的作用更大</li></ol><p>同时文章也提到，RNN 的结构能够从 Residual Network 和 Multiple Souce Attention 获益很多，或许可以在 RNN 上采用一些 Transformer 的组件，也是一个值得探索的方向。</p><h2 id="BERT-之前"><a href="#BERT-之前" class="headerlink" title="BERT 之前"></a>BERT 之前</h2><p>深度学习可以认为由两部分组成：表示学习（如何表示数据）和归纳偏好学习（根据特定的场景利用特定的 feature，引导 model），而 BERT 则就可以说是 NLP 表示的新以里程碑。但在这之前，其实就有类似的工作了，比如 ELMo，NAACL 2018 的 Best Paper，ELMo 和 BERT 差的也就是个 Transformer，所以可以看做是 Transformer 和 BERT 的桥梁，在 讨论 BERT 之前，先来看看 ELMo 究竟做了什么。</p><p><img src="/img/elmo.png" alt="ELMo"></p><p><em>图来自 <a href="##Reference">4</a></em></p><p>一言以蔽之：利用一个多层 biLSTM 训练 Language Model 得到各个 word token 的 hidden state，将其拼接起来作为表示。</p><p>Language Model 的任务就是根据前面的词预测后的词，文章用了一个双向语言模型，即也考虑从后向前的预测，来获取更多的下文信息；这里的多层可以如图中所示，可以认为是考虑不同层面的信息：语义、句法特征；实际使用的时候再结合具体任务进行 scale 以及加权：</p><p><img src="/img/elmo_formula.png" alt="Scale and Weighted Sum"></p><p><strong>不同的任务对于词表示的特征，是有不同的需求的</strong>，这就是我们之前说的归纳偏好（inductive bias），例如，sentiment analysis 可能就会对词的语义层面信息更加关注，这个词是积极的还是消极的；而生成词的时候会考虑一些句法上的信息，这个词的词性是什么？利用这个不同层表示的加权和，可以灵活的调整模型的归纳偏好，从而适应下游的任务。</p><p>带来什么好处呢？主要有两点：词的表示携带更多的信息以及解决词的多义问题。前者，可以提升各种 NLP 任务的性能：</p><p><img src="/img/elmo_impv.png" alt="Improvement ELMo"></p><p>而后者，正是因为词的表示是 deep contextualized 的，因此能够和具体语境结合，表达词的不同含义：</p><p><img src="/img/elmo_poly.png" alt="Polysemy ELMo"></p><p>如此 fancy 的结果，也难怪能够斩获 NAACL 的 Best Paper。如果我们把 biLSTM 换成抽特征能力更强的 Transformer，是不是又会更上一层楼呢？于是，BERT 应运而生。</p><h2 id="BERT-的细节"><a href="#BERT-的细节" class="headerlink" title="BERT 的细节"></a>BERT 的细节</h2><p>BERT 对于前辈 ELMo 的 Language Model 有些不满意，认为其只能单向预测 next token 限制了 Model 抽信息的能力，并且没有捕获 sentence level 的关联信息，因此提出了下列两个任务来进行 pre-train：</p><ol><li><p>Masked LM：随机把句子中的一部分（15%） token 替换为 <code>[MASK]</code>，让 Model 来预测这个 token 原本到底是什么。在实作上有一些在替换的时候用的trick 来避免之后进行下游任务时 model 完全没有见过 token 的情况：</p><ol><li>80% 的概率，正常替换</li><li>10% 的概率，保持 token 不变 </li><li>10% 的概率，把 token 替换成词表随机的 token</li></ol><p>这样的做能够强迫 model 保持对每个 Token 的上下文都有建模，从而产生一个好的表示，否则很难正确预测被 mask 的 token；另外，也因为每次只 mask 了 15%，带来收敛速度的下降。</p></li><li><p>Next Sentence Prediction（NSP）：NLP 的有些任务需要考虑的是句子间的 relation，例如 QA，NLI 等，这也是一种 inductive bias 的体现。为了建模这一点，BERT 将句对拼接起来，判断他们到底是不是正确的组合，并且通过随机拼接构造负例，用一个二分类任务来让模型学习到句子之间的关系。</p></li></ol><p>最后，把 Mask LM 的 MLE loss 和 NSP 的 MLE loss 结合作为 pre-train 的总 loss，来达到同时获取 token level 以及 sentence level 的表示的目标。</p><p>此外，在输入的表示上，BERT 也和之前 ELMo 有所不同：</p><p><img src="/img/bert_repr.png" alt="BERT-Representation"></p><p>除了 token 本身的 embedding 信息以外，还有用于句对的分割句子信息以及Position encoding 信息（这里采用的就是 learned position encoding 了，而不是 transformer 的三角函数），其中一个特别的 <code>CLS</code> 是用于分类任务的输出：</p><p><img src="/img/bert_downstream.png" alt="BERT Downstream Task"></p><p>在前两个分类任务上（前者是句对分类，e.g.  NLI，后者是单句分类，e.g. 情感分析），都是将这个标识的经过 BERT 得到的 representation 取出来，再加上全连接层进行分类的 fine-tune；而对于 QA 任务，则是将 paragraph 段对应的输出作为表示进行 answer 的生成，NER 任务也是类似，这两个任务都不再是单个句子的表示，而会利用每个 token 的信息。</p><p>至此，BERT 的全貌其实就已经在我们的眼皮底下：</p><p><img src="/img/bert_model.png" alt="BERT Model"></p><p>当然，不可能所有的细节都被我 cover 到，最好的老师还是原文 <a href="##Reference">[5]</a>。</p><h2 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h2><p>整篇文章下来，我们可以得出一个结论：<strong>Transformer 很强</strong>，强大之处在于其能够通过做的很深，并且考虑 token 之间丰富的 interactions 来捕获很多信息，得到一个很好的表示。而有了一个好的表示之后，如何<strong>整合具体的 inductive bias</strong>，例如 QA，NLI 需要的句子级别的 relation，就如 BERT 一样加入句对预测任务来引导模型去做这方面的模式识别，是很值得继续深挖的。从 ELMo 到 BERT，可以说是预训练或者说是迁移学习在 NLP 的一次胜利，相信会有更多的工作会涌现出来。正如 Ruder 在 <a href="##Reference">[6]</a> 中所说，下一波浪潮很有可能就是迁移学习在各个领域的开花结果。</p><p>我们能做的，就是努力翻出一颗属于自己的小浪花！</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://arxiv.org/abs/1803.03585" target="_blank" rel="noopener">The Importance of Being Recurrent for Modeling Hierarchical Structure</a></li><li><a href="http://aclweb.org/anthology/P18-1167" target="_blank" rel="noopener">How Much Attention Do You Need ? A Granular Analysis of Neural Machine Translation Architectures</a></li><li><a href="http://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a></li><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li><li><a href="http://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href="http://ruder.io/transfer-learning/" target="_blank" rel="noopener">Transfer Learning - Machine Learning’s Next Frontier</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从我开始学习 NLP 以来，带有轰动效应的 Paper 大概就是 Transformer 和 BERT，前者是在机器翻译领域全面超过原先的 Seq2Seq，而后者则是继 Word Embedding 作为 representation 之后的又一突破。而实际上 BERT 的提出也是建立在 Transformer 的基础上。这篇文章就是围绕这两篇 Paper 的笔记以及一些思考。就从 Transformer 说起吧！&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="Attention" scheme="https://tobiaslee.top/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>定制 RNN Cell</title>
    <link href="https://tobiaslee.top/2018/11/02/customized-RNN-cell/"/>
    <id>https://tobiaslee.top/2018/11/02/customized-RNN-cell/</id>
    <published>2018-11-02T13:31:31.000Z</published>
    <updated>2018-11-03T00:33:17.349Z</updated>
    
    <content type="html"><![CDATA[<p>之前写过一篇 IndRNN 的文章，第一次照着开源代码实现了自己定制 RNN，其实也很简单，就是把 RNN 实现里的 <code>__call__</code> 函数进行修改，每一个时间步的全连接 <code>matmul</code> 改成 element-wise 的 <code>*</code> 即可，IndRNN 的<a href="https://github.com/batzner/indrnn" target="_blank" rel="noopener">GitHub</a>，是一个很不错的练手参考。最近做文本生成的时候遇到两个经典模型，在复现的时候同样是需要对 RNN 的结构进行改动，这篇文章就记录实现的一些细节。两个模型可以分成两类：在时间步的输入上进行操作和修改 Cell 内部结构。</p><h2 id="Add-Extra-Input"><a href="#Add-Extra-Input" class="headerlink" title="Add Extra Input"></a>Add Extra Input</h2><p>第一个模型是 <a href="http://ir.hit.edu.cn/~xcfeng/xiaocheng%20Feng&#39;s%20Homepage_files/final-topic-essay-generation.pdf" target="_blank" rel="noopener">MTA-LSTM</a>，IJCAI 2018 的一篇关于主题写作的文章，也有开源<a href="https://github.com/hit-computer/MTA-LSTM" target="_blank" rel="noopener">实现</a>，但是因为其代码版本比较老，并且 inference 阶段的代码比较难并行无法和现有的 <code>seq2seq</code> 的接口结合使用。So，需求就是实现其代码并且尽可能地需要和<code>seq2seq</code> 接口符合以便于后续的使用。模型的结构如下：</p><p><img src="/img/MTA-LSTM.png" alt="MTA-LSTM"></p><p>论文的核心思想如下：</p><ol><li>根据主题词进行文章写作，利用 LSTM 作为生成器</li><li>在每个时间步，将 Topic Word Embedding 和 Inputs 拼接在一起交给 LSTM 进行生成</li><li>为了控制主题信息的流动，会维持一个 Coverage Vector，来表明哪些主题已经使用过哪些尚未被表达出来，从未让写出的文章主题信息更加明确</li></ol><p>我的实现版本代码已经放在了 <a href="https://github.com/TobiasLee/MTA-LSTM-TensorFlow" target="_blank" rel="noopener">GitHub</a>，下面主要记录一下实现的思路。</p><p>首先明确一点，为了和现有的 <code>seq2seq</code> 接口匹配，必然是无法去修改 LSTM Cell 的 <code>__call__</code> 函数的，因为目前的 <code>dynamic_decode</code> 函数对其中的 <code>cell</code> 每一个时间步都会调用 <code>cell(inputs, state)</code> ，也即我们无法在函数参数列表中增加我们想要的额外的参数。怎么做呢？利用 <strong>Wrapper</strong>！类似装饰者模式，我们在 LSTM Cell 外面包上一层，通过这一层来对 LSTM 的输入进行操作。下面是代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MTAWrapper</span><span class="params">(RNNCell)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cell, topic, v, uf, query_layer, memory_layer, mask=None, max_len=<span class="number">100</span>, attention_size=<span class="number">128</span>, state_is_tuple=True</span></span></span><br><span class="line"><span class="function"><span class="params">                 )</span>:</span></span><br><span class="line"><span class="comment"># 实际的 LSTM 由 cell 完成</span></span><br><span class="line">        self._cell = cell</span><br><span class="line">        self.topic = topuc <span class="comment"># topic embedding</span></span><br><span class="line">        <span class="comment"># initilize coverage vector</span></span><br><span class="line">        self.coverage_vector = array_ops.ones([self.batch_size, self.num_keywords])</span><br><span class="line">        <span class="comment"># get some weight variables from params</span></span><br><span class="line">        <span class="comment"># compute res </span></span><br><span class="line">        res1 = tf.sigmoid(</span><br><span class="line">            tf.matmul(tf.reshape(self.topic, [self.batch_size, <span class="number">-1</span>]), self.u_f))  <span class="comment"># batch_size x num_keyword</span></span><br><span class="line">        self.phi_res = self.seq_len * res1  <span class="comment"># batch_size x num_keywords</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">        c_t, h_t = state  <span class="comment"># h_t batch_size x hidden_size</span></span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(<span class="string">"topic_attention"</span>):</span><br><span class="line">            <span class="comment"># Attention mechanism based on coverage vector to compute mt </span></span><br><span class="line">            <span class="comment"># update coverage vector</span></span><br><span class="line">            self.coverage_vector = self.coverage_vector - score / self.phi_res</span><br><span class="line">        <span class="keyword">return</span> self._cell(tf.concat([inputs, mt], axis=<span class="number">1</span>), state)</span><br></pre></td></tr></table></figure><p>为了节约篇幅，省去一些代码，主要看两个部分：</p><ol><li><p><code>__init__</code> 函数：我们在这里传入了大量参数，其中包括了:</p><ol><li>cell：实际 RNN 的操作是由这个 LSTM cell 完成的，我们的 wrapper 只是夹在中间进行一些<strong>小小的修改</strong>。</li><li>主题词的 embedding：这是每一步计算需要用到的信息，并且是固定。</li><li>一些要使用到的 variable：为什么 variable 从外部传入而不是定义在 <code>__call__</code> 函数之内呢？<strong>因为  training 和 inference 阶段，我们会重新 wrap 一下我们的 cell</strong>，至于为什么，先按下不表。如果在内部定义的话，则 training 阶段学到的权重无法在 inference 阶段使用（其 name scope 是不一样的），inference 使用的依旧是初始化得到的权重，等于白学。</li></ol></li><li><p><code>__call__</code> 函数：在这里我们进行每个时间步的 <code>mt</code> 的计算，并且将其和 inputs 进行拼接作为额外的信息，其中计算的细节就是对照公式进行实现的。这里有个坑需要谈一下：<strong>TensorFlow 不支持一个 Tensor 出现在多个 Loop 中</strong>。而如果我们之前不对 inference 阶段的 cell 重新进行包装的话，则 coverage vector 将会出现在 training 和 inference 的循环中，无法构建计算图。为此，我们才需要对 cell 进行重新 wrap：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">training_cell = MTAWrapper(self.decoder_cell, topic_embedded,  self.v, self.uf, self.query_layer, self.memory_layer, mask=masks)</span><br><span class="line">decoder_pt = tf.contrib.seq2seq.BasicDecoder( cell=training_cell, helper=helper_pt, initial_state=self.initial_state, output_layer=self.output_layer)</span><br><span class="line"><span class="comment"># 重新 wrap，并且把之前习得的 weight variable 通过构造函数传入</span></span><br><span class="line">infer_cell = MTAWrapper(self.decoder_cell, topic_embedded, self.v, self.uf, self.query_layer, self.memory_layer, mask=masks)</span><br><span class="line">decoder_i = tf.contrib.seq2seq.BasicDecoder( cell=infer_cell, helper=helper_i, initial_state=self.initial_state, output_layer=self.output_layer)</span><br></pre></td></tr></table></figure><p>所以我们才会需要重新传入 weight variables。最后，计算完 <code>mt</code> 之后，直接拼接交给 <code>cell</code> 去调用每一时间步就 ok。</p></li></ol><h2 id="Add-Extra-Gate"><a href="#Add-Extra-Gate" class="headerlink" title="Add Extra Gate"></a>Add Extra Gate</h2><p>对 LSTM 的结构进行修改，听着很有挑战性吧，这次不仅是要增加一个额外的输入，并且还需要添加一个额外的门来控制额外输入信息的流动。<a href="http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP199.pdf" target="_blank" rel="noopener">SC-LSTM</a> 是一篇发表在 EMNLP 2015 上的文章，也是做类似的主题生成问题，模型图如下：</p><p><img src="/img/SC-LSTM.png" alt="SC-LSTM"></p><p>这里的额外输入就是 action vector，一个 one-hot 的向量来表明主题信息，额外的 gate 就是图中的 $r_t$，其对输入的 action vector 根据当前的输入以及之前的 hidden state 计算一个值，控制 topic 信息的流动，最后这一信息经过 gate 之后会参与的新的 hidden state 的计算之中。这次，我们必须对 <code>__call__</code> 函数的进行修改了，因为不再是简单的输入拼接，<strong>而 LSTM 内部的 hidden state 的计算过程也需要依赖这个 action vector</strong>。先来看代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SCLSTM</span><span class="params">(BasicLSTMCell)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kwd_voc_size, *args, **kwargs)</span>:</span></span><br><span class="line">        BasicLSTMCell.__init__(self, *args, **kwargs)</span><br><span class="line">        self.key_words_voc_size = kwd_voc_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, d_act)</span>:</span></span><br><span class="line">        sigmoid = math_ops.sigmoid</span><br><span class="line">        one = constant_op.constant(<span class="number">1</span>, dtype=dtypes.int32)</span><br><span class="line"><span class="comment"># parameters for tanh function</span></span><br><span class="line">        w_d = vs.get_variable(<span class="string">'w_d'</span>, [self.key_words_voc_size, self._num_units])</span><br><span class="line">        <span class="comment"># Parameters of gates are concatenated into one multiply for efficiency.</span></span><br><span class="line">        <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line">            c, h = state</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c, h = array_ops.split(value=state, num_or_size_splits=<span class="number">2</span>, axis=one)</span><br><span class="line">        gate_inputs = math_ops.matmul(</span><br><span class="line">            array_ops.concat([inputs, h], <span class="number">1</span>), self._kernel)</span><br><span class="line">        gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span></span><br><span class="line">        i, j, f, o = array_ops.split(</span><br><span class="line">            value=gate_inputs, num_or_size_splits=<span class="number">4</span>, axis=one)</span><br><span class="line"></span><br><span class="line">        forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)</span><br><span class="line"></span><br><span class="line">        add = math_ops.add</span><br><span class="line">        multiply = math_ops.multiply</span><br><span class="line">        <span class="comment"># add extra topic information to candidate calculation </span></span><br><span class="line">        new_c = add(add(multiply(c, sigmoid(add(f, forget_bias_tensor))),</span><br><span class="line">                        multiply(sigmoid(i), self._activation(j))),</span><br><span class="line">                    math_ops.tanh(math_ops.matmul(d_act, w_d)))</span><br><span class="line">        new_h = multiply(self._activation(new_c), sigmoid(o))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._state_is_tuple:</span><br><span class="line">            new_state = LSTMStateTuple(new_c, new_h)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_state = array_ops.concat([new_c, new_h], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> new_h, new_state</span><br></pre></td></tr></table></figure><p>其中的核心就是我们申请了一个额外的变量 <code>w_d</code>，并且在 candidate 的计算之中加上了一项额外的 <code>tanh</code> 项，这是和文章的公式对应的：</p><p><img src="/img/cd_com.png" alt="Candidate Computation"></p><p>但是修改了 LSTM Cell 的 <code>__call__</code> 函数之后，就和 seq2seq 接口不匹配了，依旧是借助 Wrapper 来实现接口的匹配，并且额外的 gate 也在 wrapper 中进行实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActionWrapper</span><span class="params">(RNNCell)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cell, action_vec, wr, hr)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(cell, SC_DropoutWrapper) <span class="keyword">and</span> <span class="keyword">not</span> isinstance(cell, SCLSTM):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"The wrapper is only designed for SCLSTM."</span>)</span><br><span class="line">        self._cell = cell</span><br><span class="line">        self.action_vec = action_vec  <span class="comment"># initial one-hot action vector</span></span><br><span class="line">        self.wr = wr  <span class="comment"># [word_embedding_size, topic_size ]</span></span><br><span class="line">        self.hr = hr  <span class="comment"># [hidden_size, topic_size]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># note: params only inputs and state</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">        ct, ht = state</span><br><span class="line">        <span class="comment"># compute sigmoid and update action vec</span></span><br><span class="line">        <span class="comment"># r_t = sigmoid( W_wr x_t + W_hr h_&#123;t-1&#125;)  = sigmoid(e1 + e2)</span></span><br><span class="line">        e1 = math_ops.matmul(inputs, self.wr)  <span class="comment"># [batch_size, topic_size]</span></span><br><span class="line">        e2 = math_ops.matmul(ht, self.hr)</span><br><span class="line">        r_t = math_ops.sigmoid(math_ops.add(e1, e2))  <span class="comment"># [batch_size, topic_size]</span></span><br><span class="line">        <span class="comment"># update action vector</span></span><br><span class="line">        self.action_vec = r_t * self.action_vec</span><br><span class="line">        <span class="keyword">return</span> self._cell(inputs, state, self.action_vec)</span><br></pre></td></tr></table></figure><p> 和之前一样，在构造函数之中我们传入了 one-hot 的 action vector，并且获取一些 variables，理由和之前一样，<strong>action vector 每步都要更新，但是不能出现在两个循环之中</strong>。<code>__call__</code> 函数的结构和原先的 LSTM 保持一致，这样就能够和 seq2seq 的接口配合着使用，而在内部，根据文章的公式设置了一个 <code>r_t</code> 作为 gate 来控制 topic information：</p><p><img src="/img/rt_formula.png" alt="r_t"></p><p>使用的时候，同样地，我们可以通过使用 <code>ActionWrapper(SCLSTMCell(hidden_state))</code> 来进行使用。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>至此，我们已经展现了如何利用 TensorFlow 的 <code>Wrapper</code> 对象来 RNN cell 功能的修改，同时能够通过接口参数的设计来让自定义的 RNN cell 和其他的 API 一起协作起来。这个 <code>Wrapper</code> 类，实际上就是装饰者模式的一种应用，一层一层地包裹住来增加功能并且能够屏蔽接口的不同。踩到的坑也就是在文中提到的：</p><ol><li>同一个 Tensor 对象不能出现在不同 Loop 中</li><li>使用  Wrapper 对象的时候注意内部定义的 variable，可能需要通过外部传入来使学到的参数进入到后续的使用中（training 和 inference）</li></ol><p>复现这两篇 Paper 的过程中主要参考了 RNN 的<a href="https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/rnn_cell_impl.py" target="_blank" rel="noopener">源码</a>（似乎新旧版本的 API 还有不一样？），虽然之前也有读过，但自己写一遍又是不一样体验。希望以后多写写这样的代码~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前写过一篇 IndRNN 的文章，第一次照着开源代码实现了自己定制 RNN，其实也很简单，就是把 RNN 实现里的 &lt;code&gt;__call__&lt;/code&gt; 函数进行修改，每一个时间步的全连接 &lt;code&gt;matmul&lt;/code&gt; 改成 element-wise 的 
      
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://tobiaslee.top/tags/TensorFlow/"/>
    
      <category term="RNN" scheme="https://tobiaslee.top/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://tobiaslee.top/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>秋寒</title>
    <link href="https://tobiaslee.top/2018/10/21/one-more-autumn/"/>
    <id>https://tobiaslee.top/2018/10/21/one-more-autumn/</id>
    <published>2018-10-21T10:22:44.000Z</published>
    <updated>2018-10-21T12:06:47.009Z</updated>
    
    <content type="html"><![CDATA[<p>​    前两天日子晴朗的时候，看到路两侧的银杏，黄的很干净，也很安静。这两天总是有些小雨，温度也骤然降下来。我是一个对时间流逝很敏感的人，用不着逐渐增厚的衣服提醒，我知道，在西电的第三个秋天到了。</p><h2 id="近况"><a href="#近况" class="headerlink" title="近况"></a>近况</h2><p>​    开学以来的这段时间，非常地充实。整个暑假都在 DeeCamp，回来之后就投入到之前论文的修改中。说起来这篇处女作也是命途多舛，被拒之后经过了大幅度的修改，再投，竟然被秒拒，甚至没有送外审。似乎是因为主题不符合，姑且算是为下一次攒人品吧。又修改了一次并且再次投出去，希望能够有一个好的结果。</p><p>​    课程依旧是波澜不惊地进行着，lanco 的实习 Project 也让我在闲暇有事可做。说实话，我不算是那种虚度一秒钟都会觉得愧疚的人，但也会因为荒废大量的时间摸鱼感到一丝紧张。这种心理从高三一直延续到现在，究其背后的 motivation，其实也很简单，我也在之前提到过：高三的时候是为了千军万马过独木桥，如今则是”我不属于这里”的不甘。这种心情从大一到现在，淡化了很多，但依旧让我保持警醒。但其实这么两年下来下来，神经不再像刚入学时候一样一直紧绷着，很多时候，<strong>弄清楚自己想要的，选择一条路慢慢走下去，不用太过着急</strong>，结果没有那么的重要。<strong>不要让自己后悔的同时，尽可能过得舒服一点</strong>，找到自己的节奏，就 ok 了。</p><p>​    身边其实也有很多优秀的同学，早早地联系老师，为未来打下基础。我觉得这真的很不错，因为之前去 THU 蹭讲座的时候发现相比北京的一些高校，西安的地理位置太吃亏了。不怕笨鸟先飞，就怕聪明的鸟飞的比你早。很多时候我们缺的是这种意识，而当你产生危机感之后，立马行动起来，是不会吃亏太多的。</p><a id="more"></a><h2 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h2><p>​    GitHub 是个好东西，作为一名 CS 学生，我觉得 GitHub 很多时候比什么英语竞赛更能够衡量专业能力。之前看杭高的学长 <a href="https://github.com/geeeeeeeeek" target="_blank" rel="noopener">Zhongyi Tong</a> 的主页， Star 加起来之前的几万，很是羡慕。所以一直在努力的维护自己的 GitHub 吧，希望有生之年能有个破千的项目（逃。之前的<a href="https://github.com/TobiasLee/Text-Classification" target="_blank" rel="noopener">文本分类</a>项目现在有 88 个 Star 了，希望能尽快破百吧哈哈哈哈哈，看到有人点 fork 但是不 star 的时候我真的好心痛啊！还有就是最近能够收到不少邮件或者是 Issue，作为项目的所有者，我很乐意去解答一些问题（当然这是要看人的，开源项目就是提供一个平台，owner 没有义务这么做）。在答复的过程中，相当于也逼迫自己再去思考一遍问题，之前就有几个小伙伴找出了一个很大的 bug，我也是再次学习了一次。<strong>这的确是一个能够帮助你快速成长的平台</strong>，也能帮助你 explore more。</p><h2 id="书和影"><a href="#书和影" class="headerlink" title="书和影"></a>书和影</h2><p>​    这段时间看了两本书：《鹿鼎记》和《白鹿原》。金庸的小说真的很过瘾，看着就停不下来，韦小宝教给的我就是：混江湖一定要重义气。《白鹿原》讲的就是陕西这片土地上的故事了，我也才知道”葫芦鸡“的来历哈哈哈哈，白鹿原上两大户白家和鹿家三代人所经历的，也是中国近代历史的一个缩影了。白家的屹立不倒的原因莫过于家主“挺直的腰杆”，“耕读传家”相比勺勺客的趋炎附势，是更能经受住时间的考验的。这种中国历史背景之下个人家族的沉浮的故事也算得上是我的心头好了，之前看的一本《北鸢》也是同样的题材，读起来很舒服。</p><p>​    看综艺真的是很能够平息做实验的烦躁，最近在看的有《向往的生活》《极限挑战》，但都不会追着看，有一搭没一搭地在看。Gakki 的新剧也有在看，但并不是很对口味，反倒是之前看了几季就撂下的《Shameless》的第九季，可以看得很轻松，就是打发等待实验的时间嘛。国庆的时候也看了不少电影，《快把我哥带走》居然催下了几滴眼泪，漫威的几部片子也就那样，毕竟不是一个真正的漫威粉，当做爆米花电影看就完事了。前几天久违地寝室小剧场一起看了《V 字仇杀队》，之前好像是在电影频道看过一些片段，这次补完也算是没有愧对这部神片。其余也就是刷了刷豆瓣 Top 250，也就是囫囵吞枣，增添一些谈资罢了。</p><p>​    其实很希望有时间能去看看话剧、音乐会什么的，不过贵校这个地理位置也真是限制了我的行动，找机会回家看了只能。</p><h2 id="希冀"><a href="#希冀" class="headerlink" title="希冀"></a>希冀</h2><p>接下来的日子里，做好下面几件事：</p><ol><li>保持广泛地阅读：Paper 也好，艺术史小说也罢，多读多看，没有坏处。努力练就看 Paper 如喝水一样的状态。</li><li>Keep Coding，Keep Thinking：回头看自己写的代码，真的觉得是一坨 Shit！写的时候仓促，能跑就行的毛病还是没改，于是就有随手就丢上 GitHub 然后被吐槽的故事，不过 GitHub 能够迫使我来重构自己的代码（毕竟是第二张简历），希望以后写代码的时候多想想。</li><li>锻炼身体，为祖国健康工作五十年！</li></ol><p>希望所有明媚的梦想都能够在不久之后绽放，Good Luck！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    前两天日子晴朗的时候，看到路两侧的银杏，黄的很干净，也很安静。这两天总是有些小雨，温度也骤然降下来。我是一个对时间流逝很敏感的人，用不着逐渐增厚的衣服提醒，我知道，在西电的第三个秋天到了。&lt;/p&gt;
&lt;h2 id=&quot;近况&quot;&gt;&lt;a href=&quot;#近况&quot; class=&quot;headerlink&quot; title=&quot;近况&quot;&gt;&lt;/a&gt;近况&lt;/h2&gt;&lt;p&gt;​    开学以来的这段时间，非常地充实。整个暑假都在 DeeCamp，回来之后就投入到之前论文的修改中。说起来这篇处女作也是命途多舛，被拒之后经过了大幅度的修改，再投，竟然被秒拒，甚至没有送外审。似乎是因为主题不符合，姑且算是为下一次攒人品吧。又修改了一次并且再次投出去，希望能够有一个好的结果。&lt;/p&gt;
&lt;p&gt;​    课程依旧是波澜不惊地进行着，lanco 的实习 Project 也让我在闲暇有事可做。说实话，我不算是那种虚度一秒钟都会觉得愧疚的人，但也会因为荒废大量的时间摸鱼感到一丝紧张。这种心理从高三一直延续到现在，究其背后的 motivation，其实也很简单，我也在之前提到过：高三的时候是为了千军万马过独木桥，如今则是”我不属于这里”的不甘。这种心情从大一到现在，淡化了很多，但依旧让我保持警醒。但其实这么两年下来下来，神经不再像刚入学时候一样一直紧绷着，很多时候，&lt;strong&gt;弄清楚自己想要的，选择一条路慢慢走下去，不用太过着急&lt;/strong&gt;，结果没有那么的重要。&lt;strong&gt;不要让自己后悔的同时，尽可能过得舒服一点&lt;/strong&gt;，找到自己的节奏，就 ok 了。&lt;/p&gt;
&lt;p&gt;​    身边其实也有很多优秀的同学，早早地联系老师，为未来打下基础。我觉得这真的很不错，因为之前去 THU 蹭讲座的时候发现相比北京的一些高校，西安的地理位置太吃亏了。不怕笨鸟先飞，就怕聪明的鸟飞的比你早。很多时候我们缺的是这种意识，而当你产生危机感之后，立马行动起来，是不会吃亏太多的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>IPC 问题</title>
    <link href="https://tobiaslee.top/2018/10/14/OS-IPC-Notes/"/>
    <id>https://tobiaslee.top/2018/10/14/OS-IPC-Notes/</id>
    <published>2018-10-14T06:45:16.000Z</published>
    <updated>2018-10-14T11:52:16.415Z</updated>
    
    <content type="html"><![CDATA[<p>操作系统线程之间的通信是一个很重要的问题，在用 C 语言在 Linux 实现经典的“生产者和消费者”以及“读者写者”问题时产生了一些疑惑，记录之。</p><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>进程间通信的需要解决的核心问题就是<strong>线程之间的同步</strong>，比如说，有一个程序，两个线程同时对一个共享变量 <code>cnt</code> 分别进行 <code>n</code> 次加法操作，如果其初始值为 0 的话，那么正确的运行结果应该是 <code>cnt = 2n</code>，但如果我们在一台单 CPU 的机器上运行的话，结果往往不是 <code>2n</code>。为什么呢？因为机器可能不会按照我们思考的方式去执行这样的指令，一种很容易想到的产生错误的方式就是：一个线程修改了值（对 <code>cnt</code> 进行加 1 ）之后还没写回去，这个时候另一个线程就读到了 <code>cnt</code> 的旧值并且在旧值之上进行了更新操作，二者都写回去的时候，<code>cnt</code> 的值只加一而不是加二。一个显而易见的解决方式就是将各自的操作都变成<strong>原子化</strong>，也即一个线程带来的任何改变都能被另一个线程所感知到，不过这样带来的 overhead（额外开销）过大，对性能是个很大的影响。因此伟大的计算机科学家们提出了两个概念用于解决这个问题，分别是 mutex（互斥量）和 semphore（信号量）：</p><blockquote><p>Strictly speaking, <strong>a mutex is a locking mechanism</strong> used to synchronize access to a resource. Only one task (can be a thread or process based on OS abstraction) can acquire the mutex. It means there will be ownership associated with mutex, and only the owner can release the lock (mutex).</p><p><strong>Semaphore is signaling mechanism</strong> (“I am done, you can carry on” kind of signal). For example, if you are listening songs (assume it as one task) on your mobile and at the same time your friend called you, an interrupt will be triggered upon which an interrupt service routine (ISR) will signal the call processing task to wakeup.</p></blockquote><p>互斥量是一把锁，用于控制对资源的访问，同一时间只有一个线程能够获得这把锁，也只有获得锁的线程才能够释放这把锁；信号量是一个信号，用于线程之间的通讯。特别地，信号量 s 有两种操作：</p><ul><li>P(s)：如果 s 非零，则将 s -1 并返回；如果 s 为 0，则挂起该线程，直到 s 非 0，一般来说，是等待 V 操作的通知。</li><li>V(s)：V 操作将 s + 1。如果有任何线程阻塞在 P 操作等待，则 V 操作会<strong>通知</strong>折线线程中的一个，让他苏醒完成 s - 1 操作。</li></ul><p>这里几点需要注意：</p><ol><li>P 和 V 操作都是原子化的，也就是说检测信号值、执行操作、写回信号值这三个子操作时不可能被打断的。也只有这样，P、V 操作才有意义。</li><li>V 操作唤醒等待线程时是随机的，我们无法预测哪个线程会苏醒，这里没有<strong>先来先走</strong>的事情存在</li></ol><p>概念就先介绍到这里，可能还有一些不清楚的地方，就结合接下来的两个问题的代码实现，再进一步的讲解。<br><a id="more"></a></p><h2 id="Producer-amp-Consumer"><a href="#Producer-amp-Consumer" class="headerlink" title="Producer &amp; Consumer"></a>Producer &amp; Consumer</h2><p>问题描述：生产者和消费者线程共享一个具有 n 个槽（slot）的有限缓冲区，生产者线程反复生产新的物品，并把它们放到缓冲区中；消费者从缓冲区中取出这些物品并且使用它们。要模拟这个问题，我们需要保证对共享变量，即缓冲区的访问是互斥的，并且，仅仅互斥还不够，还需要调度这个访问，以为存在以下两种情况：</p><ol><li>缓冲区已满：这种情况，生产者需要等待消费者消费掉缓冲区中的物品后，才有空槽来防止生产的物品，所以需要等待</li><li>缓冲区已空：消费者需要等待生产者生产出物品才能够消费</li></ol><p>现实生活这样的例子比比皆是，比如我们看视频，生产者编码视频帧，消费者解码视频并且显示出来，其中就有一个缓冲。下面就是一个代码的实现：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;pthread.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;semaphore.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX 8 <span class="comment">// buffer size 缓冲区大小</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">sem_t</span> full; <span class="comment">// 信号量 full slots</span></span><br><span class="line"><span class="keyword">sem_t</span> empty; <span class="comment">// 信号量 empty slots</span></span><br><span class="line"><span class="keyword">pthread_mutex_t</span> mutex = PTHREAD_MUTEX_INITIALIZER; <span class="comment">// 互斥量 mutex</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//sem_t mutex;</span></span><br><span class="line"><span class="keyword">int</span> front = <span class="number">0</span>; <span class="comment">// buffer array front</span></span><br><span class="line"><span class="keyword">int</span> rear = <span class="number">0</span>; <span class="comment">// buffer array rear</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">produce</span><span class="params">(<span class="keyword">void</span>* arg)</span> </span>&#123; <span class="comment">// 生产者函数</span></span><br><span class="line"><span class="keyword">int</span> i ;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span> ; i &lt; MAX * <span class="number">4</span> ; i ++) &#123; <span class="comment">// 生产 MAX * 4 个物品</span></span><br><span class="line"><span class="comment">//printf("Producer is preparing data...\n");</span></span><br><span class="line">sem_wait(&amp;empty);</span><br><span class="line">        <span class="comment">//sem_wait(&amp;mutex);</span></span><br><span class="line">pthread_mutex_lock(&amp;mutex); <span class="comment">// lock </span></span><br><span class="line">front = (front + <span class="number">1</span> )% MAX ;</span><br><span class="line"><span class="comment">//printf("now top is %d \n", top);</span></span><br><span class="line"><span class="comment">//sem_post(&amp;mutex);</span></span><br><span class="line">pthread_mutex_unlock(&amp;mutex); <span class="comment">// unlock</span></span><br><span class="line">sem_post(&amp;full); </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">void</span>*) <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">consume</span><span class="params">(<span class="keyword">void</span>* arg)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> i ;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span> ; i &lt; MAX * <span class="number">4</span>; i ++ )&#123;</span><br><span class="line"><span class="comment">//printf("consumer is preparing ...\n");</span></span><br><span class="line">sem_wait(&amp;full) ; <span class="comment">// check if there is slot to consume</span></span><br><span class="line">pthread_mutex_lock(&amp;mutex); <span class="comment">// lock</span></span><br><span class="line"><span class="comment">//sem_wait(&amp;mutex);</span></span><br><span class="line">rear  = (rear + <span class="number">1</span> ) % MAX ;</span><br><span class="line"><span class="comment">//printf("now bottom is %d \n", bottom);</span></span><br><span class="line">pthread_mutex_unlock(&amp;mutex);</span><br><span class="line">    <span class="comment">//sem_post(&amp;mutex);</span></span><br><span class="line">sem_post(&amp;empty);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">void</span>*) <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">pthread_t</span> t1;;</span><br><span class="line"><span class="keyword">pthread_t</span> t2;</span><br><span class="line"><span class="keyword">pthread_t</span> t3;</span><br><span class="line"><span class="keyword">pthread_t</span> t4;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> ret1, ret2, ret3, ret4;</span><br><span class="line"></span><br><span class="line">sem_init(&amp;full, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">sem_init(&amp;empty, <span class="number">0</span>, MAX);</span><br><span class="line">    <span class="comment">//sem_init(&amp;mutex, 0, 1);</span></span><br><span class="line"><span class="comment">// create thread</span></span><br><span class="line">pthread_create(&amp;t1, <span class="literal">NULL</span>, produce, <span class="literal">NULL</span>);</span><br><span class="line">pthread_create(&amp;t2, <span class="literal">NULL</span>, consume, <span class="literal">NULL</span>);</span><br><span class="line">pthread_create(&amp;t3, <span class="literal">NULL</span>, produce, <span class="literal">NULL</span>);</span><br><span class="line">pthread_create(&amp;t4, <span class="literal">NULL</span>, consume, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">pthread_join(t1, (<span class="keyword">void</span>**) &amp;ret1);</span><br><span class="line">pthread_join(t2, (<span class="keyword">void</span>**) &amp;ret2);</span><br><span class="line">pthread_join(t3, (<span class="keyword">void</span>**) &amp;ret3);</span><br><span class="line">pthread_join(t4, (<span class="keyword">void</span>**) &amp;ret4);</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"final front: %d\n"</span>,front);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"final rear: %d\n"</span>, rear);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先我们需要使用的两个头文件：<code>semphore.h</code> 和 <code>pthread.h</code> ，其中包含了我们的函数和需要的信号量类型，这里主要介绍一下几个重要的函数：</p><ol><li><code>sem_init()</code>：其函数原型为 <code>extern int sem_init __P ((sem_t *__sem, int __pshared, unsigned int __value));</code> 第一个参数 sem 为指向信号量结构的一个指针；第二个参数 pshared 不为０时此信号量在进程间共享，否则只能为当前进程的所有线程共享，我们的代码中都是在线程中共享，因此均为 0；value 给出了信号量的初始值。</li><li><code>pthread_create()</code> 和 <code>pthread_join()</code>：分别是创建线程的函数和等待线程结束。前者的参数分别为设定线程对象，线程属性，线程函数，最后一个参数是运行函数的参数；<code>join</code> 函数则就是线程对象，以及接受返回结果的指针。</li></ol><p>接下来讲解核心的函数 <code>consume()</code> ：</p><p>我们先在一个循环之中进行生产操作，在消费时我们先进行对信号量 <code>full</code> 的 P 操作，检测是否有物品可供消费，如果没有，则挂起线程等待；如果有，则接下来我们要对缓冲区进行操作，所以需要拿到进入缓冲区的钥匙，也就是 <strong>mutex</strong>，通过 <code>pthread_mutex_lock(&amp;mutex)</code>来获取这一把锁，同样的，如果这把锁现在在别人手里，我们也会挂起等待。拿到这把锁之后，我们就可以对缓冲去进行操作了，这里我们模拟消费的操作时将缓冲区的尾部 <code>rear</code>  加 1。而后释放缓冲区的锁，并且对 <code>empty</code> 进行 V 操作，也就是通知生产者我消耗了一个物品。</p><p>另一个生产者的函数 <code>produce</code>：就不再详细赘述了，和消费者相反，这里先要检查是否有空槽，因此对 <code>empty</code> 进行 P 操作，而后操作缓冲区，对 <code>front</code> 进行加 1 操作，结束后释放缓冲区的 mutex，以及对 <code>full</code> 进行 V 操作。</p><p>运行这个程序的时候在编译时需要加上 <code>-pthread</code> 参数，可以通过执行下列命令来运行：</p><blockquote><p>gcc -pthread consumer_and_producer.c -o c_p</p><p>./c_p</p></blockquote><p>我们可以看到的结果是最终的 <code>front</code> 和 <code>rear</code> 均为 0。</p><p>这里有一点需要注意的时，同步信号量的检查（empty 和 full）要放在互斥量 mutex 的外面，否则，可能会导致死锁。比如生产者获得了缓冲区的锁，之后却无法发现没有空槽，从而挂起，而另一边消费者又无法获得缓冲区的锁导致挂起，从而你等我我等你造成死锁。</p><p>我在学习这个代码的时候有个疑问，就是我们能不能用一个 binary semphore（二值信号量）来替代 mutex？也就是把上面的 mutex 也换成一个 <code>sem_t</code>，并且将其初始值设置为 1（代码中黄色注释部分）。Stack Overflow 上有一个很不错的<a href="https://stackoverflow.com/questions/62814/difference-between-binary-semaphore-and-mutex" target="_blank" rel="noopener">回答</a>，而我的实验结果是<strong>不建议</strong>，我修改之后运行的结果在我的多核电脑上并不是均为 0（猜测是因为多核的原因，有待思考），而在单核的云服务器上运行结果则正确。所以，正如之前我们解释这两个概念的差别的时候，尽管 binary semphore 和 mutex 很类似，但还是要小心期间细微的差别。</p><h2 id="Reader-amp-Writer"><a href="#Reader-amp-Writer" class="headerlink" title="Reader &amp; Writer"></a>Reader &amp; Writer</h2><p>有读者和写者两组并发进程，共享一个文件，当两个或以上的读进程同时访问共享数据时不会产生副作用，但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。因此要求：</p><ol><li>允许多个读者可以同时对文件执行读操作；</li><li>同一时刻，只允许一个写者往文件中写信息；</li><li>任一写者在完成写操作之前不允许其他读者或写者工作；</li><li>写者执行写操作前，应让已有的读者和写者全部退出。</li></ol><p>这里有两种实现的方式，一种是写者优先，一种是读者优先，先来看读者优先实现的代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;pthread.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;semaphore.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">data</span>&#123;</span></span><br><span class="line"><span class="keyword">int</span> read_counter;</span><br><span class="line"><span class="keyword">int</span> write_counter;</span><br><span class="line"><span class="keyword">int</span> value;</span><br><span class="line">&#125; share_data;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> READ_THREAD_NUM = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">int</span> WRITE_THREAD_NUM = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">int</span> TOTAL_TRY = <span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> stop = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> reader_num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">sem_t</span> lock_reader;</span><br><span class="line"><span class="keyword">sem_t</span> lock_writer;</span><br><span class="line"><span class="keyword">sem_t</span> w_or_r;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">read</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line"><span class="keyword">while</span>(!stop)  &#123;</span><br><span class="line">sem_wait(&amp;lock_reader);</span><br><span class="line">reader_num += <span class="number">1</span>;</span><br><span class="line">share_data.read_counter ++ ;</span><br><span class="line"><span class="keyword">if</span>(reader_num == <span class="number">1</span>) </span><br><span class="line">sem_wait(&amp;w_or_r); <span class="comment">// first reader, check if there is any writer</span></span><br><span class="line">sem_post(&amp;lock_reader);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> tmp = share_data.value; <span class="comment">// access the data</span></span><br><span class="line">        </span><br><span class="line">sem_wait(&amp;lock_reader);</span><br><span class="line">reader_num --;</span><br><span class="line"><span class="keyword">if</span>(reader_num == <span class="number">0</span> ) </span><br><span class="line">    sem_post(&amp;w_or_r); <span class="comment">// no more reader</span></span><br><span class="line">sem_post(&amp;lock_reader) ;</span><br><span class="line"><span class="comment">// use the data read</span></span><br><span class="line"><span class="comment">//printf("Reader read : %d \n", tmp);</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">write</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> idx = *(<span class="keyword">int</span> * ) ptr;</span><br><span class="line"><span class="keyword">while</span>(!stop) &#123;</span><br><span class="line">sem_wait(&amp;lock_writer);</span><br><span class="line">sem_wait(&amp;w_or_r);</span><br><span class="line">share_data.write_counter ++;</span><br><span class="line">share_data.value = idx;</span><br><span class="line"><span class="comment">//        printf("writer write : %d \n", idx);</span></span><br><span class="line"><span class="keyword">if</span>(share_data.write_counter &gt;= TOTAL_TRY) </span><br><span class="line">stop = <span class="number">1</span>;</span><br><span class="line">sem_post(&amp;w_or_r);</span><br><span class="line">sem_post(&amp;lock_writer);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">share_data.read_counter = <span class="number">0</span>;</span><br><span class="line">share_data.write_counter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">sem_init(&amp;lock_reader, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">sem_init(&amp;lock_writer, <span class="number">0</span> , <span class="number">1</span>);</span><br><span class="line">sem_init(&amp;w_or_r, <span class="number">0</span> , <span class="number">1</span> );</span><br><span class="line"></span><br><span class="line"><span class="keyword">pthread_t</span> readers[READ_THREAD_NUM];</span><br><span class="line"><span class="keyword">pthread_t</span> writers[WRITE_THREAD_NUM];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span> ; i &lt; READ_THREAD_NUM; i ++ ) </span><br><span class="line">pthread_create(&amp;readers[i], <span class="literal">NULL</span>, read, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> thread_args[] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span> ; i &lt; WRITE_THREAD_NUM ; i ++ )</span><br><span class="line">pthread_create(&amp;writers[i], <span class="literal">NULL</span>, write, (thread_args + i));</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Final result, reader count: %d, writer count : %d \n"</span>, share_data.read_counter,</span><br><span class="line">share_data.write_counter);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们用到了三个 binary 信号量 <code>lock_reader</code> 、<code>lock_writer</code> 以及 <code>w_or_r</code> ，分别用于读者数量、写者数量以及读写模式的控制。所谓读者优先，我们可以从下面两种序列来分析：</p><ol><li><code>(r1, w1, r2)</code>： 如果 <code>r1</code> 先获得了读的权限，随后来的 <code>w1</code> 写者就会阻塞在获取 <code>w_or_r</code> 这一信号量上，注意到<strong><code>r1</code> 在对数据进行读取之前已经对 <code>lock_reader</code> 进行了 V 操作</strong>，这时候 <code>r2</code> 就不会阻塞而能够读取数据，只有当读者全部完成读取退出之后，才会对 <code>w_or_r</code> 进行 V 操作，<code>w1</code> 才有机会获取写的权限；</li><li>而如果是<code>(w1, r1, w2)</code> 这样的序列，<code>w1</code> 写后，<code>r1</code> 和 <code>w2</code> 同时到达，他们就会处于一个公平竞争的地位，谁先拿到 <code>w_or_r</code> 谁就运行。</li></ol><p>这边我列出了五次运行的结果：</p><blockquote><p>Final result, reader count: 16, writer count : 2</p><p>Final result, reader count: 40, writer count : 5 </p><p>Final result, reader count: 21, writer count : 4 </p><p>Final result, reader count: 31, writer count : 0 </p><p>Final result, reader count: 28, writer count : 5 </p></blockquote><p>可以看到，读者的数量是远大于写者的数量，这也就是<strong>读优先</strong>体现。</p><p>另一种写者优先的实现如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;semaphore.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;pthread.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">data</span>&#123;</span></span><br><span class="line"><span class="keyword">int</span> r_c;</span><br><span class="line"><span class="keyword">int</span> w_c;</span><br><span class="line"><span class="keyword">int</span> value;</span><br><span class="line"><span class="keyword">int</span> v1;</span><br><span class="line">&#125; share_data;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> READ_THREAD_NUM = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">int</span> WRITE_THREAD_NUM = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">int</span> TOTAL_TRY = <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> stop = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> reader_num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> writer_num = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">sem_t</span> rlock, wlock;</span><br><span class="line"><span class="keyword">sem_t</span> w_or_r;</span><br><span class="line"><span class="keyword">sem_t</span> try_read;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">read</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(!stop) &#123;</span><br><span class="line">        <span class="comment">// extra query</span></span><br><span class="line">sem_wait(&amp;try_read);</span><br><span class="line">sem_wait(&amp;rlock);</span><br><span class="line"></span><br><span class="line">reader_num += <span class="number">1</span>;</span><br><span class="line">share_data.r_c += <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(reader_num == <span class="number">1</span> ) &#123;</span><br><span class="line">sem_wait(&amp;w_or_r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sem_post(&amp;rlock);</span><br><span class="line">sem_post(&amp;try_read);</span><br><span class="line"><span class="comment">// do read</span></span><br><span class="line"><span class="keyword">int</span> tmp = share_data.value;</span><br><span class="line"><span class="keyword">if</span>( tmp != share_data.v1) &#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"error happen: %d v.s %d\n "</span>, tmp, share_data.v1);</span><br><span class="line">stop = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">sem_wait(&amp;rlock);</span><br><span class="line">reader_num --;</span><br><span class="line"><span class="keyword">if</span>(reader_num == <span class="number">0</span>)</span><br><span class="line">    sem_post(&amp;w_or_r);</span><br><span class="line">sem_post(&amp;rlock);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">write</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> idx = * (<span class="keyword">int</span>*) ptr;</span><br><span class="line"><span class="keyword">while</span>(!stop) &#123;</span><br><span class="line">sem_wait(&amp;wlock);</span><br><span class="line">writer_num += <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(writer_num == <span class="number">1</span>) </span><br><span class="line">    sem_wait(&amp;try_read);</span><br><span class="line"></span><br><span class="line">sem_post(&amp;wlock);</span><br><span class="line"></span><br><span class="line">sem_wait(&amp;w_or_r); <span class="comment">// set write</span></span><br><span class="line">share_data.w_c ++;</span><br><span class="line">share_data.value = idx;</span><br><span class="line">share_data.v1 = idx;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(share_data.w_c &gt;= TOTAL_TRY) &#123;</span><br><span class="line">stop = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">sem_post(&amp;w_or_r);</span><br><span class="line">sem_wait(&amp;wlock);</span><br><span class="line">writer_num -= <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(writer_num == <span class="number">0</span>) </span><br><span class="line">    sem_post(&amp;try_read); <span class="comment">// no more writer</span></span><br><span class="line">sem_post(&amp;wlock);</span><br><span class="line"><span class="comment">//sleep(0.001);</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">share_data.r_c = <span class="number">0</span>;</span><br><span class="line">share_data.w_c = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">sem_init(&amp;lock, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">sem_init(&amp;wlock, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">sem_init(&amp;w_or_r, <span class="number">0</span> , <span class="number">1</span>);</span><br><span class="line">sem_init(&amp;try_read, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">pthread_t</span> readers[READ_THREAD_NUM];</span><br><span class="line"><span class="keyword">pthread_t</span> writers[WRITE_THREAD_NUM];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span> ; i &lt; READ_THREAD_NUM ; i ++ ) </span><br><span class="line">pthread_create(&amp;readers[i], <span class="literal">NULL</span>, read, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> thread_args[WRITE_THREAD_NUM];</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; READ_THREAD_NUM ; i++ ) &#123;</span><br><span class="line">thread_args[i] = i + <span class="number">1</span>;</span><br><span class="line">pthread_create(&amp;writers[i], <span class="literal">NULL</span>, write, (thread_args + i ));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span> ; i &lt; READ_THREAD_NUM ; i++) </span><br><span class="line">pthread_join(readers[i], <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Final Result: Reader Count: %d  Writer Count: %d \n"</span>, share_data.r_c, share_data.w_c);</span><br><span class="line">    </span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和读优先的区别在哪呢？就在多了一个信号量 <code>try_read</code>，放在最外层，也就是第一次获取 <code>rlock</code> 之前，我们再来思考之前的两个例子：</p><ol><li><code>(r1, w1, r2)</code>，<code>r1</code> 在读的时候，<code>w1</code> 和 <code>r2</code> 来了，<code>w1</code> 和 <code>r1</code> 都阻塞在 <code>try_read</code> 上，处于一个公平的竞争状态。</li><li><code>(w1, r1, w2)</code>：<code>w1</code> 在写的时候，<code>r1</code> 和 <code>w2</code> 来了，<code>r1</code> 阻塞在获取 <code>try_read</code> 上，<code>w2</code> 则阻塞在 <code>wlock</code> 上，而我们注意到 <strong>wlock 的 V 操作在 try_read 之前</strong>，所以 <code>w2</code> 在 <code>w1</code> 释放 <code>wlock</code> 后先于 <code>r1</code> 被唤醒，并且进入写，这个时候写者数量不为 0，<code>try_read</code> 还无法进行 V 操作，所以 <code>r1</code> 就必须等待写者完成写入才能进入。</li></ol><p>同样，看看结果：</p><blockquote><p>Final Result: Reader Count: 5  Writer Count: 100003 </p><p>Final Result: Reader Count: 59630  Writer Count: 100001 </p><p>Final Result: Reader Count: 4  Writer Count: 100001 </p><p>Final Result: Reader Count: 104908  Writer Count: 100003 </p><p>Final Result: Reader Count: 87291  Writer Count: 100000 </p></blockquote><p><strong>在大部分情况下，读者的数量少于写者</strong>，注意，这不是一定的，因为我们不知道线程运行之后调度的情况，并且不同的系统和硬件配置都可能有不一样的结果，<strong>出现读者比写者多也是可能的</strong>。就这两个实现而言，在我的多核电脑（MacOS）和单核服务器（Ubuntu）上，结果都是不一样的。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>总结一下，IPC 的实现借助了信号量和互斥量完成资源的共享访问。换一个视角，原先我说的那种 overhead 很大的实现方式（即每个操作都做成原子化），其实差别就在于<strong>原子化的粒度</strong>，粒度越大越能够保证线程间不会相互干扰，但随之而来的就是性能的下降；而信号量和互斥量用一个比较小的粒度，对一些关键变量进行保护，从而在不带来大量 overhead 的情况下实现了线程之间的共享变量。</p><p>OS 真的是博大精深，是需要好好学的，个人觉得虽然不至于要到能搓一个内核的水平，但对其实现的原理必须还是有个清楚的认知，不然愧对科班出身这个身份。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;操作系统线程之间的通信是一个很重要的问题，在用 C 语言在 Linux 实现经典的“生产者和消费者”以及“读者写者”问题时产生了一些疑惑，记录之。&lt;/p&gt;
&lt;h2 id=&quot;Problem&quot;&gt;&lt;a href=&quot;#Problem&quot; class=&quot;headerlink&quot; title=&quot;Problem&quot;&gt;&lt;/a&gt;Problem&lt;/h2&gt;&lt;p&gt;进程间通信的需要解决的核心问题就是&lt;strong&gt;线程之间的同步&lt;/strong&gt;，比如说，有一个程序，两个线程同时对一个共享变量 &lt;code&gt;cnt&lt;/code&gt; 分别进行 &lt;code&gt;n&lt;/code&gt; 次加法操作，如果其初始值为 0 的话，那么正确的运行结果应该是 &lt;code&gt;cnt = 2n&lt;/code&gt;，但如果我们在一台单 CPU 的机器上运行的话，结果往往不是 &lt;code&gt;2n&lt;/code&gt;。为什么呢？因为机器可能不会按照我们思考的方式去执行这样的指令，一种很容易想到的产生错误的方式就是：一个线程修改了值（对 &lt;code&gt;cnt&lt;/code&gt; 进行加 1 ）之后还没写回去，这个时候另一个线程就读到了 &lt;code&gt;cnt&lt;/code&gt; 的旧值并且在旧值之上进行了更新操作，二者都写回去的时候，&lt;code&gt;cnt&lt;/code&gt; 的值只加一而不是加二。一个显而易见的解决方式就是将各自的操作都变成&lt;strong&gt;原子化&lt;/strong&gt;，也即一个线程带来的任何改变都能被另一个线程所感知到，不过这样带来的 overhead（额外开销）过大，对性能是个很大的影响。因此伟大的计算机科学家们提出了两个概念用于解决这个问题，分别是 mutex（互斥量）和 semphore（信号量）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Strictly speaking, &lt;strong&gt;a mutex is a locking mechanism&lt;/strong&gt; used to synchronize access to a resource. Only one task (can be a thread or process based on OS abstraction) can acquire the mutex. It means there will be ownership associated with mutex, and only the owner can release the lock (mutex).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Semaphore is signaling mechanism&lt;/strong&gt; (“I am done, you can carry on” kind of signal). For example, if you are listening songs (assume it as one task) on your mobile and at the same time your friend called you, an interrupt will be triggered upon which an interrupt service routine (ISR) will signal the call processing task to wakeup.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;互斥量是一把锁，用于控制对资源的访问，同一时间只有一个线程能够获得这把锁，也只有获得锁的线程才能够释放这把锁；信号量是一个信号，用于线程之间的通讯。特别地，信号量 s 有两种操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(s)：如果 s 非零，则将 s -1 并返回；如果 s 为 0，则挂起该线程，直到 s 非 0，一般来说，是等待 V 操作的通知。&lt;/li&gt;
&lt;li&gt;V(s)：V 操作将 s + 1。如果有任何线程阻塞在 P 操作等待，则 V 操作会&lt;strong&gt;通知&lt;/strong&gt;折线线程中的一个，让他苏醒完成 s - 1 操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里几点需要注意：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;P 和 V 操作都是原子化的，也就是说检测信号值、执行操作、写回信号值这三个子操作时不可能被打断的。也只有这样，P、V 操作才有意义。&lt;/li&gt;
&lt;li&gt;V 操作唤醒等待线程时是随机的，我们无法预测哪个线程会苏醒，这里没有&lt;strong&gt;先来先走&lt;/strong&gt;的事情存在&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;概念就先介绍到这里，可能还有一些不清楚的地方，就结合接下来的两个问题的代码实现，再进一步的讲解。&lt;br&gt;
    
    </summary>
    
    
      <category term="OS" scheme="https://tobiaslee.top/tags/OS/"/>
    
  </entry>
  
  <entry>
    <title>GAN 在文本生成上的一些体会</title>
    <link href="https://tobiaslee.top/2018/09/30/Text-Generation-with-GAN/"/>
    <id>https://tobiaslee.top/2018/09/30/Text-Generation-with-GAN/</id>
    <published>2018-09-30T14:54:56.000Z</published>
    <updated>2018-10-01T08:39:27.826Z</updated>
    
    <content type="html"><![CDATA[<p>先抛出我的结论：</p><p><strong>SeqGAN 这一框架下的 GAN-based 文本生成模型，work 很大程度上是 training trick 的堆砌，并不适合工程应用，但依旧值得探索，或者蹭热点发 Paper</strong>。</p><p>这段时间做用 GAN 做文本生成还是蛮多的，这里指的是 SeqGAN 这一框架，其简要特点如下：</p><ol><li>RNN-based Generator + Classifier-based Discrminator：用一个 RNN 来建模 language model； CNN 之类分类器来对生成的文本/真实文本进行判别，或者是对文本的某种属性进行判定</li><li>利用 MLE 进行 Pretrain：让 G 和 D 具备初始的能力</li><li>利用 Monte Carlo 来得到 reward，通过 Policy Gradient 指导 Generator 更新</li></ol><p>起初我也是为止着迷，认为这一框架非常 fancy，但是随着时间推移，跑了不少实验之后发现，adversarial training 在其中起到的作用实在是<strong>微不足道</strong>（对比之前的 MLE pretrain，adversarial training 并不会带来生成文本质量的显著提升），为什么呢？接下来谈一下 Adversarial Training 在 Text Generation 中的两个主要的问题。</p><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><h3 id="Sparse-reward"><a href="#Sparse-reward" class="headerlink" title="Sparse reward"></a>Sparse reward</h3><p>adversarial training 没起作用很大的一个原因就在于，discriminator 提供的 reward 具备的 guide signal 太少，Classifier-based Discriminator 提供的只是一个为真或者假的概率作为 reward，而这个 reward 在大部分情况下，是 0。这是因为对于 CNN 来说，分出 fake text 和 real text 是非常容易的，CNN 能在 Classification 任务上做到 99% 的 accuracy，而建模 Language Model 来进行生成，是非常困难的。除此以外，<strong>即使 generator 在这样的 reward 指导下有一些提升，此后的 reward 依旧很小</strong>。从这一点出发，现有不少工作一方法不再使用简单的 fake/true probability 作为 reward，我在之前的 <a href="https://tobiaslee.top/2018/04/22/GAN-in-NLP-Notes/">GAN in NLP Notes</a> 中也提到了有 LeakyGAN（把 CNN 的 feature 泄露给 generator），RankGAN （用 IR 中的排序作为 reward）等工作来提供更加丰富的 reward；另一个解决的思路是使用 language model-based discriminator，以提供更多的区分度，北大孙栩老师组的 <a href="http://arxiv.org/abs/1802.01345" target="_blank" rel="noopener">DP-GAN</a> 在使用了 Languag model discrminator 之后，在 true data 和 fake data 中间架起了一座桥梁：</p><p><img src="/img/dp-gan.png" alt="DP-GAN"></p><p>从而 discriminator 不再是非 0 即 1。据其他同学的一些经验，DP-GAN 的实验效果也是非常不错的，这一点或许可以和之前的两个数据流型分布中间没有交集有关，使用了更 distinguishable 的 reward 之后，fake data 的分布和 true data 的分布加大了，GAN 的距离度量才有了变化。</p><h3 id="Monte-Carlo-Search"><a href="#Monte-Carlo-Search" class="headerlink" title="Monte Carlo Search"></a>Monte Carlo Search</h3><p>在 SeqGAN 以及后续的很多工作之中，对于 Reward 的评估都是基于句级别的，也就是会先使用 Monte Carlo Search 的方法将句子进行补全再交给 Discriminator，但是这个采样方法的时间复杂度是 $O(n mL^2)$，其中 $n$ 是 batch size，$m$ 是采样的次数，$L$ 是句子的 max len。就 SeqGAN 的实验来说，$m = 16$ 并且 $L=20$  ，速度尚且可以接受，但是如果我们需要去生成一篇文章 $L=200$，那么每次计算 reward 就会来带很大的开销，我个人的体验是在 Tesla M40 上 $L$ 在 100 左右，$m = 16, n = 64$ 一个 batch 需要 230 s。对于常见的万级别的 corpus，一个 Epoch 的训练时间就到了一天，而最终对于性能的提升还不如 MLE 一个 epoch 来的显著，这也是我为什么不建议在工程上使用的很大程度的一个原因。 </p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><ul><li>Sparse Reward：这里可以用的除了 DP-GAN 以外，<a href="https://www.ijcai.org/proceedings/2018/0618.pdf" target="_blank" rel="noopener">SentiGAN</a> 也是一个不错的尝试，其 Penalty-based objective function 效果还是很不错的，可以尝试着使用一下；另外，我们也可以从 discriminator 的角度，适当地减弱其能力，像 GAN 中的一些 <a href="http://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">trick</a> 比如 <code>label smothing</code> 也可以尝试。</li><li>Monte Carlo Search：这一点的解决方法比较困难，<a href="http://arxiv.org/abs/1801.07736" target="_blank" rel="noopener">MaskGAN</a> 提出用 Actor-Critic 的结构来对 word level 给出 reward，并且在其“完形填空”的任务上效果不错，但对于语言模型来说， word-level reward 还是不怎么符合 intuition。降低 sample time 并不是一个可取的方案，因为本身 MC 带来的 variance 已经很大了，再降低 sample time 只能是雪上加霜；或许<strong>搜索的剪枝</strong>是一个可以考虑的方向，会去看看有没有一些方案。</li></ul><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p>展望一下未来，只能说 SeqGAN 这个坑不出意外还会吸引很多同学义无反顾地往里跳，想要做出一些东西，还是很有挑战性的。但是 RL 和 NLP 结合的思路是没什么大毛病的，Jiwei Li 最近的几篇 Dialogue 的文章也都是用着 RL，我觉得这一点很 fancy 的一个原因是我们可以通过设计 reward 来指导生成，这其实是蛮 hand-crafted 的，把<strong>规则比较隐式地放进神经网络里</strong>，或许 RL 是一条可以走的路；IRL 在 Text Generation 上的应用邱锡鹏老师也有一篇<a href="http://arxiv.org/abs/1804.11258" target="_blank" rel="noopener">文章</a>。希望自己也能够在这条路上走地更远一些~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;先抛出我的结论：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SeqGAN 这一框架下的 GAN-based 文本生成模型，work 很大程度上是 training trick 的堆砌，并不适合工程应用，但依旧值得探索，或者蹭热点发 Paper&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这段时间
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="Text Generation" scheme="https://tobiaslee.top/tags/Text-Generation/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow——Seq2Seq 踩坑记</title>
    <link href="https://tobiaslee.top/2018/09/14/TensorFlow-Seq2Seq-%E8%B8%A9%E5%9D%91%E8%AE%B0/"/>
    <id>https://tobiaslee.top/2018/09/14/TensorFlow-Seq2Seq-踩坑记/</id>
    <published>2018-09-14T02:31:09.000Z</published>
    <updated>2018-09-14T07:29:40.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>Seq2Seq 的 TensorFlow 实现有很多，而 TensorFlow 之前也推出了一套新的 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq" target="_blank" rel="noopener">API</a>，文档依旧是令人蛋疼地杂乱。最近在用新的 API 写一个 Many2Many 结构的 Seq2Seq，踩到了一个坑，记录之，也进一步地提醒我应该把主力框架迁移到 PyTorch 提上日程了。</p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>根据示例，只要把 encoder 和 decoder 拼接起来，并且使用 TrainingHelper + BasicDecoder：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 省略参数</span></span><br><span class="line">helper_pt = tf.contrib.seq2seq.TrainingHelper()</span><br><span class="line">decoder_pt = tf.contrib.seq2seq.BasicDecoder()</span><br><span class="line">outputs_pt, _final_state, sequence_lengths_pt = tf.contrib.seq2seq.dynamic_decode()</span><br><span class="line"><span class="comment"># logits</span></span><br><span class="line">logits = outputs_pt.rnn_output </span><br><span class="line"><span class="comment"># loss</span></span><br><span class="line">loss = tf.contrib.seq2seq.sequence_loss(logits, target, target_mask,</span><br><span class="line">                    average_across_timesteps=<span class="keyword">True</span>,</span><br><span class="line">                    average_across_batch=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>其中 <code>logits</code> 是 vocab_size 上的分布，shape 为 <code>[batch_size, ?, vocab_size]</code></p><p>而 <code>target</code> 则是正确的输出，这里我将其做了 padding，补齐到最大长度，shape 为 <code>[batch_size, max_len]</code>，<code>target_mask</code> 是对应的一个权重，非补齐的部分才会参与到 loss 的计算。 </p><p>如果不 Padding，则会在 feed_dict 这一步报错：</p><blockquote><p>ValueError: setting an array element with a sequence.</p></blockquote><p><strong>因为 target_input 的长度是变长的 </strong>，NumPy 无法将其视作一个 array，导致错误。</p><p>然后问题就来了，<code>sequence_loss</code> 函数报错，说其内部调用的 <code>sparse_soft_max</code> 函数的 <code>labels</code> 和 <code>logits</code>     第一维不匹配，其中：logits 的形状为 <code>[?, vocab_size]</code> ；label 的形状为 <code>[batch_size x max_len]</code></p><p>为什么会这样呢？来进一步的分析分析</p><h2 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a>Source Code</h2><p>首先是看一下 <code>sequence_loss</code> 的源代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> ops.name_scope(name, <span class="string">"sequence_loss"</span>, [logits, targets, weights]):</span><br><span class="line">   num_classes = array_ops.shape(logits)[<span class="number">2</span>]</span><br><span class="line">   logits_flat = array_ops.reshape(logits, [<span class="number">-1</span>, num_classes])</span><br><span class="line">   targets = array_ops.reshape(targets, [<span class="number">-1</span>])</span><br><span class="line">   <span class="keyword">if</span> softmax_loss_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">     crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">         labels=targets, logits=logits_flat)</span><br></pre></td></tr></table></figure><p>先是对 logits 和 labels 做了 reshape 操作，之前的形状也能对上，也就是说 logits reshape 之后的 <code>? = ? x batch_size</code> ，那么这个 <code>?</code> 究竟是是什么呢，不出意外，应该是生成的序列长度，但为什么是不定长的呢？</p><p>来看看 <code>TrainingHelper</code> 的核心源代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TrainingHelper</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_inputs</span><span class="params">(self, time, outputs, state, name=None, **unused_kwargs)</span>:</span></span><br><span class="line">      next_time = time + <span class="number">1</span></span><br><span class="line">      finished = (next_time &gt;= self._sequence_length)</span><br><span class="line">      all_finished = math_ops.reduce_all(finished)</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">read_from_ta</span><span class="params">(inp)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> inp.read(next_time)</span><br><span class="line">      next_inputs = control_flow_ops.cond(</span><br><span class="line">          all_finished, <span class="keyword">lambda</span>: self._zero_inputs,</span><br><span class="line">          <span class="keyword">lambda</span>: nest.map_structure(read_from_ta, self._input_tas))</span><br><span class="line">      <span class="keyword">return</span> (finished, next_inputs, state)</span><br></pre></td></tr></table></figure><p><code>next_inputs</code> 函数是 RNN 不断获取下一步的迭代函数，其中 <code>finished</code> 相当于指示了当前的时间步是否已经超出最大长度，即 <code>self._sequence_length</code> 一个记录目标输出长度 int32 向量。<code>reduce_all()</code> 是对某一个维度求逻辑与，如果没有指定 axis 参数，则是对所有元素进行与运算。由此，我们可以得知：当未运行至 target 目标时间步时，会用 Groud-Truth 作为下一个的输入，否则则为 0。但这里并没有给出什么时候停止，所以进一步地，看看 decoder 在哪里调用这个的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, inputs, state, name=None)</span>:</span></span><br><span class="line">    cell_outputs, cell_state = self._cell(inputs, state)</span><br><span class="line">    <span class="keyword">if</span> self._output_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        cell_outputs = self._output_layer(cell_outputs)</span><br><span class="line">    sample_ids = self._helper.sample(</span><br><span class="line">    time=time, outputs=cell_outputs, state=cell_state)</span><br><span class="line">    (finished, next_inputs, next_state) = self._helper.next_inputs(</span><br><span class="line">              time=time,</span><br><span class="line">              outputs=cell_outputs,</span><br><span class="line">              state=cell_state,</span><br><span class="line">              sample_ids=sample_ids)</span><br><span class="line">    outputs = BasicDecoderOutput(cell_outputs, sample_ids)</span><br><span class="line">    <span class="keyword">return</span> (outputs, next_state, next_inputs, finished)</span><br></pre></td></tr></table></figure><p>这是 decoder 的 <code>step</code> 函数，相当于对 <code>helper</code> 进一步地封装，以便使用一些功能（TrainingHelper 是训练时 feed groud-truth，在 inference 阶段会使用 GreedyEmbeddingHelper 在无 Groud-Truth 帮助下进行生成等）。再向上，我们去看 <code>dynamic_decode</code> 的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dynamic_decode 为了节约篇幅，仅保留重要的代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_decode</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    initial_finished, initial_inputs, initial_state = decoder.initialize()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(unused_time, unused_outputs_ta, unused_state, unused_inputs,</span></span></span><br><span class="line"><span class="function"><span class="params">                  finished, unused_sequence_lengths)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> math_ops.logical_not(math_ops.reduce_all(finished))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(time, outputs_ta, state, inputs, finished, sequence_lengths)</span>:</span></span><br><span class="line">      (next_outputs, decoder_state, </span><br><span class="line">       next_inputs, decoder_finished) = decoder.step(time, inputs, state)</span><br><span class="line"><span class="comment"># next_finished 是 decoder_finished 和 finished 的 or</span></span><br><span class="line">      next_finished = math_ops.logical_or(decoder_finished, finished)</span><br><span class="line">      next_sequence_lengths = array_ops.where(</span><br><span class="line">          math_ops.logical_not(finished),</span><br><span class="line">          array_ops.fill(array_ops.shape(sequence_lengths), time + <span class="number">1</span>),</span><br><span class="line">          sequence_lengths)</span><br><span class="line">      <span class="keyword">return</span> (time + <span class="number">1</span>, outputs_ta, next_state, next_inputs, next_finished,</span><br><span class="line">              next_sequence_lengths)</span><br><span class="line"></span><br><span class="line">    res = control_flow_ops.while_loop(</span><br><span class="line">        condition,</span><br><span class="line">        body,</span><br><span class="line">        loop_vars=(</span><br><span class="line">            initial_time,</span><br><span class="line">            initial_outputs_ta,</span><br><span class="line">            initial_state,</span><br><span class="line">            initial_inputs,</span><br><span class="line">            initial_finished,</span><br><span class="line">            initial_sequence_lengths,</span><br><span class="line">        ),</span><br><span class="line">        parallel_iterations=parallel_iterations,</span><br><span class="line">        maximum_iterations=maximum_iterations,</span><br><span class="line">        swap_memory=swap_memory)</span><br></pre></td></tr></table></figure><p>这里，总算看到了我们要的 while loop，循环的控制变量是 <code>finished</code>，而其又 是 decoder_finished 和 finished 的逻辑或所得到，所以可以得出：当 decoder 解码到 sequence_length 时，其才会停止；另一方面，因为一个 batch 中的长度不都相同，所以得到的 dynamic_length 应该是某个 batch 中最长的一句的长度，到了这里，问题总算是知道根源所在了，那么，怎么解决呢？</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>GitHub 上有人提出过这个问题 <a href="https://github.com/tensorflow/nmt/issues/2" target="_blank" rel="noopener">Issue</a> ，并且有很长的讨论，有一个比较粗暴的解决方案，在无法喂给它 padding 之前的情况下，对 target 做一个截取，因为之前的研究能够让我们确信生成的序列的长度是一定小于等于 batch 中 target 最长的长度的，所以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取当前的长度，max_len 和 logits 的较小者，事实上，我们可以认为就是 logits 的长度</span></span><br><span class="line">current_ts = tf.to_int32(tf.minimum(tf.shape(self.target_input)[<span class="number">1</span>], tf.shape(logits)[<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 对 target 进行截取</span></span><br><span class="line">target_sequence = tf.slice(self.target_input, begin=[<span class="number">0</span>, <span class="number">0</span>], size=[<span class="number">-1</span>, current_ts])</span><br><span class="line">mask_ = tf.sequence_mask(lengths=self.target_len, maxlen=current_ts, dtype=logits.dtype)</span><br><span class="line">logits = tf.slice(self.logits_pt, begin=[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], size=[<span class="number">-1</span>, current_ts, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure><p>截取之后，就可以保证二者的长度一致，再使用<code>sequence_loss()</code> 计算就可以了。</p><p>另外有一个疑问就是，有些代码是可以运行不会报错（网上 Seq2Seq 的教程都是这么写的），猜测是输入数据的格式问题，日后碰到了再提。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;缘起&quot;&gt;&lt;a href=&quot;#缘起&quot; class=&quot;headerlink&quot; title=&quot;缘起&quot;&gt;&lt;/a&gt;缘起&lt;/h2&gt;&lt;p&gt;Seq2Seq 的 TensorFlow 实现有很多，而 TensorFlow 之前也推出了一套新的 &lt;a href=&quot;https://ww
      
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://tobiaslee.top/tags/TensorFlow/"/>
    
      <category term="Seq2Seq" scheme="https://tobiaslee.top/tags/Seq2Seq/"/>
    
  </entry>
  
  <entry>
    <title>论文杂记</title>
    <link href="https://tobiaslee.top/2018/09/01/Recent-Paper-Notes/"/>
    <id>https://tobiaslee.top/2018/09/01/Recent-Paper-Notes/</id>
    <published>2018-09-01T02:06:38.000Z</published>
    <updated>2018-09-01T12:42:52.575Z</updated>
    
    <content type="html"><![CDATA[<p>记录最近读的一些文本生成相关的 Paper。</p><h2 id="Sentiment-Modification"><a href="#Sentiment-Modification" class="headerlink" title="Sentiment Modification"></a>Sentiment Modification</h2><p>这是一个比较有趣的问题，定义如下：</p><blockquote><p>The task of sentiment modification requires re- versing the sentiment of the input and preserv- ing the sentiment-independent content.</p></blockquote><p>但是存在几个难点：</p><ol><li>没有 Parallel 的语料，比如“松林食堂的生煎很好吃”是 positive 的，对应的 negative 例子可能就是“松林食堂的生煎不好吃”，而这种语料是很难成 pair 出现的。</li><li>没有平行语料，可能说，那我们把情感词直接换成 opposite 是不是就可以了，可能上面的例子可以 work，但是这个 “The food is cold like rock” 变成 “The food is warm like rock”就很不恰当了。</li></ol><p>为了解决这两个问题，孙栩老师组有两篇 Paper，尝试通过不同的方式来实现 Sentiment Modification 这一任务，而其核心都是通过将句子去极性后，在进行极性的添加或者说是修改。<br><a id="more"></a></p><h3 id="Unpaired-Sentiment-to-Sentiment-Translation-A-Cycled-Reinforcement-Learning-Approach"><a href="#Unpaired-Sentiment-to-Sentiment-Translation-A-Cycled-Reinforcement-Learning-Approach" class="headerlink" title="Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach"></a>Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach</h3><p>这篇文章通过 RL 的方式来实现没有 Parallel 语料场景下的 Sentiment Modification：</p><p><img src="/img/RL_S2S.png" alt="RL S2S"></p><p>比较有意思的是这个 Neutralization Module，是怎么实现这个情感词的去除呢：<strong>通过 Attention Weight</strong>。我们知道，情感分类中经常对各个 hidden state 做一个 weighted sum 来形成最后的一个 logits，而这个 <strong>weight</strong> 就能够反应其对情感极性的贡献程度。那么如果某个词的权重大于平均权重，就可以将其看做是一个情感词，去除掉之后得到不带极性的句子，<strong>这个思路非常自然</strong>。</p><p>Emotion module 的任务是根据去掉情感词的句子和要求的情感进行修改，在 pre-train 阶段通过和原句对比计算 loss 来进行基本的生成能力的训练。而后，怎么生成<strong>另一种极性</strong>的句子呢？通过 Emotion module 和 Neutralization Module 的相互提升，文章称之为 Cycled RL，将两个模块视为两个 agent（$N_\theta$ 和 $E_\phi$），具体的做法如下：</p><ul><li>让 $E_\phi$ 根据去除情感词后的输入 $\hat x$ 生成两个极性相反的文本 $\overline x$ 和 $x$，而通过这两个句子来计算一个 reward。这个 reward 需要考虑到两点：文本的质量和情感极性是否正确，因此其 Reward 最后是一个 BLEU 和 sentiment classifier 给出概率的调和平均值。这个 reward 就作为 guide signal 来负责提升 $ N_\theta $。</li><li>对于 $E_\phi$ 来说，原句依旧可以视作是一个 supervision，因而其目标就是通过 $N_\theta$ 给出的去除极性的句子来 recover 原句，通过生成原句 log-prob 作为 reward 来进行 $E_\phi$ 的训练。</li></ul><p>这样的方法在几个数据集上都取得了不错的效果，但是在 Sentiment Accuracy 方面有所欠缺；同时，作者也提出对于一些具有 sentiment-conflict （自相矛盾，e.g. “Outstanding and bad service”）和中性句的效果不好（因为没有情感词可去），这也是意料之中。</p><h3 id="Learning-Sentiment-Memories-for-Sentiment-Modification-without-Parallel-Data"><a href="#Learning-Sentiment-Memories-for-Sentiment-Modification-without-Parallel-Data" class="headerlink" title="Learning Sentiment Memories for Sentiment Modification without Parallel Data"></a>Learning Sentiment Memories for Sentiment Modification without Parallel Data</h3><p>对于 Emotion Word 的选取和上篇文章是一样的思路，首先选出 emotion word，加权求和得到一个向量表示 $s^{pos}$ （以 positive 为例子），而后利用此计算一个情感记忆 $\textbf{M}^{pos}$ 来辅助 Seq2Seq 结构 Decoding 阶段：</p><p><img src="/img/si.png" alt="Sentiment Information"></p><p>而 Context 信息则由 $ 1- weight$ 的加权和得到：</p><p><img src="/img/s_context.png" alt="Context"></p><p>再以此做了一个类似 attention 的操作，从而能够从情感记忆中提取出和上下文搭配的情感词来。最终得到一个 $\tilde c$ 作为 decoder 的 initial state。</p><p>相比之前的 Cycled RL，这篇基于 Seq2Seq 进行 Sentiment Modification 的文章在 Decoder 的 initial state 上做了些文章，提供关于 sentiment 的信息在里面，帮助 decoder 生成具有极性的文本。</p><h2 id="IRL-For-Text-Generation"><a href="#IRL-For-Text-Generation" class="headerlink" title="IRL For  Text Generation"></a>IRL For  Text Generation</h2><p>IRL，Inverse Reinforcement Learning 逆向强化学习，其核心是在于通过<strong>学习 reward function</strong> 而不是 manually defined 来指导 policy 的更新。其框架又和 GAN 非常的类似，其元素有一种对应关系：</p><table><thead><tr><th>IRL</th><th>GAN</th></tr></thead><tbody><tr><td>Trajectory $\tau$</td><td>sample $x$</td></tr><tr><td>Policy  $ \pi $</td><td>Generator $G_\theta$</td></tr><tr><td>Reward $r_\phi$</td><td>Discriminator $D_\phi$</td></tr></tbody></table><p>既然原先 GAN 能用在文本生成，那么 IRL 也可以；复旦大学邱锡鹏老师最近的一篇 Paper 《Toward Diverse Text Generation with Inverse Reinforcement Learning》就尝试将 IRL 框架和文本生成结合，进行尝试：</p><p><img src="/img/irl_tg.png" alt="IRL Text Generation"></p><p>实现思路其实也很简单，将原来 SeqGAN 中的 Discriminator 换成 IRL 中的 Reward Approximator，由此得到一个 $r_\phi(s_t, a_t)$，对一个文本序列进行求和得到对应的 $R_\phi(\tau)$ ，据此对 Generator 进行更新即可。但是用 IRL 有什么好处呢？</p><ul><li><p>解决 Reward Sparsity 问题：这里的稀疏可能不是类似传统 RL 中很多个 action 都没有 reward，我认为是<strong>区分度</strong>的问题，如 DP-GAN 中所谈到的，很多时候 Discriminator 的 reward 不是 0 就是 1，这种极端的分布会致使 Generator 得不到进化的动力，即其 <strong>guide signal</strong> 比较弱，致使生成的文本质量不高。文章也谈到了一些工作来增强这个信号，比如 LeakGAN、RankGAN 等。而 IRL，如果能够学到一个比较好的 reward function 而不是通过一个很强力的 classifier 来做出非黑即白的判断的话，确实是能够缓解这一问题，而这一论点文章尚无实验（reward plot）来支撑，有待后续继续研究。</p></li><li><p>解决 Mode Collapse 问题：这是一个老大难问题，我对于此的观察和理解是：GAN-based model 倾向于生成重复的句子，因为在最大化生成器的目标函数时，会将 G(X) （生成某个句子的概率）推向较大的一侧，即少数的几种句子 domain 整个生成空间。而基于这种理解的一种解决方案就是 SentiGAN 中的 Penalty-based Objective Function，迫使 G(X) 变小，更为均匀的分布，来缓解 mode collapse 问题，<strong>这一点在我的实验观察中得到了验证</strong>。IRL 中 Generator 的 Objective Function 是两项之和，前一项可以看做是最大化生成的文本的 reward，和 GAN 类似。后一项则被称之为 entropy term，作者说这一项能够鼓励 Generator 生成多样的文本。</p><p><img src="/img/entropy_term.png" alt="Entropy Term"></p><p><strong>利用熵的极值性，当所有事件的概率都相同时，熵最大（即事件发生的不确定性最大）</strong>，这样概率分布就趋向于均匀，可以视作缓解 mode collapse 问题。</p></li></ul><h2 id="Query-and-Output"><a href="#Query-and-Output" class="headerlink" title="Query and Output"></a>Query and Output</h2><p>前面所述的三篇文章都有一个共同点，即在进行文本生成的时候，都是生成一个<strong>词表上的概率分布</strong>，对其进行采样得到生成的词，而这会带来什么问题呢？孙栩老师组的 《Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generation》指出两点：</p><p>首当其冲地，参数量巨大：由低维（一般数百左右） hidden representation 映射到 word space （高维，一般词表大小在数万左右），其参数量是巨大的（例如用一个全连接层做 map，二者相乘，即是百万级别）。这会带来训练速度缓慢，收敛困难的问题；其次，word distribution 是一个巨大的 vector，其每个位置上代表了对应词的概率，而这个概率与概率之间是没有什么关联的（原文中写的是 irrelevant to each other，我觉得只是没有显式地考虑其他词的概率，事实上这个 distribution 是从 hidden representation 得来，其中应该包含这个关联信息），因而无法学到语义上的关系。</p><p>为了解决这两个问题，这篇文章提出了利用 query 方式来进行 word 生成。</p><p>query 向量由当前的 hidden state 和 cell state 构成：</p><p>$$q_t = tanh(W_c[s_t;c_t])$$</p><p>而要查询的 Key-Value Pair 对应的就是 word $w_i$与 对应的 word embedding $e_i$ ，通过一个 score function：</p><p><img src="/img/aef.png" alt="Attention Engergy Function"></p><p>最后通过 softmax 层得到词表分布：</p><p>$$ p(y_t) = softmax( f(q_t, e_i)) $$</p><p><strong>这种检索式的生成带来的显著好处就是参数量的下降</strong>，和原先百万级别的全连接层参数相比，这里至多只有 $W_q$ 、 $W_e$ （concatl）和一个 $v$，假设 hidden size 和  embedding size 都是 256，其参数量也就是 256 x 256 x 2 + 256，在十万左右。不过这也意味着我们需要对每一个 word embedding 都做一次 query，似乎是在用空间换时间；但是好在可以通过向量并行化进行操作，所以最终的收敛速度还是要优于原先的 Seq2Seq。同时，这个 query 是在 word embedding 层面上做的，<strong>各个词之间的 relation 能够得到得到一个强化</strong>（比如 semantic meaning 类似的词概率 query 后的分数比较相近），对应地缓解先前的各个词的概率关联度不高的问题。</p><p>另外，可以看出，这种 Query 式的 Attention 有一点《Attention is All you Need》的味道在里面。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录最近读的一些文本生成相关的 Paper。&lt;/p&gt;
&lt;h2 id=&quot;Sentiment-Modification&quot;&gt;&lt;a href=&quot;#Sentiment-Modification&quot; class=&quot;headerlink&quot; title=&quot;Sentiment Modification&quot;&gt;&lt;/a&gt;Sentiment Modification&lt;/h2&gt;&lt;p&gt;这是一个比较有趣的问题，定义如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The task of sentiment modification requires re- versing the sentiment of the input and preserv- ing the sentiment-independent content.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但是存在几个难点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;没有 Parallel 的语料，比如“松林食堂的生煎很好吃”是 positive 的，对应的 negative 例子可能就是“松林食堂的生煎不好吃”，而这种语料是很难成 pair 出现的。&lt;/li&gt;
&lt;li&gt;没有平行语料，可能说，那我们把情感词直接换成 opposite 是不是就可以了，可能上面的例子可以 work，但是这个 “The food is cold like rock” 变成 “The food is warm like rock”就很不恰当了。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了解决这两个问题，孙栩老师组有两篇 Paper，尝试通过不同的方式来实现 Sentiment Modification 这一任务，而其核心都是通过将句子去极性后，在进行极性的添加或者说是修改。&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="Text Generation" scheme="https://tobiaslee.top/tags/Text-Generation/"/>
    
      <category term="Paper Notes" scheme="https://tobiaslee.top/tags/Paper-Notes/"/>
    
  </entry>
  
  <entry>
    <title>My Sophomore Year</title>
    <link href="https://tobiaslee.top/2018/08/24/My-sophomore-year/"/>
    <id>https://tobiaslee.top/2018/08/24/My-sophomore-year/</id>
    <published>2018-08-24T12:52:21.000Z</published>
    <updated>2018-08-25T02:51:34.346Z</updated>
    
    <content type="html"><![CDATA[<p>在从 DeeCamp 回来的高铁上，翻着开复老师新书内部版的我，想到大二就这样结束了，一些瞬间从脑海中闪过，拿出手机，开始码字。</p><p>从最近的事情说起吧，刚结束的中国高校人工智能夏令营，我作为为数不多的大二学生之一，带领团队做出了能够欺骗开复老师眼睛的嘻哈歌词机器人。虽说有一定讨巧成分，但这个应该够我吹一年了。在夏令营听了各路大咖老师的讲座，也和一线的工程师老师一起为项目奋斗了一个月，收获不必说，更多的是眼界上的开阔。我现在觉得当初选择拥抱人工智能的决定无比正确，未来十年，是属于AI的十年，不拥抱浪潮的结果就是被取代。去年暑假也随学院队伍去了深圳，但那时候的我们都是无名之辈，大公司都不愿意待见；这次，300名营员中不仅清北复交满地走（甚至很多人没听说过西电），还有KDD（世界顶级数据挖掘会议）比赛冠军和顶会作者，甚至少年班的天才也有几个。虽然西电层次不高，但出来的同学们也都是非常优秀，给学校涨了不少脸。如此质量的生源，待遇自是不可同日而语：HR 亲切问候，恨不得马上把我们抓到公司去；各大独角兽公司也都敞开大门，拿出如数家珍生怕在这里掉了价。跳出西电这一隅，发现原来外面的世界这么大，有那么多优秀的同学在向着同一目的地奋力前进，我感到很高兴，也很自豪能和他们成为伙伴。 </p><p>再谈谈科研方面，进展如期，相信用不了就会出结果，虽然不是顶级会议期刊，却也是我迈出的第一步。这次夏令营也从很多同学身上做到了做研究的方法，更多是方法论的东西，也是学长学姐的血汗经验，之前也有提到，尝试用在下一篇作品上，相信会更上一层楼。另外，西电做自然语言处理的同学不多，一度让我觉得很孤独。而在北大的一个月，到计算语言所和清华智能实验室都蹭了报告，我了解到，人不多一方面是因为西电的传统（主要做雷达出身，遥感图像领域），还有一方面是因为问题很多，很难解决；并且自然语言处理处在上升的阶段，这颗人工智能皇冠上的明珠在不久的将来就能释放其巨大的能量。选择投身于此，学术上努力做一些创新的基础工作，未来工作后将其应用在各种产品之上进行落地，造福全世界的所有人，是多么有意义的事情啊。 </p><p>最后，竞赛和成绩方面。去年觉得要在竞赛上下点功夫，今年也在意料之中的，拿了一些奖项。其实最初的目的就是不想让简历上显得太过寒碜。现在心态好很多，因为参与过之后，发现很多竞赛水分很大，拿奖与否更多是一个运气问题，而非实力问题。当然，做到最好同样是需要付出很大努力的，这一点毋庸置疑。在成绩方面，是看得越来越淡：100分和99分是一样的，98分和95分也没多大差别，重要的是自己从中有所收获。GPA 只是衡量一个人的一个方面，但他也仅仅是一个指标而已，太过功利于己与人都不好；并且，换个角度，GPA的投入产出比太低了，为了那么一两分削减脑袋 还不如多读两本书看两部电影或者是两篇Paper呢。 </p><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p><strong>打铁还要自身硬</strong>，接下来的四个学期，要做的事情其实很简单：</p><ul><li>把手头的工作做好，保持和老师的联系，希望能拿到一张保研的入场券</li><li>没有体育课了，主动增加运动量，避免读了四年书胖了四十斤这种尴尬的事情发生在我的身上（逃</li><li>提升算法能力</li></ul><p>顾城曾说：</p><blockquote><p>一个彻底诚实的人是从不面对选择的，那条路永远会清楚无二地呈现在你面前，这和你的憧憬无关，就像你是一颗苹果树，你憧憬结橘子，但是你还是诚实地结出苹果一样。</p></blockquote><p>知道自己要走的路了之后，就一往无前吧~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在从 DeeCamp 回来的高铁上，翻着开复老师新书内部版的我，想到大二就这样结束了，一些瞬间从脑海中闪过，拿出手机，开始码字。&lt;/p&gt;
&lt;p&gt;从最近的事情说起吧，刚结束的中国高校人工智能夏令营，我作为为数不多的大二学生之一，带领团队做出了能够欺骗开复老师眼睛的嘻哈歌词机器
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>DeeCamp 2018 - AI 有嘻哈，惊掉你下巴</title>
    <link href="https://tobiaslee.top/2018/08/23/Generate-hip-pop-lyrcis-using-GAN/"/>
    <id>https://tobiaslee.top/2018/08/23/Generate-hip-pop-lyrcis-using-GAN/</id>
    <published>2018-08-23T02:27:08.000Z</published>
    <updated>2018-08-25T07:16:30.851Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>非常有幸参与了 2018 Dee Camp 人工智能夏令营，在这一个月的时间里面，我们享受了来自学界、业界各种大咖的分享，同时也和导师一起参与到一个项目中，将所学知识进行落地。我们组的题目是 AI 有嘻哈，利用相关技术进行嘻哈歌词的生成。</p><p>先来看看模型的效果，给定第一句，生成接下来的几句，猜猜下面哪句是模型生成的哪句是原作？</p><blockquote><p> 不是乐理专修 做点儿曲式研究 我们的力量来自宇宙 自己的节奏</p><p>不是乐理专修 所有听的观众 打破他们传统 进到环球 继续让你感受</p></blockquote><p>再来一个：</p><blockquote><p>自己就带上了有色眼镜 金钱摧毁多少事情 瓦解你的中枢神经</p><p>自己就带上了有色眼镜 我只想把世界分的更清 却发现自己却模糊了心</p></blockquote><p>正确答案是：<strong>第一行都是模型生成的，第二行是原作</strong>，你答对了吗？有没有被惊艳到，我们组内的测试结果显示，大家的正确率不超过 30%，值得一提的是：<strong>输入都是来自于测试集中</strong>，没有参与到模型的训练中。</p><p><strong>因为某些原因，代码和数据暂时无法开源，以后有机会会整理出来</strong>。</p><p><del>更多的例子和项目的代码已经上传到 <a href="https://github.com/TobiasLee/Chinese_Hip-pop_Generation" target="_blank" rel="noopener">Github</a>，欢迎大家 Star。</del></p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>创新工场的导师为我们提供了 10 w 条嘻哈歌词，并且已经将一些不符合社会主义核心价值观的句子标注了出来。数据的预处理主要步骤如下：</p><ul><li>在对句子进行筛选之后，我们利用 <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">Jieba</a> 进行分词，观察到单句长度集中在 8~10 左右；</li><li>在利用 Tensorflow 中的 Tokenizer 进行 tokenize 并构建 word2idex 字典后，词表大小在 11000 左右，考虑到这个大小还可以接受，没有做限制词表大小的操作；</li><li>利用 <code>pad_sequence</code> 将句子 padding 到 20（和 SeqGAN 中相同）；</li><li>构建 x-y pair，利用上一句预测下一句（导师后来建议可以借鉴用 Skip-gram 的思路，同时预测上一句和下一句，但没有时间去尝试了），分割数据集</li></ul><p><del>知道大家都不喜欢洗数据，所以我们的训练数据也已经放到了 <a href="https://drive.google.com/drive/folders/1QrO0JAti3A3vlZlUemouOW7jC3K5dFZr" target="_blank" rel="noopener">Google Drive</a>，大家自取即可。</del></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>我们的生成模型的整体基于 SeqGAN，并对其做了一些修改，模型架构如下：</p><p><img src="/img/AI-hippop.jpg" alt="AI-Hippop"></p><p>主要改动有两点：</p><ol><li><p>增加输入语句的编码：这一点类似 Seq2Seq 的 Encoder，SeqGAN 原本的 initial state 是全 0 的，为了将上文的信息传递给生成器，我们采用了一个简单的全连接层（Fully Connected Layer），将输入句子的 Word Embedding 经过一个线性变化之后作为生成器的 LSTM。事实上也可以尝试使用 RNN（LSTM）来作为 Encoder，不过这样模型的速度可能会比较慢。</p></li><li><p>将原先 Generator 的 Loss Function 改为 Penalty-based Objective：在训练模型的过程中我们发现，模型在 Adversarial Training 多轮之后出现了严重的 mode collapse 问题，比如：</p><blockquote><p>别质疑自己<br>遮罩错的消息不要过得消极<br>世间人都笑我太疯癫<br>世间人都笑我太疯癫</p><p>​</p><p>守护地狱每座坟墓<br>世间人都笑我太疯癫<br>你不知道rapper付出多少才配纸醉金迷<br>世间人都笑我太疯癫</p><p>​</p><p>但却从来没有心狠过<br>如果你再想听<br>你不知道rapper付出多少才配纸醉金迷<br>你不知道rapper付出多少才配纸醉金迷</p></blockquote><p>可以看到 <code>世间人都笑我太疯癫</code> 和 <code>你不知道rapper付出多少才配纸醉金迷</code> 占据了我们生成的结果。mode collapse，简单来说就是输入的改变不会影响生成的结果。为此我们调研了一些 Paper，最终采用了<a href="https://www.ijcai.org/proceedings/2018/0618.pdf" target="_blank" rel="noopener">SentiGAN</a> 中提出的 Penalty-based Objective Function：</p><p><img src="/img/pbof.png" alt="Penalty based Objective Function"></p><p>SeqGAN 是将原先 Discriminator 的认为 Generator 生成句子为真的概率作为奖赏，Generator 通过最大化奖赏（最小化其相反数）来更新自己的梯度；而 SentiGAN 则恰好相反，将 Discriminator 判断为假的概率作为惩罚，Generator 通过最小化惩罚来更新自己，并且去掉了概率上的 log 函数。作者分析其起作用的效果在于：一、去掉 log，可以视作 WGAN 中的 generator loss（这一点我持怀疑态度，因为代码中并没有对梯度进行裁剪来满足 WGAN 所需的 Lipschitz 条件）；二、惩罚和奖赏存在一个 $ 1- V = D$ 的关系，根据 $ G(X|S;\theta_g) V(X) = G(X|S;\theta_g)(1-D(X;\theta_d) )=G(X|S;\theta_g) - G(X|S;\theta_g)D(X;\theta_g)$ ，这样会让 generator 偏向于一个更小的 $G(X|S;\theta_g) $ ，即概率比较小的句子，从而避免生成很多重复（概率比较大）的句子。Anyway，在我们的模型上，SentiGAN 的 loss 立竿见影，更多的细节请大家参考原作。</p></li></ol><h2 id="押韵"><a href="#押韵" class="headerlink" title="押韵"></a>押韵</h2><p>嘻哈歌词非常重要的一个特点就是句与句之间的押韵，我们在实现这一功能的时候尝试了两种方案：</p><ol><li>Reward based，在 reward 函数上增加额外的押韵奖赏项， $ \  r_{rhyme}$：对 Generator 的生成的句子和输入的句子进行押韵的判断，如果押韵，则提供额外的奖赏。</li><li>Rule-based，生成时只对押韵的词进行采样：在生成句尾的词的概率分布时候，通过获取和输入句尾押韵的词，只在这些押韵的词进行采样。</li></ol><p>方法一，如果能够通过设计 reward function 就能实现押韵的功能，那模型就是完全 end2end，非常 fancy 了。但是理想很丰满，现实很骨感，经过几天的调整押韵奖赏的权重，都没能看到押韵率（我们设置的用于检测押韵奖赏效果的指标，每个 batch 中和 input 押韵的句子的比例）的上升 。我们怀疑是<strong>这种奖赏的结合会让 Generator 产生混淆，并不能明确自己 reward 来自何处</strong>，应该需要更加具体的一些限制才能够实现这一方法。</p><p>方法二，一开始我是拒绝这么做的，用基于规则的方法不是我的理想。</p><p><img src="/img/zx.gif" alt="真香"></p><p>但是为了做出产品来，我还是屈服了。但还有一个问题摆在面前：怎么知道生成的是句尾呢?导师提醒我们，我们可以把<strong>输入倒过来</strong>。这是 NMT 中常用的一个手段，对于 LSTM，句子是真的还是反的差别不大，即使有差别，也可以通过一个 Bi-LSTM 来捕获不同顺序的信息。而为了知道哪些字词是押韵的，我们实现制作了一张 <code>vocab_size x vocab_size</code> 的大表 <code>rhyme</code>，如果两个词（index 分别为 i, j）押韵，则 <code>rhyme[i, j]</code> 非 0，否则为 0。</p><p><img src="/img/rhyme_module.png" alt="Rhyme Mask"></p><p>如上图所示，如果我们的输入为“你真美丽”，句尾词为“美丽”，韵脚为 i；最终采样结果只会在押韵的词中采样，示例的采样结果为“春泥”。</p><p>据此，我们就可以对生成过程的第一个词的词表分布进行一个 mask 操作，使得非押韵的词的概率都变成 0，就能够保证押韵了，代码片段如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取 input 的最后一个词</span></span><br><span class="line">first_token = self.inputs[:, <span class="number">0</span>]  <span class="comment"># (batch_size, 1)</span></span><br><span class="line"><span class="comment"># 控制押韵的概率, 现在设置为 1.0 ，即 100% 押韵</span></span><br><span class="line">select_sampler = Bernoulli(probs=<span class="number">1.0</span>, dtype=tf.bool)</span><br><span class="line">select_sample = select_sampler.sample(sample_shape=self.batch_size)</span><br><span class="line"><span class="comment"># 获取对应的 index 押韵行</span></span><br><span class="line">token_rhyme = tf.cast(tf.gather(self.table, first_token), tf.float32)</span><br><span class="line"><span class="comment"># 进行 mask</span></span><br><span class="line">prob_masked = tf.where(select_sample, tf.log(tf.multiply(token_rhyme, tf.nn.softmax(o_t))),tf.log(tf.nn.softmax(o_t)))</span><br><span class="line"><span class="comment"># 根据 mask 之后的概率分布进行采样</span></span><br><span class="line">next_token = tf.cast(tf.reshape(tf.multinomial(prob_masked, <span class="number">1</span>), [self.batch_size])</span><br></pre></td></tr></table></figure><p>不过这个制表的过程比较耗费时间（大约跑了 3 个小时，i7）。另一种思路是可以根据韵脚对字词进行分类，将相同韵脚的词的 index 编到一起，这样我们可以通过获取每个词的韵脚来知道目标词的范围，而不用挨个的去判断是否押韵。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在 DeeCamp 的一个月里，除了讲座以外，更让我觉得收获许多的是认识了很多非常优秀的小伙伴，以及开拓了自己的视野，看到了更大的舞台。我愈发地坚信，<strong>未来的十年，是属于人工智能的，是属于愿意投身于这波浪潮中我们的</strong>。也再次向大家安利一波 <a href="https://challenger.ai/" target="_blank" rel="noopener">DeeCamp</a>，如果你对人工智能感兴趣，明年暑假，千万别错过~ 最后希望 DeeCamp 能够一直办下去，并且越办越好！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;非常有幸参与了 2018 Dee Camp 人工智能夏令营，在这一个月的时间里面，我们享受了来自学界、业界各种大咖的分享，同时也和导师一起参
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Text Generation" scheme="https://tobiaslee.top/tags/Text-Generation/"/>
    
      <category term="SeqGAN" scheme="https://tobiaslee.top/tags/SeqGAN/"/>
    
  </entry>
  
  <entry>
    <title>Controllable Text Generation Notes</title>
    <link href="https://tobiaslee.top/2018/08/10/Controllable-Text-Generation-Notes/"/>
    <id>https://tobiaslee.top/2018/08/10/Controllable-Text-Generation-Notes/</id>
    <published>2018-08-10T06:44:13.000Z</published>
    <updated>2018-12-21T02:26:10.571Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Controllable-Text-Generation"><a href="#Controllable-Text-Generation" class="headerlink" title="Controllable Text Generation"></a>Controllable Text Generation</h2><p>文本生成，包括机器翻译、文档摘要、Image Caption 在内，是一个非常广泛的问题，而这篇文章所讲述的文本生成，并不是类似机器翻译一样有具体的目的，有 x-y pair 的监督学习任务，而是侧重于无监督的生成模型。而这生成模型是<strong>可控</strong>的，即并不是仅仅根据 corpus 进行学习语言模型进行分布的模拟（SeqGAN），同时我们也希望能够对于生成的文本具备一定的控制能力，例如，我们可以控制生成的文本和某个主题相关，又或者是具有某种情感极性，在 charbot 这样的场景下，能够做到这到这一点是非常重要的。本文从三种模型架构出发，介绍关于可控文本生成的一些工作。<br><a id="more"></a></p><h2 id="GAN-based——SentiGAN"><a href="#GAN-based——SentiGAN" class="headerlink" title="GAN-based——SentiGAN"></a>GAN-based——SentiGAN</h2><p>基于 SeqGAN 框架的文本生成工作有很多，但之前的工作主要关注点在于解决 SeqGAN 生成文本过短、mode collapse 问题等问题，而 SentiGAN 则是基于 GAN 框架少有的可控文本生成的工作。</p><p>很有幸邮件和 SentiGAN 的作者<a href="https://nrgeup.github.io/resume/" target="_blank" rel="noopener">王科</a>学长交流过，并且还面基蹭了一顿饭。<a href="https://www.ijcai.org/proceedings/2018/0618.pdf" target="_blank" rel="noopener">SentiGAN</a> 这篇文章也被评为了 IJCAI 的 Distinguished Paper，这篇文章的 motivation 是基于 GAN 框架来进行带有情感极性的文本生成，模型结构如下：</p><p><img src="/img/sentigan.png" alt="SentiGAN"></p><p>模型的一大亮点是在于使用多个 Generator 来负责各种情感极性的生成，也即我们可以选用不同的 generator 来生成不同极性的文本，达到控制的目的。以及相应地利用判别器来判别生成的文本是否具有对应的情感极性，这和原先 SeqGAN 中只负责判断文本的真假与否相比，对生成的文本提出了更高的要求。另外一大亮点是 Generator 的 reward 的设计上，采用了 <strong>Penalty based objective function </strong>：</p><p><img src="/img/pbof.png" alt="Penalty based Objective Function"></p><p>原先 SeqGAN 中，Generator 希望使判别器认为自己生成文本为真的概率最大化，并且使用的是 log probability，这就是直接将 MLE 的梯度根据 reward 值进行加权；而在 SentiGAN 中，首先去掉了 log，并且是希望将判别器认为自己为假的概率最小化。作者认为这样能够缓解 mode collapse 问题：去掉 $\log$，这和 WGAN 中 的设置类似，作者也在论述中提到了这一点，不过比较奇怪的是，作者开源的<a href="https://github.com/Nrgeup/SentiGAN" target="_blank" rel="noopener">代码</a>中并没有梯度的限制（WGAN 的一个实现 Lipschitz 条件的手段），不过问了学长，说是代码还会再更新，日后再看。接着没有让 Generator 来最大化判别器判断其为真的概率，而是最小化判别器判断其为假的概率，称之为 Penalt-based Obejctive Function，作者给出了简单的一个证明：</p><p>$ G(X|S;\theta_g) V(X) = G(X|S;\theta_g)(1-D(X;\theta_d) )=G(X|S;\theta_g) - G(X|S;\theta_g)D(X;\theta_g)$</p><p>并认为这样会让 generator 偏向于一个更小的 $G(X|S;\theta_g) $ ，即概率比较小的句子，从而避免生成很多重复（概率比较大）的句子。<strong>就我个人的实验效果来看，确实对 mode collapse 有很大缓解，但究竟是去 $\log$ 还是 Penalty 带来的效果有待细考</strong>。</p><h2 id="C-VAE"><a href="#C-VAE" class="headerlink" title="C-VAE"></a>C-VAE</h2><p><a href="http://arxiv.org/abs/1703.00955" target="_blank" rel="noopener">Towards Controlled Generation of Text</a>，是我见到的第一篇把 Controlled 写进题目中的文本生成文章。作者基于 VAE 进行可控的文本生成，模型架构如下：</p><p><img src="/img/cvae.png" alt="C-VAE"></p><p>VAE 做文本生成的步骤简要说明如下：</p><ul><li>将原句 $x$ 通过 Encoder 进行编码，得到 latent variable 表示 $z$ （通过 mean value 和 std 生成）</li><li>将 $ z $ 交给一个 Generator 进行文本生成，得到 $\hat x$ </li><li>根据 $x$ 和 $\hat x$ 计算重构 loss，来更新 Generator 和 Encoder</li></ul><p>这和 Seq2Seq 结构的 Encoder-Decoder 结构是非常类似的，但 VAE 一个比较好的性质就在于我们可以通过操作 $z$ 来变化输出的结果，我个人的理解就是讲文本映射到一个高维的语义空间中，而在这个空间中文本的分布是近似连续的：比如我们可以通过 $z$ 的线性变化实现文本极性的转变。而这种连续性这就为控制文本提供了空间，于是作者在隐变量 $z$ 表示上挖了一个空叫做 strucutred code $c$，并且认为 $c$ 这一部分：</p><blockquote><p>without entangling with other attributes, especially those not explictly modeled</p></blockquote><p>换句话说，句子的某些属性（例如情感极性、时态）只和 $c$ 有关，$c$ 一样，则这些属性就是一样的。而为了让生成器能够生成具有相应属性的文本，就需要在原有的 Loss 上加上和属性相关的项。作者从两方面施加了限制：</p><ol><li><p>生成的文本具有特定的属性，这很简单，再训练一个分类器来判断是否具有这样的属性即可，这里的分类器就称之为 Discriminator，定义的 attribute loss $L_{attr, c}$ 如下：</p><p><img src="/img/attr_c.png" alt="Attribute Loss-c"></p></li><li><p>$z$ 和 $c$ 的功能尽可能分离，即 $c$ 负责文本的属性，而 $z$ 则需要建模文本的其他方面，比如我们不感兴趣的属性以及语义的通顺性等。我们希望根据生成的句子能够恢复出 $ z$ 来，定义的 attribute loss $L_{attr, z} $ 如下：</p><p><img src="/img/attr_z.png" alt="Attribute Loss-z"></p></li></ol><p>结果的 loss 就是三者之和：</p><p>$ L_{G} = L_{VAE} + \lambda_c L_{attr, c} + \lambda_z L_{attr, z} $</p><p>通过 minimize 这个 loss，达到上述的目的。</p><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>Seq2Seq 也是一种广泛应用的生成模型，在机器翻译上有很多作品，框架也基本上是 <code>Encoder-Decoder + Attention</code> 。在这介绍两个问题：根据属性生成评论以及根据主题生成回复。</p><h3 id="Learning-to-Generate-Product-Reviews-from-Attributes"><a href="#Learning-to-Generate-Product-Reviews-from-Attributes" class="headerlink" title="Learning to Generate Product Reviews from Attributes"></a>Learning to Generate Product Reviews from Attributes</h3><p>作者提出了一个比较有趣的问题，过去我们关注的是从评论中挖掘情感，比如商品的质量的好坏，但反过来，根据商品的属性生成对应的评论，很少被大家所关注（可能也没什么用，（逃 ）。模型的架构如下：</p><p><img src="/img/a2s.png" alt="Attributes To Review"></p><ul><li>Encoder：作者利用一个全连接层，对评论的属性（用户 id、产品、评分）进行编码，将编码后的 vector 拼接后经过 tanh 层作为 Decoder 的 initial state。</li><li>Decoder：作者使用了 MultiLayer-LSTM 作为 Decoder，并将最后一层 LSTM 的输出作为最终的输出用于 review token 的生成</li><li>Attention：考虑到 Encoder 编码的属性信息在 LSTM 传递过程中可能逐渐衰减，为了提高信息的利用率，作者引入了 Attention 机制，在生成 token $y_t$ 的时候，将之前的属性的向量经过 attention 得到 $c_t$，并将 $c_t$ 和 $h_t$ 做一个加权之后经过 tanh 层得到最终的 hidden state $h_t^{attr}$，通过 softmax 层得到词表分布，采样得到词表分布</li></ul><p>问题：作者的衡量指标使用的是 BLEU，而 BLEU 事实上并不是一个非常好的衡量指标（不过也没能找到更好的），上次听 William Wang 老师报告时他举了一个例子：</p><blockquote><p>We had a great time to have a lot of the. They were to be a of the. They were to be in<br>the. The and it were to be the. The, and it were to be the.</p></blockquote><p>这个句子的 BLEU 值可能很高，但是并不 make sense。所以，<strong>文本生成的衡量指标</strong>是一个非常值得深究的话题，也只有找准了这个 target，进一步的长文本生成才能有的放矢。</p><h3 id="Topic-Aware-Neural-Response-Generation"><a href="#Topic-Aware-Neural-Response-Generation" class="headerlink" title="Topic Aware Neural Response Generation"></a>Topic Aware Neural Response Generation</h3><p>这篇文章关注的是 Chatbot 中一个另一个问题：如何从输入中捕获主题信息，并据此进行回复的生成。作者举了一个例子：</p><blockquote><p>Input: My skin is dry</p><p>Output: Me too. </p><p>Human: You may need to  hydrate and moisturize your skin.</p></blockquote><p><code>Me too</code> 这样的词在之前 Jiwei Li 的文章中也有提到，是一种 trival 的回复，万金油；但如果我们希望微软小冰能够做到和人一样，告诉你需要补水，那么就必须从输入中挖掘出 <code>skin</code> 这个主题，并据此产生回复。模型的架构如下：</p><p><img src="/img/ta-seq2seq.png" alt="TA-Seq2Seq"></p><p>如何获得输入中的主题呢？通过主题建模 LDA 的方法，抽出主题词来。作者用了 Twitter LDA 模型来实现这一过程。具体到三个组件：</p><ul><li><p>Encoder: 首先是对输入的句子进行编码， 作者采用的 Bi-GRU 作为 Encoder，并将两个方向的 $h_t$ 拼接在一起作为 hidden state；拿到抽出的主题词之后，进行 embedding，准备交给 Decoder</p></li><li><p>Decoder 和 Attention: 这里的 Attention 作者称之为 Joint  Attention，分为两个部分：</p><ul><li>Message Attention: 就是我们常用的 Attention，得到一个 Context Vector $c_t$</li><li>Topic Attention: 作者将 Topic Embedding 做了一个 Attention 之后构成一个 $o_t$，而这个 Attention 的权重的计算是根据 topic embedding 、前一个输出的词、最后一个 hidden state $h_T$计算得到。和传统的 Attention 对比，作者认为加入 $h_T$ ，其代表的是语句的整体信息，能够对主题词权重计算起到帮助作用，比如减少不相关主题的权重和增加相关主题的权重。</li></ul><p>最后，$c_t$ 和 $o_t$ 参与到 $s_i$ 的计算： $ s_i = f(c_i, o_i, y_{i-1}, s_{i-1}) $</p></li><li><p>词表分布的计算：作者又在最终的词表分布上根据主题词进行一个类似偏置操作（真是花里胡哨），$p(y_i) = p_V(y_i) + p_K(y_i)$，前者就是我们根据 $s_i$ 和 $y_{i-1}$ 计算得到的词表分布，而后者则再一次利用主题词的信息，对词表分布进行一个修正，但比较奇怪的一点是，作者再一次利用了 $c_t$ ，但我认为 $c_t$ 并没有携带 Topic 的信息。<strong>或许是希望考虑 $s_t$ 中的主题词信息和 $c_t$ 的关联？</strong> </p></li></ul><p>问题：作者做了三个实验，但并没有针对其 Claim 设置相关的支撑实验，比较侧重于几个指标提升，我觉得并不是很信服，而且这个 model 在词表分布上的 bias 的设置，让我很怀疑前面一系列设置是否有意义？</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这篇文章拖了很久，总算是完成了。而在整个阅读过程中，我也感受到读 Paper 的速度变快，以及关注点的改变：以前重理解模型和关注其效果，现在的关注点不在于指标多高，而在于作者为什么这么设置模型，会去看作者的 motivation 和 claim，以及实验的设置能否支撑起他的说明。也愈发的感受到，写出一篇好文章和灌水文章的差距。加油！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Controllable-Text-Generation&quot;&gt;&lt;a href=&quot;#Controllable-Text-Generation&quot; class=&quot;headerlink&quot; title=&quot;Controllable Text Generation&quot;&gt;&lt;/a&gt;Controllable Text Generation&lt;/h2&gt;&lt;p&gt;文本生成，包括机器翻译、文档摘要、Image Caption 在内，是一个非常广泛的问题，而这篇文章所讲述的文本生成，并不是类似机器翻译一样有具体的目的，有 x-y pair 的监督学习任务，而是侧重于无监督的生成模型。而这生成模型是&lt;strong&gt;可控&lt;/strong&gt;的，即并不是仅仅根据 corpus 进行学习语言模型进行分布的模拟（SeqGAN），同时我们也希望能够对于生成的文本具备一定的控制能力，例如，我们可以控制生成的文本和某个主题相关，又或者是具有某种情感极性，在 charbot 这样的场景下，能够做到这到这一点是非常重要的。本文从三种模型架构出发，介绍关于可控文本生成的一些工作。&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Text Generation" scheme="https://tobiaslee.top/tags/Text-Generation/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>DeeCamp 第一周随记</title>
    <link href="https://tobiaslee.top/2018/07/29/DeeCamp-1st-Week-Feeling/"/>
    <id>https://tobiaslee.top/2018/07/29/DeeCamp-1st-Week-Feeling/</id>
    <published>2018-07-29T04:45:51.000Z</published>
    <updated>2018-07-29T07:41:12.787Z</updated>
    
    <content type="html"><![CDATA[<p>一开始并不抱太大希望往 DeeCamp 投了简历，笔试最后一道题犯了低级错误，还以为要与这次夏令营失之交臂，结果居然成为了 300 个幸运儿之一。7.20 考完试稍微收拾一下，怀着忐忑的心情，第二天就坐上了去北京的高铁。唰的一下，紧张而又充实的一周就这样度过了。</p><h2 id="北大，第二次见面"><a href="#北大，第二次见面" class="headerlink" title="北大，第二次见面"></a>北大，第二次见面</h2><p>上一次到北大还是高中毕业旅行，那时候是个游客，靠着同学姐姐带了进来，走马观花地游览了一番。还是觉得蛮神奇的，两年之后再次和北大见面，并且这里要在生活一个月。虽然只是营员，但是拿到北大学生卡的时候还是小小激动了一把。在这第一个礼拜里，吃了学五鸡腿饭，松林的包子，和高中同学一起夜游北大…… 都是缘分呐。</p><p>环境方面，心里难免会和西电做个比较。宿舍方面，西电算是胜出吧，毕竟西北第一，两室一厅也是没谁（虽然今年的新生是 10 间，gg）；食堂方面，虽然说非本校学生要加 15%，但我觉得北大的食堂还是很棒的，整体比我校要看起来干净许多，不过人也是爆多了，毕竟知名学府2333；教学楼方面，北大胜出，就凭打热水的机器还有<strong>常温水</strong>的选择，我就给他满分！而且教室整体也都高出西电一个档次（钱多就是好啊）。</p><h2 id="知识课，饕餮盛宴"><a href="#知识课，饕餮盛宴" class="headerlink" title="知识课，饕餮盛宴"></a>知识课，饕餮盛宴</h2><p>昨天刚上完知识课，导师的阵容应该是找不出第二家了，并且内容基本上 cover 了人工智能相关的各个方面。不过因为我主要研究的是自然语言处理，所以很多其他领域的 talk 并不能 follow 很多，但整个一周下来，也算是对 AI 的全景有了一个整体上的感知。</p><p>印象比较深的几个讲座有下面几个：</p><ul><li>李开复老师讲 AI 的全景，人才的培养，甚至后面单独拿出一个午休的时间来答疑。原先我看过李开复老师的一些书，比如说《人工智能》，当然这本书可能更适合非专业人士来了解 AI，因为其写的过于浅显导致我甚至怀疑李开复老师的水平（捂脸）。但当他作为亲历者，给我们娓娓道来整个 AI 的发展浪潮，了解到他所研究的项目包括 CV、语音识别、下棋都是当时世界顶级的，事实上，他是在学界、工业界都非常有成就的，我为自己的无知感到无比羞愧。李开复老师真的有一种谦谦君子温润如玉的感觉，还把他的微信小号分享给大家。这种 dalao 真的见一次能让你感受到什么叫 level 上的差距，后面景驰的 CEO 说他当年也是李开复的迷弟，没想到今天李开复就成了他的投资人，一起喝酒说要做点什么。希望有一天我也能和李开复一起做点事情（逃</li><li>孙栩老师讲 NLP：因为我本身做 NLP，所以最感兴趣的自然是北大孙老师的自然语言处理课程。之前也看过孙老师的主页，感觉真人比照片上更精干一些，也更睿智一些。内容方面因为考虑到大家水平层次不齐，所以都是比较基础的内容。但是课间的时候同学们向孙老师咨询问题的时候，我凑上去旁听也问了几个问题，收获还是蛮大的。我问了一个问题是 word embedding 是一种好的表示吗（或者说是只有这样一条路子来做词的表示吗？），孙老师指出 COLING 18 有相关的工作不仅考虑 word embedding，同时把语言模型的参数也附加进来的一种方法，开拓了我的思路。另外发现孙老师组的作品有一个共性，就是看待问题的方式是很新颖的，比如把 Multi-label Classification 任务看成是生成任务，一篇 SGM 也因此拿了 COLING 的 best paper。总的来说，孙老师人很 nice，希望明年有时间的话能到孙老师组实习。</li><li>王咏刚老师讲创新创业：先前在学校里上过创新创业课程，不得不说，差距还是太大了，毕竟创业工场是国内最著名的孵化器，作为其中的 AI 工程院副院长的王老师眼光自然很是毒辣，其所经历的和。王老师拿 pdd 做例子告诉我们，即使在座的 300 人里没有用 pdd 的，但它还是成功了。中国的人口那么多，流量怎么下沉？黄峥多次到四五线城市去调研，就是为了发觉这一部分人群真正的需求。而技术人员、学生创业很容易陷入怪圈，并且告诫我们，在创业的时候一定要问问自己：<strong>看到是个体需求，还是普遍逻辑</strong>？<strong>是技术闪光点，还是大的框架</strong>？以前自己真的是很 naive 了，啊我有个 Paper 效果很好，accuracy 很高，但是抱歉，市场不买账。另外，AI 在未来毫无疑问是重中之重，但是它能够 to C 吗？不能，<strong>纯 AI 并不能作为一个独立的产品，往往是卖给企业，和已有的环节结合，赋能增效，创造更大的价值</strong>。</li></ul><h2 id="Find-My-Way"><a href="#Find-My-Way" class="headerlink" title="Find My Way"></a>Find My Way</h2><p>收获最大的是孙栩老师让他的研究生，同时也是今年 COLING Best Paper 的第一作者杨鹏程来给我们讲工作以及科研的经历。其中他说到，首先我们需要想清楚自己要做什么？是一个 researcher 还是一个 engineer，甚至是一个 PM。当然了，目前我的选择就是 research，而如果选择做科研那么就就会经常处于两中情况：</p><ul><li>没有 Idea：<ol><li>尝试做一个应用型任务，学以致用</li><li>多读论文，厚积薄发</li><li>重视小的创新，循序渐进，积少成多</li></ol></li><li>有了 Idea：<ol><li>能否进一步创新，拉大与 baseline 的 gap</li><li>成功的 idea 存在什么问题，能否进一步解决</li><li>如何解释 idea 成功的原因</li><li>如何设计全方位的实验来验证 motivation 以及解释原因</li></ol></li></ul><p>之前有过写一篇 Paper 的经历，知道了 Paper 的架构是怎么样的，对整个流程也有一个模糊的概念，但始终没有梳理出一个框架来。而杨鹏程学长的分享让我有一种醍醐灌顶的感觉，据说这也是其和组内师兄总结得出的，我想这一周最大的收获莫过于此。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一开始并不抱太大希望往 DeeCamp 投了简历，笔试最后一道题犯了低级错误，还以为要与这次夏令营失之交臂，结果居然成为了 300 个幸运儿之一。7.20 考完试稍微收拾一下，怀着忐忑的心情，第二天就坐上了去北京的高铁。唰的一下，紧张而又充实的一周就这样度过了。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>RaSGAN Notes</title>
    <link href="https://tobiaslee.top/2018/07/05/RaGAN-Notes/"/>
    <id>https://tobiaslee.top/2018/07/05/RaGAN-Notes/</id>
    <published>2018-07-05T02:27:13.000Z</published>
    <updated>2018-07-24T10:22:31.342Z</updated>
    
    <content type="html"><![CDATA[<p>在机器之心上看到说 Goodfellow 给一篇刚上 arXiv 的 <a href="https://arxiv.org/abs/1807.00734" target="_blank" rel="noopener">The relativistic discriminator: a key element missing from standard GAN</a> 文章点了赞，就去看了眼，发现确实很有意思。</p><h2 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h2><p>文章说标准的 GAN（SGAN） 在 Generator（下文用 G 代替） 生成的样本越来越逼真的时候 Discriminator 缺了个东西。什么呢？<strong>相对</strong>的概念，先来看下面这张图片：</p><p><img src="/img/corgi_and_bread.png" alt="Corgis and Bread"></p><p>可以看到，我们的 real data 是面包，fake data 是柯基（好萌啊2333)，$ C(x) $ 越大则说明是面包的概率越大。图中列出了三种情况：</p><ol><li>真的面包，真的柯基：这种情况二者的区别很明显，因而 $ P(bread |\bar C) = 1 $</li><li>真的面包，柯基屁股（很像面包）：这种情况区别就没有第一种那么明显了，因而 $ P $ 有所下降</li><li>像狗的面包，真的柯基：和第二种情况类似，<strong>相对的区别度</strong>降低了， $ P $ 同样有所下降</li></ol><p>有了一个模糊的印象之后，我们展开说说这个相对究竟是什么。</p><h2 id="Arguments-of-RaGAN"><a href="#Arguments-of-RaGAN" class="headerlink" title="Arguments of RaGAN"></a>Arguments of RaGAN</h2><p><strong>相对</strong>，一言以蔽之：Discriminator（下文中用 D 代替）衡量样本真实性的时候，应该要<strong>同时</strong>利用 real data 和 fake data，衡量的由绝对的真假变成<strong>相对的为真或为假</strong>的概率。 作者从三个方面论述了其观点：</p><h3 id="Priori-Knowledge"><a href="#Priori-Knowledge" class="headerlink" title="Priori Knowledge"></a>Priori Knowledge</h3><p><strong>先验知识</strong>的利用，即每次我们喂给 Discriminator（下文中用 D 代替） 的样本中，基本上是一半 real data，一半 fake（Generator generated）。也就是说，<strong>不知道这个前提的话</strong>，那么如果 G 生成的样本（比如说图片）能够以假乱真的话，那么 D 会认为所有的样本都是 real 的，而如果知道这个前提，那么当 fake 比 real 更 real 的时候，discrinimator 应该给 real samples 打低分（认为他们是 fake） 而不是认为所有的 samples 都是 real。因为在看到了更 real 的 fake samples 之后，相对地，利用先验知识，我们会认为不那么 real 的 samples（比如狗面包）是 fake 的。而 SGAN 的训练中并没有利用到这一部分先验知识。</p><h3 id="Divergence-Optimization"><a href="#Divergence-Optimization" class="headerlink" title="Divergence Optimization"></a>Divergence Optimization</h3><p>我们知道，SGAN 在训练 D 的时候事实上是在 minimize 生成器分布和真实数据分布的 JS-Divergence，而 JS-Divergence 在两个分布相同时达到最小值，其表现是 D 无法区分 real data 和 fake data，认为其为真的概率均为 0.5；JS-Divergence 在两个分布差异较大的时候较大，即 D 认为 real data 为真的概率为 1，而认为 fake data 为真的概率为 0。理想的训练过程是如下 (C) 图所示：</p><p><img src="/img/divergence_argument.png" alt="Divergence Optimization Process"></p><p>但事实上，我们在训练的时候<strong>一味地希望生成的图片足够逼真</strong>，即一直在将 <code>D(fake_data)</code> 向 <code>1</code> 推，而不管 <code>D(real_data)</code>。这有做是达不到最小值的，这样的 optimization 过程是存在问题的，WGAN 论文中似乎也有提到这一点。</p><h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><p>WGAN 等一系列对 GAN 做了改进的 GAN 被称为 IPM-based GAN，作者将其梯度和标准的 GAN 进行了对比：</p><p><img src="/img/gradient_sgan.png" alt="SGAN Gradient"></p><p>下面是 IPM-based GAN 的梯度：</p><p><img src="/img/gradient_ipm.png" alt="IPM-based GAN Gradient"></p><p>当下面这些条件满足的时候，两式相等：</p><ol><li>在训练 D 的时候，$ D(x_r) = 0, D(x_f) = 1 $ </li><li>训练 G 的时候，$ D(x_f) = 0 $</li><li>$C(x) \in \mathit{F}$，其中 $F$ 是一类实值函数（这个一般都能满足）</li></ol><p>考虑到 IPM-based GAN 相对于 SGAN 具有更好的稳定性，可以推断，如果将 SGAN 推向 IPM-based GAN，能够提高其稳定性。怎么才能达到这个转变呢？如果我们认为 D 足够好，即能够在训练 G 时做到认为 $D(x_r) = 1 \ and \  D(x_f) = 0 ​$（这是一个比较强的假设，但是在训练一开始是能够满足的），而在训练 D 的时候 $D(x_r) = D(x_f) = 1 ​$，如开头图片的 (b) 所示那种情况，那么唯一缺少的条件就是 $D(x_r) = 0 ​$。但是在 SGAN 中，$D(x_r)​$ 只跟 real data 有关，也即是<strong>绝对的真假</strong>，如果 G 生成的样本能够影响 $D(x_r)​$，让它变得不那么真实，也即<strong>相对的真假</strong>。这个时候，$D(x_r)​$ 才可能变成 0。总结起来一句话就是，在 $D(x_f)​$ 提升的时候 $D(x_r)​$ 相应地减少（这是一种相对的变化），这样 GAN 的训练才能更加稳定。</p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Relativistic-GAN"><a href="#Relativistic-GAN" class="headerlink" title="Relativistic GAN"></a>Relativistic GAN</h3><p>如何把这种相对为真和为假的概率考虑进去？很简单，只要把 logits 做一个简单相减，即：</p><p><img src="/img/relative_gan.png" alt="Relative Loss"></p><p>这里用 <code>sigmoid</code> 函数将 logits 转化成概率，然后再取对数；我们可以很容易地把这个式子进行泛化，即用更一般的函数（不一定是似然函数）来替换它。</p><h3 id="Relativistic-average-GAN"><a href="#Relativistic-average-GAN" class="headerlink" title="Relativistic average GAN"></a>Relativistic average GAN</h3><p>不过这个时候，我们发现 GAN 中 D 的功能已经悄然发生的改变，由原来的：<strong>衡量输入数据为真的概率</strong>，变成了输入的数据与其对立类型随机的一个样本（如果输入为 real data，那么衡量其比 fake data 更像真的概率，反之亦然）相比<strong>更</strong>像真的的概率。那么，更一般地，我们利用对立类型的<strong>平均真实度</strong>来作为更可靠的参照对象。</p><p><img src="/img/ragan.png" alt="Relativistic average GAN"></p><p>所谓平均真实度，就是对整个（或者一个 mini-batch）  real data 和 fake data 求其 $D(x)$ 的数学期望，这样的估计能够更加整体的反映训练在这一时刻生成器生成的 fake data 的真实程度。同样，我们可以轻易地将其泛化到别的函数之上。最后整个算法流程如下：</p><p><img src="/img/algorithm_ragan.png" alt="Algorithm of RaGAN"></p><h2 id="Toy-Demo"><a href="#Toy-Demo" class="headerlink" title="Toy Demo"></a>Toy Demo</h2><p>原作给了很多例子来说明 RaGAN 的效果，并且也提供了相应的<a href="https://github.com/AlexiaJM/RelativisticGAN" target="_blank" rel="noopener">代码</a>。这里我就拿 MNIST 做一个小小的 Demo，在原版的 GAN 的 demo 上做了一个很小的改动，就是把 loss 计算中的 logits 进行对应的相减。</p><h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generator Net</span></span><br><span class="line">Z = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">G_W1 = tf.Variable(xavier_init([<span class="number">100</span>, <span class="number">128</span>]), name=<span class="string">"G_W1"</span>)</span><br><span class="line">G_W2 = tf.Variable(xavier_init([<span class="number">128</span>, <span class="number">784</span>]), name=<span class="string">"G_W2"</span>)</span><br><span class="line"></span><br><span class="line">G_b1 = tf.Variable(tf.zeros([<span class="number">128</span>]), name=<span class="string">"G_b1"</span>)</span><br><span class="line">G_b2 = tf.Variable(tf.zeros([<span class="number">784</span>]), name=<span class="string">"G_21"</span>)</span><br><span class="line"></span><br><span class="line">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(z)</span>:</span></span><br><span class="line">    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)</span><br><span class="line">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class="line">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G_prob</span><br></pre></td></tr></table></figure><h3 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Discriminator Net</span></span><br><span class="line">D_W1 = tf.Variable(xavier_init([<span class="number">784</span>, <span class="number">128</span>]), name=<span class="string">"D_W1"</span>)</span><br><span class="line">D_b1 = tf.Variable(tf.zeros(shape=[<span class="number">128</span>]), name=<span class="string">"D_b1"</span>)</span><br><span class="line"></span><br><span class="line">D_W2 = tf.Variable(xavier_init([<span class="number">128</span>, <span class="number">1</span>]), name=<span class="string">"D_W2"</span>)</span><br><span class="line">D_b2 = tf.Variable(tf.zeros(shape=[<span class="number">1</span>]), name=<span class="string">"D_b2"</span>)</span><br><span class="line"></span><br><span class="line">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x)</span>:</span></span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class="line">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    D_prob = tf.nn.sigmoid(D_logit)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D_prob, D_logit</span><br></pre></td></tr></table></figure><h3 id="RaGAN"><a href="#RaGAN" class="headerlink" title="RaGAN"></a>RaGAN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ra GAN  simple version </span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">"X"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G_sample = generator(Z)</span><br><span class="line">D_real, D_logit_real = discriminator(X)</span><br><span class="line">D_fake, D_logit_fake = discriminator(G_sample)</span><br><span class="line"><span class="comment"># discriminator </span></span><br><span class="line"><span class="comment"># log( real_logits - fake_logits)</span></span><br><span class="line">D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real - D_logit_fake, labels=tf.ones_like(D_logit_fake)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># generator loss</span></span><br><span class="line"><span class="comment"># log(fake_logits - real_logits )</span></span><br><span class="line">G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake - D_logit_real, labels=tf.ones_like(D_logit_fake)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer</span></span><br><span class="line">D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)</span><br><span class="line">G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)</span><br></pre></td></tr></table></figure><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_Z</span><span class="params">(m, n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[m, n])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">steps = <span class="number">1000001</span></span><br><span class="line">mb_size = <span class="number">128</span></span><br><span class="line">Z_dim = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(samples)</span>:</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">    gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">        ax = plt.subplot(gs[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        ax.set_xticklabels([])</span><br><span class="line">        ax.set_yticklabels([])</span><br><span class="line">        ax.set_aspect(<span class="string">'equal'</span>)</span><br><span class="line">        plt.imshow(sample.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">'Greys_r'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">        X_mb, _ = mnist.train.next_batch(mb_size)</span><br><span class="line">        </span><br><span class="line">        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=&#123;</span><br><span class="line">            X: X_mb,</span><br><span class="line">            Z: sample_Z(mb_size, Z_dim)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment"># 注意，这里的 feed_dict 和原来不一样了</span></span><br><span class="line">        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=&#123;</span><br><span class="line">            X: X_mb, Z: sample_Z(mb_size, Z_dim)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment"># 下面的 vanilla GAN 的 loss</span></span><br><span class="line">        <span class="comment"># _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=&#123;Z: sample_Z(mb_size, Z_dim)&#125;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Step %d"</span> % i)</span><br><span class="line">            print(<span class="string">"G loss: %f"</span> % G_loss_curr)</span><br><span class="line">            print(<span class="string">"D loss: %f"</span> % D_loss_curr)</span><br><span class="line">            print()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            samples = sess.run(G_sample, feed_dict=&#123;Z: sample_Z(<span class="number">16</span>, Z_dim)&#125;)</span><br><span class="line">            fig = plot(samples)</span><br><span class="line">            fname = <span class="string">'&#123;&#125;.png'</span>.format(str(num).zfill(<span class="number">5</span>))</span><br><span class="line">            plt.savefig(fname, bbox_inches=<span class="string">'tight'</span>)</span><br><span class="line">            print(<span class="string">'saved image '</span> + fname)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># plt.clf()</span></span><br><span class="line">            plt.close(fig)</span><br></pre></td></tr></table></figure><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>GAN 的难训练是<strong>臭名昭著</strong>了，作者通过考虑引入相对这个概念来使得训练过程变得更加稳定。这类见微知著的工作，还有前不久的 IndRNN 真的非常符合我的胃口了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在机器之心上看到说 Goodfellow 给一篇刚上 arXiv 的 &lt;a href=&quot;https://arxiv.org/abs/1807.00734&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The relativistic discriminat
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 初体验</title>
    <link href="https://tobiaslee.top/2018/06/30/Pytorch-Tutorial-Walkthrough/"/>
    <id>https://tobiaslee.top/2018/06/30/Pytorch-Tutorial-Walkthrough/</id>
    <published>2018-06-30T13:48:55.000Z</published>
    <updated>2018-06-30T17:30:56.366Z</updated>
    
    <content type="html"><![CDATA[<p>深度学习框架很多，TensorFlow 算是比较流行的了，但静态图有时候确实写的很难受，以及一些比较复杂的 model 实现起来比较蛋疼；最近在看的文章的实现没有 TensorFlow 版本，起初我想改写一个 TensorFlow 版本出来，结果实现不了，一方面可能我确实比较菜，另一方面，也只能说 TensorFlow 有些东西在设计上让人为难。所以，那就试试 PyTorch 咯。这篇文章主要是记录我学习 PyTorch 官方 60 分钟<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">教程</a>的过程。</p><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>逃不开的还是 Tensor 这个对象，PyTorch 创建、操作 Tensor 的方法和 numpy 很类似，据说 Torch 一开始就是 Numpy 的 GPU 加速版，简单列些一些 API：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>) <span class="comment"># 创建一个未初始化的 5x3 tensor</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>) <span class="comment"># 随机生成 5x3 </span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br></pre></td></tr></table></figure><p>比较有意思的是，创建 Tensor 的时候 size 参数不是一个 <code>tuple</code> e.g. <code>(5, 3)</code> ，而是用 <code>*args</code> 的方式传入。</p><p>常用的算术操作可以通过 <code>x+y</code> 或者是 <code>torch.add(x, y)</code> 来实现，同时 PyTorch 也提供了<strong>就地 </strong>(in place) 的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.add_(x) <span class="comment"># equals y = y + x</span></span><br></pre></td></tr></table></figure><p>所有的操作名 + 下划线都是一个就地操作，<code>x.copy_(y)</code> 会用 <code>y</code> 的值<strong>替代</strong><code>x</code>。</p><p>另外，对于单元素的 Tensor，可以通过使用 <code>.item()</code> 来获取它；TensorFlow 中的 <code>reshape()</code> 对应的是 <code>view()</code>，并且，和 numpy 一样，<strong>Tensor</strong> 支持切片操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>) <span class="comment"># x -&gt; 4x4</span></span><br><span class="line">x.view(<span class="number">-1</span> , <span class="number">8</span>) <span class="comment"># x -&gt; 2 x 8</span></span><br><span class="line">x[:, <span class="number">1</span>] <span class="comment"># 取第二列的所有元素</span></span><br></pre></td></tr></table></figure><p>既然之前说 Torch 是 Numpy 的一个 GPU 版本，也就支持 ndarray 和 Tensor 的相互转化（这就比 TensorFlow 用起来舒服很多），可以用 <code>Tensor.numpy()</code> 获取到对应的 ndarray，逆操作则是 <code>torch.from_numpy()</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example 1 tensor -&gt; numpy</span></span><br><span class="line">a = torch.ones(<span class="number">3</span>) <span class="comment"># tensor([1., 1. , 1.])</span></span><br><span class="line">b = a.numpy() <span class="comment"># 通过 tensor.numpy() 获取对应的 ndarray 对象</span></span><br><span class="line">a.add_(<span class="number">1</span>)  <span class="comment"># 就地 + 1 , a= a+1</span></span><br><span class="line">print(a)<span class="comment"># tensor([2., 2., 2.])</span></span><br><span class="line">print(b) <span class="comment"># [2., 2., 2.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># example 2   numpy -&gt; tensor</span></span><br><span class="line">a = np.ones(<span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)  <span class="comment"># [2., 2., 2.]</span></span><br><span class="line">print(b) <span class="comment"># tensor([2., 2., 2.])</span></span><br></pre></td></tr></table></figure><p>上面这个例子很重要，说明了<strong>ndarray 和 Tensor 对象之间的转换是一种引用关系，而非创建新的对象</strong>。也就是说，<strong>任何一个相关对象的变化都会影响到另一方</strong>，就像例子 1 中，我们对原始的 <code>a</code> 进行 <code>add_(1)</code>，在此之前得到的 <code>b</code> 对象的值也改变了，例子二中同样出现了这样的情况，这一点需要特别注意。</p><h3 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h3><p>我们可以把 Tensor 放到 GPU 上来加速计算，而 PyTorch 为此提供了很方便的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available(): <span class="comment"># 如果 CUDA 可用</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># CUDA 设备对象</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接在 GPU 上创建 Tensor 对象</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 也可以用``.to("cuda")`` 代替</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` 同时可以改变数据类型</span></span><br></pre></td></tr></table></figure><p>如上面的例子所示，我们可以在创建 Tensor 对象时指定 <code>device</code> 参数来指明 Tensor 创建的位置，也可以再创建了之后使用 <code>.to()</code> 方法来进行双向的迁移：既可以从 CPU 到 GPU（例子中的 <code>x.to(device)</code>，也可以从 GPU 到 CPU（<code>z.to(&quot;cpu&quot;, torch.double)</code>）。对比 TensorFlow ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    <span class="comment"># something you want to do on CPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    <span class="comment"># somethin you want to do on GPU</span></span><br></pre></td></tr></table></figure><p>就我所知，TensorFlow 只能在创建的时候指明位置，而无法像 PyTorch 这般灵活的迁移。</p><h2 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h2><p>自动求导已经是深度学习框架的必备了，首先，我们可以通过设置 Tensor 的 <code>.requires_grad</code> 参数来指明 Tensor 是否参与梯度运算，这和 TensorFlow 中的 <code>trainable</code> 是类似的；接着我们通过对某个对象（比如 loss ）函数进行 <code>.backward()</code> 操作，来进行反向梯度的计算，并且能够通过计算链上对象的 <code>.grad</code> 属性来获取到对应的梯度。拿官方的例子做一个简单的说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>) <span class="comment"># 声明 x，指明需要参加梯度的计算 x: 2x2 全 1</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span> <span class="comment"># z = 3 * (x+2)^2</span></span><br><span class="line">out = z.mean()</span><br></pre></td></tr></table></figure><p>梯度计算的过程如下：</p><p>$ o =\frac{1}{4}\sum_i z_i$</p><p>$ z_i = 3(x_i+2)^2 $</p><p>$ z_i\bigr\rvert_{x_i=1} = 27 $</p><p>$ \frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2) $</p><p>$  \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5 $</p><p>因此，<code>x.grad</code> 的值在我们对 <code>out</code> 进行 <code>out.backward()</code> 操作之后，结果为：</p><blockquote><p>tensor([[ 4.5000,  4.5000],</p><p>​             [ 4.5000,  4.5000]])</p></blockquote><p><code>backward()</code> 函数的原型如下：</p><blockquote><p> torch.autograd.backward(<em>tensors</em>, <em>grad_tensors=None</em>, <em>retain_graph=None</em>, <em>create_graph=False</em>, <em>grad_variables=None</em>)</p></blockquote><p>其中第一个参数用来声明<strong>梯度的权重</strong>。如果目标函数是一个标量，则可以不传这个参数，比如上面的 <code>out.backward()</code> ，则默认权重为 <code>1</code> ，如果我们改成 <code>out.backward(torch.tensor(2,dtype=torch.float))</code> 则各个 $$x_i$$ 梯度就会变成原来的 2 倍，利用权重我们就可以来控制各个参数的更新速度。</p><h2 id="Build-A-Neural-Network"><a href="#Build-A-Neural-Network" class="headerlink" title="Build  A Neural Network"></a>Build  A Neural Network</h2><p>学框架不搭一个 NN 来玩 MNIST 或者 CIFAR 怎么能叫入门呢？来来来，走一波~</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>) <span class="comment"># input 3 channels, output 6 channels , filter size = 5</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>) <span class="comment"># input 3 channels, output 16 channels, filter size = 5</span></span><br><span class="line">        <span class="comment"># Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># max pooling, window size : 2x2</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果是 方形的输入，window size 可以只用一个参数</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span> <span class="comment"># 展平 x</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 获取各个维度的大小，除了第一维 batch_size</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure><p>诶，有没有觉得有点像 Keras，不过高级的 API 用起来就是爽，不用写那么多代码2333，接下来就是定义 loss 和进行 backprop 来更新参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 定义 loss</span></span><br><span class="line">criterion = nn.CrossEntropy()</span><br><span class="line"><span class="comment"># 创建 optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在循环里不断进行如下操作：</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># 清零梯度</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># 梯度更新</span></span><br></pre></td></tr></table></figure><p>和 TensorFlow 的区别就在于，TensorFlow 的话只要我们用 <code>session.run(optimizer)</code> 就可以完成梯度的更新操作，而 PyTorch 则需要：</p><ol><li>Optimizer 清零梯度</li><li>对 loss 进行手动的 <code>backward()</code> 操作</li><li>Optimizer 更新 <code>.step()</code></li></ol><p>三步走，记住了没有！</p><p>还有一点比较方便的是，和 Tensor 类似，我们可以使用 <code>Net.to()</code> 来把模型部署到 GPU 上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(<span class="string">"cuda:0"</span>)</span><br></pre></td></tr></table></figure><p>但是！记得要把你的输入也放到 GPU 上，不然就会报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = inputs.to(device), labels.to(device) <span class="comment"># 把 input 和 label 同样迁移到 GPU 上</span></span><br></pre></td></tr></table></figure><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>初体验如果要给个评价的话，我觉得是比 TensorFlow 好很多（毕竟 TensorFlow 一上来的 Session、静态图会让人有点摸不着头脑）。但用什么框架其实都无所谓，就和语言一样，虽然争来争去，但每种语言都有自己的用武之地，以前我还会和室友争辩 Java 和 Python 谁才是最好的语言，现在就不会了（因为我也觉得 Python 好写一点，逃）。框架更不用说，TensorFlow 有他应用的工业场景（希望能早日接触到），PyTorch 现在看来也许更适合需要快速实现原型的科研人员。而我们能做的，就是多接触，横向比较着来看而不要因为自己擅长而蒙蔽了双眼，<strong>多学一技压身，总是没错的</strong>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;深度学习框架很多，TensorFlow 算是比较流行的了，但静态图有时候确实写的很难受，以及一些比较复杂的 model 实现起来比较蛋疼；最近在看的文章的实现没有 TensorFlow 版本，起初我想改写一个 TensorFlow 版本出来，结果实现不了，一方面可能我确实比
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Brief Overview of Text Generation</title>
    <link href="https://tobiaslee.top/2018/06/09/Brief-overview-of-text-generation/"/>
    <id>https://tobiaslee.top/2018/06/09/Brief-overview-of-text-generation/</id>
    <published>2018-06-09T12:44:22.000Z</published>
    <updated>2018-06-11T14:34:45.176Z</updated>
    
    <content type="html"><![CDATA[<p>最近走马观花地读了关于一些 Text Generation 的文章，梳理一下脉络。</p><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>记得在 Coursera 以及各大深度学习框架都有 RNN 做 char 级别的莎士比亚风格的文本生成 Demo，也算是一个非常入门的模型了。<a href="https://arxiv.org/pdf/1308.0850.pdf" target="_blank" rel="noopener">Generating Sequences With Recurrent Neural Networks</a> 提出了用 RNN 来做 Text 的生成任务，因为其 Intuition 很符合我们的认知，<strong>后面的文字基于已经生成的文字而产生</strong>。</p><p>事实上，我们在用 RNN 建模一个语言模型：</p><p>$ P(s) = P(x_1) P(x_2|x_1) … P(x_t|x_1, .. , x_{t-1}) = \Pi_{t=1}^{T} P(x_t|x_1, .., x_{t-1})$</p><p>取 log 之后就变成相加：</p><p>$ log P(s) = \sum_{t=1}^{T} log P(x_t|x_1,…,x_{t-1}) $</p><p>之前的一些 n-gram 模型则会假定当前的词 $$x_t$$ 只和其附近一定范围的词语有关，通过条件概率进行一定的简化，这里就不展开，因为现在的 RNN 以及其变种能较好的解决的长距离的依赖问题，所以效果自然比简化的版本要好。</p><p>在训练这个模型的时候，我们会去 maximize 这个句子在 corpus 中出现的概率，也就是 minimize 整个句子的负 log：</p><p>$L(x) = -\sum_{t=1}^{T} log P(x_t|x_1,…,x_{t-1}) $</p><p>一般地，我们会随机从预料中取一个句子的开头作为第一个 token，以这个句子的每个 token 作为 groud-truth，来进行 loss 的计算。但是这样会出现一个问题，就是如果我们一个词生成的不是很好，会导致后面生成的词误入歧途，整个句子就报废了，降低了某些本来是正确的词出现的概率，从而导致训练非常不稳定，类似<strong>一颗老鼠屎坏了一锅粥</strong>。所以，Teacher Forcing 被提出，也就是，每个词的生成会基于 groud-truth 生成，而不是根据模型自己之前生成的词，这样子，即使中途跑偏也能被“老师“纠正回来。</p><p>但是呢但是，Teacher Force 也有一个问题，就是 Train 和 Inference 阶段的不同：<strong>Train 阶段是有 ground-truth 给你纠偏，但是 Inference 的时候没有</strong>，这下就歇菜了。就好比平时考试都有老师站在边上看你做题，一做错就告诉你正确答案，到高考了结果就凉透了，因为你可能发现<strong>有一些题你从来没有见过</strong>，导致一败涂地。这种依赖叫做 <strong>Exposure Bias</strong>，随后呢，有人提出了解决方案来：既然没有正确答案不行，那就抛硬币决定要不要给你正确答案，并且越到后面给你正确答案的概率越小，即 <a href="http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf" target="_blank" rel="noopener">Scheduled Sampling</a>。</p><p>你以为到这里就完了吗，没有。<a href="https://arxiv.org/pdf/1511.05101.pdf" target="_blank" rel="noopener">How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?</a> 指出 Schedule Sampling 是一个不一致的方法，简而言之：Maximize Likelihood 可以视作是 minimize 两个分布 P 和 Q 的 KL-Divergence，而前提是两个分布相同时 KL-Divergence最小，但是 Schedule Sampling 的 KL-Divergence 的最小值并不是 P = Q。而为什么它能起作用呢：</p><blockquote><p>Scheduled sampling works by pushling models towards a trivial solution of memorising distribution of symbols conditioned on their position in the sequence.</p></blockquote><p>说人话就是：治标不治本。</p><p>另外，除了 exposure bias，还有个比较 trival 的问题：逐字的 loss 是不是不太符合我们的习惯，一般都是以<strong>句子</strong>这个粒度来评估生成的质量，有个别错别字可能无伤大雅呢？有待以后再讨论。</p><a id="more"></a><h2 id="Seq2Seq-Architexture"><a href="#Seq2Seq-Architexture" class="headerlink" title="Seq2Seq Architexture"></a>Seq2Seq Architexture</h2><p>如果把机器翻译也看作是一种文本生成任务的话，那么 Seq2Seq 也是非常经典的生成架构。一般会用两个网络，一个用于编码（Encoder），将一些信息，比如待翻译的文本，经过编码之后形成一个 semantic representation 交给另一个解码网络（Decoder），解码网络根据编码网路结果进行目标文本的生成，比如生成对应的翻译文本。</p><p>如果没记错的话 Seq2Seq 首先是在 Machine Translation 提出来，就是那篇 Attention 的大作<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">《Neural Machine Translation By Jointly Learning To Align and Translate》</a>，之后被生成任务所采用。Attention 之所以会在机器翻译任务中提出来，也是很直觉的：<strong>经过编码之后的语义表示向量并不足够支撑较长文本的翻译</strong>，并且和人类翻译的过程中类似，我们在翻译的某句话的个别词汇的时候，会<strong>特别</strong>关注其周围的一些词，来辅助我们的翻译，这种关注也就是一种注意力机制，更多的可以参考我之前的一篇 <a href="http://tobiaslee.top/2018/05/13/Attention-Notes/">Blog</a>。</p><p>回到 Seq2Seq 对于文本生成任务，事实上可以把上面的语言模型少做修改，也即我们建模的是一个条件概率：$$ P(s | c)$$，这里的 $c$ 在机器翻译任务中可以视作是待翻译的文本，在 Image Caption 任务中就可以看做是 Image 所携带的信息。和单纯的 RNN 模型相比，我们可以把 Decoder 就看做是先前的 RNN，而 Encoder 则是 <strong>Seq2Seq 提供了的给 model 先验信息的窗口</strong>，从而能够在特定的任务上有很出色的表现。摘掉 Encoder，就退化成 RNN Generator，所以 Seq2Seq 并没有解决 RNN 模型遇到的问题。</p><p>Seq2Seq 有很多基于模型架构微调的工作，但我看到基本都是类似在生成文本之后再进行修改的作品：</p><ol><li>推敲网络：<a href="https://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf" target="_blank" rel="noopener">Deliberation Networks: Sequence Generation Beyond One-Pass Decoding</a>，模仿人类写作，在初稿完成之后再进行润色。想起那经典的”僧推月下门“和”僧敲月下门“的故事。</li><li>编辑网络： <a href="https://arxiv.org/pdf/1805.06064.pdf" target="_blank" rel="noopener">Paper Abstract Writing through Editing Mechanism</a>，思路和上面类似。</li></ol><h2 id="GAN-and-VAE"><a href="#GAN-and-VAE" class="headerlink" title="GAN and VAE"></a>GAN and VAE</h2><p>前面提到，RNN 和 Seq2Seq 的 maximize likelihood 的训练方式存在问题，并没有得到很好的解决。现在我的觉得，很多机器学习问题，归结下来都是一个模拟概率分布的问题。想通过某些手段来 recover 一个已知的分布 P，除了最大似然以外，GAN 通过 minimize 两个分布的 JS-divergence，VAE 则是 KL-divergence。GAN 和 VAE 各有优缺点，Eric P. Xing 组有一篇 <a href="https://arxiv.org/pdf/1706.00550.pdf" target="_blank" rel="noopener">On Unifying Deep Generative Models</a> 从 Adversarial Domain Adaption 的角度来将 GAN 和 VAE 统一起来的文章，当初学的时候就觉得两个模型 minimzie 的东西挺像的，虽然一个是生成器和判别器打架，另一个是 reconstruction loss（如果我没记错的话）。</p><p>GAN 用在文本领域，在我之前的笔记里也都有提到，主要是通过强化学习来解决梯度传递的问题；</p><p>VAE 用于文本生成的思路是这样的：将真实的文本交给一个 Encoder，编码成 latent vector $z$，然后通过这个 $z$ 来经过 Decoder 重构输入的文本，感觉和 Seq2Seq 也有点类似，鉴于 GAN 似乎已经大量地取代 VAE，所以也就略读了几篇主要的 Paper。</p><ol><li><a href="">Generating Sentences from a Continuous Space</a>：提出用 VAE 做文本生成</li><li><a href="https://arxiv.org/pdf/1702.02390.pdf" target="_blank" rel="noopener">A Hybrid Convolutional Variational Autoencoder for Text Generation</a>：对 Encoder 和 Decoder 的网络结构进行了修改，采用 CNN 和 RNN 的混合。</li><li><a href="http://proceedings.mlr.press/v70/hu17e/hu17e.pdf" target="_blank" rel="noopener">Toward Controlled Generation of Text</a> ：生成属性能够控制的文本（比如情感极性、句子的时态），通过在 $z$ 中挖出一块来编码语义要求信息，增加相应的 loss 来引导 model 生成我们想要的文本。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近走马观花地读了关于一些 Text Generation 的文章，梳理一下脉络。&lt;/p&gt;
&lt;h2 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink&quot; title=&quot;RNN&quot;&gt;&lt;/a&gt;RNN&lt;/h2&gt;&lt;p&gt;记得在 Coursera 以及各大深度学习框架都有 RNN 做 char 级别的莎士比亚风格的文本生成 Demo，也算是一个非常入门的模型了。&lt;a href=&quot;https://arxiv.org/pdf/1308.0850.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt; 提出了用 RNN 来做 Text 的生成任务，因为其 Intuition 很符合我们的认知，&lt;strong&gt;后面的文字基于已经生成的文字而产生&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;事实上，我们在用 RNN 建模一个语言模型：&lt;/p&gt;
&lt;p&gt;$ P(s) = P(x_1) P(x_2|x_1) … P(x_t|x_1, .. , x_{t-1}) = \Pi_{t=1}^{T} P(x_t|x_1, .., x_{t-1})$&lt;/p&gt;
&lt;p&gt;取 log 之后就变成相加：&lt;/p&gt;
&lt;p&gt;$ log P(s) = \sum_{t=1}^{T} log P(x_t|x_1,…,x_{t-1}) $&lt;/p&gt;
&lt;p&gt;之前的一些 n-gram 模型则会假定当前的词 $$x_t$$ 只和其附近一定范围的词语有关，通过条件概率进行一定的简化，这里就不展开，因为现在的 RNN 以及其变种能较好的解决的长距离的依赖问题，所以效果自然比简化的版本要好。&lt;/p&gt;
&lt;p&gt;在训练这个模型的时候，我们会去 maximize 这个句子在 corpus 中出现的概率，也就是 minimize 整个句子的负 log：&lt;/p&gt;
&lt;p&gt;$L(x) = -\sum_{t=1}^{T} log P(x_t|x_1,…,x_{t-1}) $&lt;/p&gt;
&lt;p&gt;一般地，我们会随机从预料中取一个句子的开头作为第一个 token，以这个句子的每个 token 作为 groud-truth，来进行 loss 的计算。但是这样会出现一个问题，就是如果我们一个词生成的不是很好，会导致后面生成的词误入歧途，整个句子就报废了，降低了某些本来是正确的词出现的概率，从而导致训练非常不稳定，类似&lt;strong&gt;一颗老鼠屎坏了一锅粥&lt;/strong&gt;。所以，Teacher Forcing 被提出，也就是，每个词的生成会基于 groud-truth 生成，而不是根据模型自己之前生成的词，这样子，即使中途跑偏也能被“老师“纠正回来。&lt;/p&gt;
&lt;p&gt;但是呢但是，Teacher Force 也有一个问题，就是 Train 和 Inference 阶段的不同：&lt;strong&gt;Train 阶段是有 ground-truth 给你纠偏，但是 Inference 的时候没有&lt;/strong&gt;，这下就歇菜了。就好比平时考试都有老师站在边上看你做题，一做错就告诉你正确答案，到高考了结果就凉透了，因为你可能发现&lt;strong&gt;有一些题你从来没有见过&lt;/strong&gt;，导致一败涂地。这种依赖叫做 &lt;strong&gt;Exposure Bias&lt;/strong&gt;，随后呢，有人提出了解决方案来：既然没有正确答案不行，那就抛硬币决定要不要给你正确答案，并且越到后面给你正确答案的概率越小，即 &lt;a href=&quot;http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scheduled Sampling&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;你以为到这里就完了吗，没有。&lt;a href=&quot;https://arxiv.org/pdf/1511.05101.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?&lt;/a&gt; 指出 Schedule Sampling 是一个不一致的方法，简而言之：Maximize Likelihood 可以视作是 minimize 两个分布 P 和 Q 的 KL-Divergence，而前提是两个分布相同时 KL-Divergence最小，但是 Schedule Sampling 的 KL-Divergence 的最小值并不是 P = Q。而为什么它能起作用呢：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Scheduled sampling works by pushling models towards a trivial solution of memorising distribution of symbols conditioned on their position in the sequence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;说人话就是：治标不治本。&lt;/p&gt;
&lt;p&gt;另外，除了 exposure bias，还有个比较 trival 的问题：逐字的 loss 是不是不太符合我们的习惯，一般都是以&lt;strong&gt;句子&lt;/strong&gt;这个粒度来评估生成的质量，有个别错别字可能无伤大雅呢？有待以后再讨论。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="Text Generation" scheme="https://tobiaslee.top/tags/Text-Generation/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>又一年</title>
    <link href="https://tobiaslee.top/2018/06/09/New-Stage/"/>
    <id>https://tobiaslee.top/2018/06/09/New-Stage/</id>
    <published>2018-06-09T07:09:21.000Z</published>
    <updated>2018-06-09T08:00:48.195Z</updated>
    
    <content type="html"><![CDATA[<p>很久没更 Blog 了，有一种欠账的感觉，就随便写点什么记录一下这些日子发生的事情。我会尽快把最近读的论文梳理一下输出一篇文章的（逃</p><h2 id="又一年"><a href="#又一年" class="headerlink" title="又一年"></a>又一年</h2><p>翻到去年这时候写的东西，现在看来真是很心疼那个时候的自己。今年学聪明了，去年想拿的奖都拿了，没有特别突出的成绩，但总不至于拖后腿成为短板，简历也能稍微好看一点。也就是说，一年过去了，还是有一些长进的，感到庆幸。</p><p>读的 Paper 越多，越觉得山外有山，人外有人，看别人本科一堆 CCF A 类会议，又或是各种奖牌拿到手软，换做一年前的我肯定懊丧死了，会怀疑自己怀疑人生。不过今年经历的一些事情让我才明白一个被说烂了的道理：<strong>不要和别人比，和自己比</strong>。这也侧面证明了“听了很多大道理，依然过不好这一生”是有道理的，有些事情说说大家都明白，真的能领悟又要到另外一个时候了。水到渠才会成，别着急，慢慢来。</p><p>但依旧存在一些自己觉得不足的地方，比如说代码能力，头条的笔试算是给我敲了个警钟。以前一直逃避着不去刷题，觉得有一种做数学题的恐惧，但那一刻总会到来的，所以是要提上日程了。</p><p>另外，今年家里的表妹和堂弟参加了高考，也祝愿他们都能取得好成绩吧。</p><h2 id="岔路"><a href="#岔路" class="headerlink" title="岔路"></a>岔路</h2><p>一年前的我还在迷茫前途，现在这个时候路基本上已经清楚（也应该清楚了，不然就太浑浑噩噩了），无非有一些小小的分岔需要再做决定，但心中也已经有了优先级。想到了高中语文的一次作文题：</p><blockquote><p>一个彻底诚实的人是从不面对选择的，那条路永远会清楚无二地呈现在你面前，这和你的憧憬无关，就像你是一颗苹果树，你憧憬结橘子，但是你还是诚实地结出苹果一样。</p></blockquote><p>我可能不是一颗参天大树，但一定会努力向上生长。</p><h2 id="摸鱼"><a href="#摸鱼" class="headerlink" title="摸鱼"></a>摸鱼</h2><p>很愧疚，最近画了很多时间在刷剧上；工作也在做，但进度不快，可能无法预期完成工作了，但心底总是给自己找借口：没有设备，快不起来。</p><p>怎么说呢，懈怠总是会有的，应付期中考试软考还有笔试，也确实有点疲惫。看看肥皂剧挺好的，毕竟生活艰苦，需要补充一些糖分。但想到明年这个时候应该奔波在各大高校的夏令营之间，应该拿出足够的成绩来证明自己以获得一张晋级的入场券，继续摸鱼下去可不行，有一种负罪感。</p><p>再坚持一下，离终点不远了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;很久没更 Blog 了，有一种欠账的感觉，就随便写点什么记录一下这些日子发生的事情。我会尽快把最近读的论文梳理一下输出一篇文章的（逃&lt;/p&gt;
&lt;h2 id=&quot;又一年&quot;&gt;&lt;a href=&quot;#又一年&quot; class=&quot;headerlink&quot; title=&quot;又一年&quot;&gt;&lt;/a&gt;又一年
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>别人家的 Attention</title>
    <link href="https://tobiaslee.top/2018/05/13/Attention-Notes/"/>
    <id>https://tobiaslee.top/2018/05/13/Attention-Notes/</id>
    <published>2018-05-13T05:48:19.000Z</published>
    <updated>2018-05-13T08:34:28.575Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> 前段时间火了一把，其提出完全用 Attention 替代传统的 CNN 和 RNN 架构来做特征的提取，也在 NMT 上也取得了 state-of-the-art。这两天读了一下这篇 Paper，并且在熟悉的 Text Classification 问题上用其模型做了一下尝试，这篇 Blog 就用来记录过程中的一些想法和感受。</p><h2 id="What-is-Attention"><a href="#What-is-Attention" class="headerlink" title="What is Attention"></a>What is Attention</h2><p>注意力机制之前在学的时候就有过一次<a href="http://tobiaslee.top/2017/08/15/Attention-Mechanism-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">梳理</a>，上一次对于什么是注意力机制，我的回答是：</p><blockquote><p>聚焦在某个局部的 focus</p></blockquote><p>现在我的回答是：Attention（一般指的是 Self-Attention），是特征提取过程中，信息融合的手段。其目的是能够让模型的信息视野有的放矢，其数学上的表现就是加权和。</p><p>NLP community 曾经有过这么一种说法：</p><blockquote><p>an LSTM with attention will yield state-of-the-art performance on any task</p></blockquote><p>以及这样一张图：</p><p><img src="/img/joke.jpg" alt="Joke"></p><p>中心思想就是：Attention + LSTM 是一个非常 Powerful 的 model，基本能在所有的 NLP 任务上 work。就我有限的经验来说，大抵如此了。特别是 Attention，简直是即插即用效果还特别好的万金油。</p><p>而论文中对 Attention 的定义是这样的：</p><blockquote><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. </p></blockquote><p>这里的 query、key、value 是理解的重点：</p><p>对于机器翻译任务来说，在传统的 Seq2Seq 架构中，假设我们将要输出第 k 个词，那么这个 query 就代表这<strong>第 k 个词</strong>对应的 hidden state，key 和 value 一般是相等的（作者也提出了一种不相等的方式，详见下图），即之前 encode 的所有 hidden state：</p><p><img src="/img/qkv.jpg" alt="Query、Key and Values"></p><p>一开始提出 Attention 的使用一个 Alignment Function 来描述，并且提出了几种 score 的计算方式。这里的计算公式就是用最普通的矩阵乘法：</p><p>$Attention(Q, K, V) = softmax( \frac{QK^T}{\sqrt{d_k}})V$</p><p>Softmax 项就是权重项，$V$ 是一系列 hidden states，也就是说，attention 最终的表现形式依旧是<strong>加权和</strong>。</p><a id="more"></a><h2 id="Multihead-Attention"><a href="#Multihead-Attention" class="headerlink" title="Multihead Attention"></a>Multihead Attention</h2><p>到了本文最重要的部分， Multi-head Attention。作者的 Motivation 认为是原有的 RNN 和 CNN 并行化不够，太慢了；同时觉得原先的复杂度太高，像 RNN，从头滚到尾关于序列长度是一个 $O(n)$ 的复杂度。所以期望单用一个 Attention 来做特征的提取，因而提出了 Multi-head Attention。</p><p><img src="/img/multi-head.png" alt="Multi-head Attention"></p><p>$MultiHead(Q, K, V) = Concat( head_1, … , head_h) W^O$</p><p>$head_i = Attention(QW_i^Q, KW_i^L, VW_i^V)$</p><p>就是先让 Q，K，V 做一个线性的投影（分别乘上个矩阵），再做 Attention，这样重复多次，将结果拼接起来，得到一个“多头” Attention。</p><p>背后的动机是什么呢？文章中这样说：</p><blockquote><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. </p></blockquote><p>一方面，从直觉上多次 Attention 操作就能够捕获更多的信息；另一方面，先进行的投影操作能够把 Q、K、V 映射到不同空间，也许能够发现更多的特征。</p><p>然后再给他套上一层全连接：</p><p>$FFN(x) = ReLU(xW_1 + b_1) W_2 + b_2$</p><p>这样的 Attention 操作没有考虑到时序信息，但序列位置的信息还是很重要的，因此，作者对位置信息进行了 Encoding:</p><p><img src="/img/pe.png" alt="Position Encoding"></p><p>同时文章还仿照 CNN，增加了常用的 Residual Connection 以及 Layer Normalization 操作，这里就不再展开。</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>该 Paper 有 TensorFlow 的开源<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">实现</a>，侧重看一下 Multi-head Attention 以及 FFN 的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span><span class="params">(queries,</span></span></span><br><span class="line"><span class="function"><span class="params">                        keys,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_units=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        dropout_rate=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        is_training=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                        causality=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                        scope=<span class="string">"multihead_attention"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        reuse=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=reuse):</span><br><span class="line">        <span class="keyword">if</span> num_units <span class="keyword">is</span> <span class="keyword">None</span>:  <span class="comment"># set default size for attention size C</span></span><br><span class="line">            num_units = queries.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Linear Projections 线性投影</span></span><br><span class="line">        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu)  <span class="comment"># [N, T_q, C]</span></span><br><span class="line">        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  <span class="comment"># [N, T_k, C]</span></span><br><span class="line">        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  <span class="comment"># [N, T_k, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split and concat 分割成 head = 8 块，再拼起来</span></span><br><span class="line">        Q_ = tf.concat(tf.split(Q, num_heads, axis=<span class="number">-1</span>), axis=<span class="number">0</span>)  <span class="comment"># [num_heads * N, T_q, C/num_heads]</span></span><br><span class="line">        K_ = tf.concat(tf.split(K, num_heads, axis=<span class="number">-1</span>), axis=<span class="number">0</span>)  <span class="comment"># [num_heads * N, T_k, C/num_heads]</span></span><br><span class="line">        V_ = tf.concat(tf.split(V, num_heads, axis=<span class="number">-1</span>), axis=<span class="number">0</span>)  <span class="comment"># [num_heads * N, T_k, C/num_heads]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Attention  根据公式，做 Attention 计算 weight matrix</span></span><br><span class="line">        outputs = tf.matmul(Q_, tf.transpose(K_, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])) <span class="comment"># (num_heads * N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scale 缩放操作 outputs = outputs / sqrt( d_k)</span></span><br><span class="line">        outputs = outputs / (K_.get_shape().as_list()[<span class="number">-1</span>] ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Key Masking</span></span><br><span class="line">        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=<span class="number">-1</span>)))  <span class="comment"># (N, T_k)</span></span><br><span class="line">        key_masks = tf.tile(key_masks, [num_heads, <span class="number">1</span>])  <span class="comment"># (h*N, T_k)</span></span><br><span class="line">        key_masks = tf.tile(tf.expand_dims(key_masks, <span class="number">1</span>), [<span class="number">1</span>, tf.shape(queries)[<span class="number">1</span>], <span class="number">1</span>])  <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        paddings = tf.ones_like(outputs) * (<span class="number">-2</span> ** <span class="number">32</span> + <span class="number">1</span>)  <span class="comment"># -infinity</span></span><br><span class="line">        outputs = tf.where(tf.equal(key_masks, <span class="number">0</span>), paddings, outputs)  <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Causality = Future blinding</span></span><br><span class="line">        <span class="keyword">if</span> causality:</span><br><span class="line">            diag_vals = tf.ones_like(outputs[<span class="number">0</span>, :, :])  <span class="comment"># (T_q, T_k)</span></span><br><span class="line">            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()  <span class="comment"># (T_q, T_k)</span></span><br><span class="line">            masks = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), [tf.shape(outputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">            paddings = tf.ones_like(masks) * (<span class="number">-2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line">            outputs = tf.where(tf.equal(masks, <span class="number">0</span>), paddings, outputs)  <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Activation: outputs is a weight matrix</span></span><br><span class="line">        outputs = tf.nn.softmax(outputs)  <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Query Masking</span></span><br><span class="line">        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=<span class="number">-1</span>)))  <span class="comment"># (N, T_q)</span></span><br><span class="line">        query_masks = tf.tile(query_masks, [num_heads, <span class="number">1</span>])  <span class="comment"># (h*N, T_q)</span></span><br><span class="line">        query_masks = tf.tile(tf.expand_dims(query_masks, <span class="number">-1</span>), [<span class="number">1</span>, <span class="number">1</span>, tf.shape(keys)[<span class="number">1</span>]])  <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line">        outputs *= query_masks  <span class="comment"># broadcasting. (N, T_q, C)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropouts</span></span><br><span class="line">        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># weighted sum</span></span><br><span class="line">        outputs = tf.matmul(outputs, V_) <span class="comment"># ( h*N, T_q, C/h)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reshape</span></span><br><span class="line">        outputs = tf.concat(tf.split(outputs, num_heads, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># (N, T_q, C)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># residual connection</span></span><br><span class="line">        outputs += queries</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer normaliztion</span></span><br><span class="line">        outputs = layer_normalization(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>基本就是按着 Paper 来的，不过一个很让人费解的地方就是其中的 Key Masking 和 Query Masking，Paper 中写是 Optional 的，代码的作者非常细致的实现了这一部分。其目的是考虑到<strong>变长的序列</strong>，比如第一句的长度为 128 而第二句只有 64，对于第二句，其 Encoding 的结果或者说是 Hidden State 的后面 64 个单元是没有意义的，因此将其设置为一个非常小的数，从而对应的权重接近 0；Query 类似。具体内容可以参考这个<a href="https://github.com/Kyubyong/transformer/issues/3" target="_blank" rel="noopener">Issue</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feed Forward Network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(inputs,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_units=[<span class="number">2048</span>, <span class="number">512</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                scope=<span class="string">"multihead_attention"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=reuse):</span><br><span class="line">        <span class="comment"># Inner layer</span></span><br><span class="line">        params = &#123;<span class="string">"inputs"</span>: inputs, <span class="string">"filters"</span>: num_units[<span class="number">0</span>], <span class="string">"kernel_size"</span>: <span class="number">1</span>,</span><br><span class="line">                  <span class="string">"activation"</span>: tf.nn.relu, <span class="string">"use_bias"</span>: <span class="keyword">True</span>&#125;</span><br><span class="line">        outputs = tf.layers.conv1d(**params)</span><br><span class="line">        <span class="comment"># Readout layer</span></span><br><span class="line">        params = &#123;<span class="string">"inputs"</span>: outputs, <span class="string">"filters"</span>: num_units[<span class="number">1</span>], <span class="string">"kernel_size"</span>: <span class="number">1</span>,</span><br><span class="line">                  <span class="string">"activation"</span>: <span class="keyword">None</span>, <span class="string">"use_bias"</span>: <span class="keyword">True</span>&#125;</span><br><span class="line">        outputs = tf.layers.conv1d(**params)</span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line">        outputs += inputs</span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line">        outputs = layer_normalization(outputs)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>FFN 的实现就很简单，用两个 conv1d 的卷积，手动写矩阵乘法也可以；另外就是最后两步的 Residual Connection 直接加上输入以及 Layer Normalization。</p><p>PS：我拿着这个代码跑了一下 IMDB 的文本分类，只用了 Multi-head 和 FFN，Query 是一个随机初始化的向量，Key 和 Value就是经过 embedding 后的句子。 和 LSTM 对比下来，时间是 LSTM 的 6 倍，效果比 LSTM 还差… 为什么呢？因为没有<strong>并行化</strong>，事实上那些矩阵乘法都是可以用多块 GPU 来并行进行的，论文就说他们用了 <strong>8 块 P100</strong>。流下了没有钱的泪水。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/34781297" target="_blank" rel="noopener">《attention is all you need》解读</a></p><p><a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Github-Kyubyong/transformer</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention Is All You Need&lt;/a&gt; 前段时间火了一把，其提出完全用 Attention 替代传统的 CNN 和 RNN 架构来做特征的提取，也在 NMT 上也取得了 state-of-the-art。这两天读了一下这篇 Paper，并且在熟悉的 Text Classification 问题上用其模型做了一下尝试，这篇 Blog 就用来记录过程中的一些想法和感受。&lt;/p&gt;
&lt;h2 id=&quot;What-is-Attention&quot;&gt;&lt;a href=&quot;#What-is-Attention&quot; class=&quot;headerlink&quot; title=&quot;What is Attention&quot;&gt;&lt;/a&gt;What is Attention&lt;/h2&gt;&lt;p&gt;注意力机制之前在学的时候就有过一次&lt;a href=&quot;http://tobiaslee.top/2017/08/15/Attention-Mechanism-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/&quot;&gt;梳理&lt;/a&gt;，上一次对于什么是注意力机制，我的回答是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;聚焦在某个局部的 focus&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;现在我的回答是：Attention（一般指的是 Self-Attention），是特征提取过程中，信息融合的手段。其目的是能够让模型的信息视野有的放矢，其数学上的表现就是加权和。&lt;/p&gt;
&lt;p&gt;NLP community 曾经有过这么一种说法：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;an LSTM with attention will yield state-of-the-art performance on any task&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;以及这样一张图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/joke.jpg&quot; alt=&quot;Joke&quot;&gt;&lt;/p&gt;
&lt;p&gt;中心思想就是：Attention + LSTM 是一个非常 Powerful 的 model，基本能在所有的 NLP 任务上 work。就我有限的经验来说，大抵如此了。特别是 Attention，简直是即插即用效果还特别好的万金油。&lt;/p&gt;
&lt;p&gt;而论文中对 Attention 的定义是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里的 query、key、value 是理解的重点：&lt;/p&gt;
&lt;p&gt;对于机器翻译任务来说，在传统的 Seq2Seq 架构中，假设我们将要输出第 k 个词，那么这个 query 就代表这&lt;strong&gt;第 k 个词&lt;/strong&gt;对应的 hidden state，key 和 value 一般是相等的（作者也提出了一种不相等的方式，详见下图），即之前 encode 的所有 hidden state：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/qkv.jpg&quot; alt=&quot;Query、Key and Values&quot;&gt;&lt;/p&gt;
&lt;p&gt;一开始提出 Attention 的使用一个 Alignment Function 来描述，并且提出了几种 score 的计算方式。这里的计算公式就是用最普通的矩阵乘法：&lt;/p&gt;
&lt;p&gt;$Attention(Q, K, V) = softmax( \frac{QK^T}{\sqrt{d_k}})V$&lt;/p&gt;
&lt;p&gt;Softmax 项就是权重项，$V$ 是一系列 hidden states，也就是说，attention 最终的表现形式依旧是&lt;strong&gt;加权和&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>BPTT 推导</title>
    <link href="https://tobiaslee.top/2018/04/28/BPTT-notes/"/>
    <id>https://tobiaslee.top/2018/04/28/BPTT-notes/</id>
    <published>2018-04-28T08:27:12.000Z</published>
    <updated>2018-04-28T11:56:46.319Z</updated>
    
    <content type="html"><![CDATA[<p>写 IndRNN 解读的时候，对于 RNN 求导那一块其实没什么底气，因为框架都帮你实现了，所以学的时候也都避重就轻地略过去了。但 Back Propagation Through Time (BPTT) 是绕不过去的，这就是地基，不把基础打扎实了，就会发生“眼看他起高楼，眼看他楼塌了”这样的悲剧。</p><h2 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h2><p>首先是矩阵求导，可以参考一下<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术</a>，这里就给出两个最用的公式：</p><ol><li>$ \mathbf{J}_{i,j} = \frac{\partial f_i}{\partial x_j\ }$，其中： $ \mathbf{f} : \mathbb{R}^n - &gt;  \mathbb{R}^m$ 是一个 n 维到 m 维的映射，其求导的结果是一个雅各比矩阵，其中元素为 $f_i$ 对 $x_j$ 的导数</li><li>$ \mathbf{f} = A\mathbf{x}$, $ \frac{\partial f}{\partial x} = A^T$</li></ol><p>在这两个式子的基础之上，就可以开始我们的求导啦~</p><h2 id="BPTT"><a href="#BPTT" class="headerlink" title="BPTT"></a>BPTT</h2><p>来一起默写一下 RNN 的公式，其中 $\sigma$ 代表激活函数；：</p><p>$$ \mathbf{a}^{(t)} = \mathbf{b} + \mathbf{W} \mathbf{h}^{(t-1)} + \mathbf{U} \mathbf{x}^{(t)}$$</p><p>$$ \mathbf{h}^{t} = \sigma (\mathbf{a}^{(t)})$$</p><p>$$ \mathbf{o}^{(t)} = \mathbf{c} + \mathbf{V} \mathbf{h}^{(t)}$$</p><p>$$ \hat{y}^{(t)} = softmax(\mathbf{o}^{(t)}) $$ </p><p>这里我们用双曲正切 $tanh(x)$ 作为激活函数，其导数为 $ 1 - tanh^2(x)$ ，并且假设最后输出层使用 softmax 函数来得到一个概率向量。</p><p>来进行愉快地求导吧，记住一点原则，<strong>由外向内一层层求导</strong>。</p><a id="more"></a><p>如果我们的 Loss Function 是一个负对数似然，则对于 t 时刻对 $o^{(t)}$ 导数的第 i 个单元的值为</p><p><img src="/img/deri_final.png" alt="o-T and h-T"></p><p>因此，我们就可以很轻松求出 Loss 对最后一个时刻 T 的 $h_T$ 的导数。</p><p>有了最后一个时刻 T 的导数之后，我们通过反向迭代，从 $t = T -1$ 一直到 $ t = 1$，对 $h^{(t)}$ 求导：</p><p><img src="/img/deri_h.png" alt="Derivative of h-t"></p><p>注意，这里的导数由两部分组成，一部分直接来自 $o^{(t)}$；另一部分，因为 $h^{(t+1)}$ 的计算依赖于 $h^{(t)}$ ，所以这一部分的梯度也从下一时刻流入。梯度不仅来自于当前输出，还来自于下一时刻的输出，这也就是 BPTT 名字的由来。这里的 <code>diag</code> 是对角阵，其产生的原因是因为我们的激活函数 $tanh(x)$ 是一个 element-wise 的操作，形状不变，求导得到的雅各比矩阵是一个方阵；同时，又有 $\mathbf{J}_{i,j} = 0, i != j$（还是因为 element-wise，结果只和原来对应位置上的元素有关），因而只有主对角线上的元素不为 0，而是 $tanh(x)$ 的导数，因此结果就如上图所示。</p><p>有了以上的结果，我们再对我们需要更新的偏置和权重变量进行求导就方便了很多，首先是偏置：</p><p><img src="/img/bias_deri.png" alt="Bias Derivative"></p><p>同样需要注意的是这里也有一个 <code>diag</code>，同样是因为激活函数存在的缘故，不再赘述。</p><p>权重矩阵也类似：</p><p><img src="/img/weight_deri.png" alt="Weight Derivative"></p><p>至此，所有需要更新的变量的求导就已经完成了，这里诸如转置和矩阵乘的位置的细节不必太过纠结，知道整体的思路就可以我认为。</p><h2 id="再来看梯度消失问题"><a href="#再来看梯度消失问题" class="headerlink" title="再来看梯度消失问题"></a>再来看梯度消失问题</h2><p>求完导之后我第一反应就是找矩阵乘法，发现就根本没有乘法啊，只有一系列求和？其实是隐藏在在 $ \nabla _{h_{t}} L$里了；如果我们简化一下这前向传播的过程：</p><p>$$\mathbf{h}^{(t)} = \mathbf{W}^T \mathbf{h}^{(t-1)} $$</p><p>进一步递推得到：</p><p>$$\mathbf{h}^{(t)} = \mathbf{(W^t)}^T \mathbf{h}^{(0)} $$</p><p>然后我们再用特征值分解的方法，来计算这个连乘：</p><p>$$\mathbf{h}^{(t)} = \mathbf{Q}^TA^t\mathbf{Q} \mathbf{h}^{(0)} $$</p><p>特征值不到 1 的，在 t 次之后，就衰减为 0；超过 1 的，则会激增。那么 $h^{(0)}$ 中不与最大特征向量对齐的部分就会被丢弃。</p><p>这是从正向传播的过程来看梯度消失，其表现就是<strong>后面的状态把前面的忘干净了</strong>，也就是长期依赖的问题。</p><p>不过简化分析的时候我们没有考虑激活函数，采用的激活函数的导数也是有界的，比如 tanh 导数就小于 1，sigmoid 导数小于等于 0.25，这个导数也是参与到连乘中的结果也会导致梯度急速下降，这也可能就是梯度消失概率比爆炸概率大得多的原因之一。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>推导的过程主要参考了花书《Deep Learning》中文版，此时突然想到高中老师的一句话：人生需要几本垫底的书。希望花书能成为替我垫底的几本书之一吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;写 IndRNN 解读的时候，对于 RNN 求导那一块其实没什么底气，因为框架都帮你实现了，所以学的时候也都避重就轻地略过去了。但 Back Propagation Through Time (BPTT) 是绕不过去的，这就是地基，不把基础打扎实了，就会发生“眼看他起高楼，眼看他楼塌了”这样的悲剧。&lt;/p&gt;
&lt;h2 id=&quot;矩阵求导&quot;&gt;&lt;a href=&quot;#矩阵求导&quot; class=&quot;headerlink&quot; title=&quot;矩阵求导&quot;&gt;&lt;/a&gt;矩阵求导&lt;/h2&gt;&lt;p&gt;首先是矩阵求导，可以参考一下&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24709748&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;矩阵求导术&lt;/a&gt;，这里就给出两个最用的公式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$ \mathbf{J}_{i,j} = \frac{\partial f_i}{\partial x_j\ }$，其中： $ \mathbf{f} : \mathbb{R}^n - &amp;gt;  \mathbb{R}^m$ 是一个 n 维到 m 维的映射，其求导的结果是一个雅各比矩阵，其中元素为 $f_i$ 对 $x_j$ 的导数&lt;/li&gt;
&lt;li&gt;$ \mathbf{f} = A\mathbf{x}$, $ \frac{\partial f}{\partial x} = A^T$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这两个式子的基础之上，就可以开始我们的求导啦~&lt;/p&gt;
&lt;h2 id=&quot;BPTT&quot;&gt;&lt;a href=&quot;#BPTT&quot; class=&quot;headerlink&quot; title=&quot;BPTT&quot;&gt;&lt;/a&gt;BPTT&lt;/h2&gt;&lt;p&gt;来一起默写一下 RNN 的公式，其中 $\sigma$ 代表激活函数；：&lt;/p&gt;
&lt;p&gt;$$ \mathbf{a}^{(t)} = \mathbf{b} + \mathbf{W} \mathbf{h}^{(t-1)} + \mathbf{U} \mathbf{x}^{(t)}$$&lt;/p&gt;
&lt;p&gt;$$ \mathbf{h}^{t} = \sigma (\mathbf{a}^{(t)})$$&lt;/p&gt;
&lt;p&gt;$$ \mathbf{o}^{(t)} = \mathbf{c} + \mathbf{V} \mathbf{h}^{(t)}$$&lt;/p&gt;
&lt;p&gt;$$ \hat{y}^{(t)} = softmax(\mathbf{o}^{(t)}) $$ &lt;/p&gt;
&lt;p&gt;这里我们用双曲正切 $tanh(x)$ 作为激活函数，其导数为 $ 1 - tanh^2(x)$ ，并且假设最后输出层使用 softmax 函数来得到一个概率向量。&lt;/p&gt;
&lt;p&gt;来进行愉快地求导吧，记住一点原则，&lt;strong&gt;由外向内一层层求导&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="BPTT" scheme="https://tobiaslee.top/tags/BPTT/"/>
    
  </entry>
  
  <entry>
    <title>GAN in NLP Notes</title>
    <link href="https://tobiaslee.top/2018/04/22/GAN-in-NLP-Notes/"/>
    <id>https://tobiaslee.top/2018/04/22/GAN-in-NLP-Notes/</id>
    <published>2018-04-22T14:16:25.000Z</published>
    <updated>2018-04-23T14:41:01.793Z</updated>
    
    <content type="html"><![CDATA[<p>最近在感兴趣的一个方向是文本生成，读了以 SeqGAN 为代表的一系列通过 RL 手段来做生成的文章，做一下简单的梳理。</p><h2 id="SeqGAN"><a href="#SeqGAN" class="headerlink" title="SeqGAN"></a>SeqGAN</h2><p>算的上是开山之作，具体的解读可以看我之前的一篇文章 <a href="http://tobiaslee.top/2018/03/11/SeqGAN/">SeqGAN – GAN + RL + NLP</a>，其通过引入强化学习中的 Policy Gradient 来解决因为离散 token 生成前采样动作造成的不可微。后面的文章也都是基于这个框架来进行深一步地探索。SeqGAN 在 Oracle 和古诗生成任务上做了测试，回过头来看，效果只能说一般。但其开创性的将文本生成看做序列决策问题， 并且将 RL 和 GAN 进行了有机的结合，令人佩服。</p><h2 id="LeakGAN"><a href="#LeakGAN" class="headerlink" title="LeakGAN"></a>LeakGAN</h2><p>LeakGAN 是交大继 SeqGAN 之后的又一作品，其旨在解决 SeqGAN 中出现的一个问题：一方面，是 Discriminator 提供给 Generator 的 reward 需要等句子完成之后才能被计算（即使用 Monte Carlo 来计算，也只是一种近似的模拟），对于每一步的 token 生成不能得到及时的反馈；另一方面，是 Reward  本身只是一个 Scalar，并不能携带太多的信息。何况对于文本这种结构复杂，同样的意思不同的说法都是可以的，那么数值所包含的指导信号比较弱。这一点我在和学长的交流之中以及自身实验体会也有所感受，GAN 的训练很多时候 Discriminator 的作用不大， Generator 生成文本质量的提高更多是靠 Generator 自学。</p><p>对于信息不足，就让 Discriminator 向 Generator “泄露”一些消息，也就是把作为 Discriminator 的 CNN 最后一层的 Feature Vector 交给 Generator，让这个 Feature Vector 携带大量的信息来指导 Generator 更好的生成。</p><p>同时，语义结构的复杂也促使作者使用一个具有层次的生成器，因此在生成器端使用了 Manager 和 Worker 两个模块，分别用于解析 CNN 提供的 Feature Vector 和具体的 token 生成。</p><p>整个模型的架构如下：</p><p><img src="/img/leakgan.png" alt="Leak GAN"></p><h2 id="RankGAN"><a href="#RankGAN" class="headerlink" title="RankGAN"></a>RankGAN</h2><p>RankGAN 认为 Discriminator 的 Binary Classification 对于生成多样、符合现实逻辑的文本是不够的。其通过让 Discriminator 对于一个由 human-written 和 machine-generated 构成 Reference 集中的句子进行排序，来指导 Generator 的生成。借鉴 IR 中的相似度的思想，其首先计算两个句子的 cosine，然后据此给出一个语料集上的排序分：</p><p><img src="/img/cosine.png" alt="Cosine Similarity of Sentence"></p><p><img src="/img/rank_score.png" alt="Rank Score"></p><p>Softmax 之后得到的依旧是一个类似概率的结果，但这一次不再是真/假的概率，而是衡量句子之间相似程度的概率。最终，和 Reference 中的每个句子的相似概率的数学期望则作为一个 Rank Score 得出，显然，Discriminator 希望真实的句子 Rank Score 越高越好，同时机器生成的 Rank Score 越低越好，因此我们将这个想法套进 GAN 的公式：</p><p><img src="/img/rank_gan.png" alt="Formula"></p><p>所以，RankGAN 的核心就是<strong>用一个 Ranker 来替代 Discriminator，以提供更好地生成句子的评估，进而生帮助 Generator 生成更为真实的句子</strong>。</p><h2 id="MaskGAN"><a href="#MaskGAN" class="headerlink" title="MaskGAN"></a>MaskGAN</h2><p>MaskGAN 的思路则是从<strong>生成器端</strong>来为生成提供更多的信息，具体的内容也可以参考我的这篇 <a href="http://tobiaslee.top/2018/04/01/MaskGAN-Notes/">MaskGAN 学习笔记</a>。Masked Token 给 GAN 提供了额外的信息窗口，通过使用 Seq2Seq 架构来将进行生成，将提供的信息整合进生成过程之中，获取更好的结果。另外一点，Mask GAN 采用了 Actor-Critic 来替换 Policy Gradient，相比 Monte Carlo，能够较好地对 reward function 做一个拟合。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这四篇文章看下来，可以发现，SeqGAN 提出了这样一个框架之后，主要工作的方向有两个：</p><ol><li>提高生成文本的质量：我认为，将人类的语言学知识合理地整合进生成的过程，是提高文本生成质量的核心。现有的工作大致是从两个端入手，生成器端通过 Masked Token，比较粗暴地灌进 Generator，Discriminator 端利用句子地相似性，隐式地给判别器端输入语言的知识。<strong>二者如果能够做一个结合，可能是一个很有力的架构</strong>。</li><li>短文本 -&gt; 长文本：想要生成长文本，一点是 LeakGAN 中提到的，Discriminator 的指导需要更加 informative；另外一点我觉得和 RNN 的 Long-dependency 的能力也息息相关，LSTM 虽然说能够解决长程依赖的一部分问题，但想要让机器写小说，那就不是 LSTM 能够 handle 的了。LeakGAN 能够把 SeqGAN 的长度由 20 提升到 40左右，但真的到作曲、写小说这种长度，可能需要一个新的框架。</li><li>解决 Mode Collapse 问题：这个问题可能是前两个问题解决之后，才需要着手考虑的一个问题，即使是我们人类本身，也有着这方面的局限性。比如你的文风事实上就是被你所读过的文章所决定地，虽然说会有一些神来之笔，但我认为那也是早就植根于我们大脑深处的记忆。想要做到真正地创作而不是简单地重复，任重而道远。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在感兴趣的一个方向是文本生成，读了以 SeqGAN 为代表的一系列通过 RL 手段来做生成的文章，做一下简单的梳理。&lt;/p&gt;
&lt;h2 id=&quot;SeqGAN&quot;&gt;&lt;a href=&quot;#SeqGAN&quot; class=&quot;headerlink&quot; title=&quot;SeqGAN&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>从 IndRNN 来回顾 RNN 模型</title>
    <link href="https://tobiaslee.top/2018/04/19/IndRNN-Notes/"/>
    <id>https://tobiaslee.top/2018/04/19/IndRNN-Notes/</id>
    <published>2018-04-19T15:01:39.000Z</published>
    <updated>2018-04-28T08:25:02.370Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.04831" target="_blank" rel="noopener">IndRNN</a> 是最近新提出的一种 RNN 架构，作者认为其能够更好地解决 gradient vanishing / exploding 以及具有更好的解释性。</p><h2 id="从-RNN-到-IndRNN"><a href="#从-RNN-到-IndRNN" class="headerlink" title="从 RNN 到 IndRNN"></a>从 RNN 到 IndRNN</h2><p>首先来复习一下 RNN t 时刻的 hidden state 的计算公式：</p><p><img src="/img/vanilla_rnn_formula.png" alt="Vanilla RNN Formula"></p><p>这个公式相信都是了然于心，如果画成图的话，就是下面这个样子：</p><p><img src="/img/RNN-rolled.png" alt="RNN rolled"></p><p><em>图片引用自 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">colah’blog</a></em></p><p>但是，请问一下自己，这个 A 里面是怎么样的一个结构？如果不能一下子回答出来，那么你可能和我一样，从来都不曾真正的理解 RNN。</p><p>在这个 RNN cell 的内部单元，其连接的状态是这样的：</p><p><img src="/img/vanilla_rnn.png" alt="Vanilla RNN"></p><p>注意：<strong>$h_{t-1}$ 和 $h_t$ 之间是一个全连接的关系</strong>。 </p><p>而 IndRNN 的计算公式呢，做了一个小小的修改：</p><p><img src="/img/ind_rnn_formula.png" alt="IndRNN Formula"></p><p>从矩阵乘变成了 haramard dot，也就是 element wise 的乘法，能给 RNN 带来什么样的变化呢？</p><p><img src="/img/ind_rnn.jpg" alt="IndRNN"></p><p>全连接变成了单个神经元自身之间的传递，这带来两个变化：</p><ol><li>更好地解决梯度爆炸 / 消失</li><li>更好的解释性</li></ol><p>接下来就从这两个方面入手来深入了解一下 IndRNN</p><a id="more"></a><h2 id="梯度爆炸-消失"><a href="#梯度爆炸-消失" class="headerlink" title="梯度爆炸/消失"></a>梯度爆炸/消失</h2><p>首先来复习一下 RNN 的梯度怎么计算：</p><p>假设目标函数为 $J$，我们对第 $T$ 个 hidden state $h_T$求导，结果为：</p><p>$$\frac{\partial{J}}{\partial{h_T}} \Pi_{k=t}^{T-1}diag(\sigma’(h_{k+1}))\mathbf{U^T}$$</p><p>其中 <code>diag</code> 是指对角矩阵（其中对角的元素是激活函数对 $h_{k+1}$ 的导数），其实就是后一 hidden state 通过链式法则展开到最前的一个 hiddent state，求导的结果连乘。</p><p>矩阵的连乘可以通过对角化来简便计算，即化为 $PAP^{-1}​$，$A​$ 是一个对角阵，主对角线上的元素就是其特征值。而如果其特征值小于 1，那么在连续的乘积之后其值就会接近 0，梯度消失；如果大于 1，那么就会接近变成 NaN，梯度爆炸。解决的手段分别就是梯度裁剪（gradient clipping）和 合适的初始化 + 更换为 ReLU acitivation（目的是为了选择合适的特征值？）</p><p>LSTM 解决这一问题的思路是增加 gates，来控制信息的流动，从而较好的解决梯度消失问题（因为梯度爆炸用 clipping 能够比较粗暴地解决，而 vanishing 并不行）。LSTM 的求导比较繁琐，可以参考一下 <a href="http://arunmallya.github.io/writeups/nn/lstm/#/" target="_blank" rel="noopener">LSTM Forward and Backward </a> 。从繁琐的公式中比较难看出 LSTM 解决梯度消失和爆炸，我们可以通过下面这张图来直观的感受一下 LSTM 的作用：</p><p><img src="/img/compare.png" alt="Comparision Between RNN and LSTM"></p><p>左边是 RNN，右边是 LSTM；颜色的深浅表示了梯度影响的程度。可以看到，RNN 第一个时刻的受最后一个时刻的影响微乎其微，这种情况下就可以认为是出现了梯度消失，无法较好地更新我们的参数；右边的 LSTM，为了简化起见，我们将将 input gate 设为 0（图中的 <code>-</code> 符号），forget gate 始终记忆前一状态的信息（图中的 <code>o</code> 符号），我们可以将第一个时刻的信息一直传递至我们想要的 4, 6，并且其梯度的也能够通过这一条路径成功的回传。所以，通过<strong>控制输入以及先前状态的流动方式</strong>，LSTM 能够较好地解决梯度消失的问题。</p><p>GRU 则在 LSTM 基础之上做了简化：</p><p><img src="/img/gru.png" alt="GRU"></p><ol><li>将 Forget Gate 和 Input Gate 合并成一个 Update Gate $z_t$，不像 LSTM 是由两个<strong>独立</strong>的门来控制</li><li>使用一个 Reset Gate $r_t$ 来直接控制 $h_{t-1}$ 对 $h_t$ 的贡献，而 LSTM 在计算 $\hat c_t$ 的时候是没有一个 $r_t$ 来控制 $h_{t-1}$ 的</li></ol><p>这样做的好处很直观地一点就是减少参数的数量，能够加快训练速度；另外 $r_t$ 对于 $h_{t-1}$ 的控制能够让 cell 更好的理清过去时刻状态对现在状态的影响程度，但 Update 门的不独立性又使得它的效用有所下降。我猜测如果直接在 LSTM 的基础之上加一个 Reset Gate，可能效果会更好，但参数的数量就上去了，所以这里必然存在一个性能和速度的 tradeoff。</p><p>回到 IndRNN，其对于 $h_{n,t}$ 的梯度计算如下（t 时刻 hidden state 的第 n 个单元）：</p><p><img src="/img/grad_ind.png" alt="Gradient Of IndRNN"></p><p>最大的差别就是<strong>矩阵连乘变成了一个数的幂次</strong>，这样我们就可以通过控制这个矩阵中元素的大小来避免梯度消失和爆炸。当然，我们也可以选择一个合适的数值范围来让让梯度更好地流动，加快训练的速度。</p><p>实验的结果也证明，IndRNN 的长期记忆（也就是梯度能够传递的时间步数）效果远好于 RNN 和 LSTM（5000 vs 500~1000）</p><h2 id="解释性"><a href="#解释性" class="headerlink" title="解释性"></a>解释性</h2><p>因为 $h_{t-1}$ 和 $h_t$ 各个单元之间是相互独立的，那么就可以提供了看待 IndRNN 的两个权重矩阵的新的视角：</p><ol><li>$W$：负责提取输入的空间特征</li><li>$ U$: 提取时间上的特征</li></ol><p>而且，在这之后加一层全连接层，IndRNN 在一些条件的限制下（ two constrains : 1. linear activation; diagonalizable weight matrix）能够变成一个普通的 RNN 模型。</p><p>不过关于这一点，我觉得还需要更好地可视化的手段来帮助解释，否则这样的解释依旧停留在 Intuition 上，不足以说明问题。</p><h2 id="更深的-RNN"><a href="#更深的-RNN" class="headerlink" title="更深的 RNN"></a>更深的 RNN</h2><p>写这篇文章的时候突然想到一个问题，为什么 RNN 里面都使用 tanh /sigmoid 而不是 ReLU，Leaky ReLU 这种现在的爆款标配 activation 呢？事实上是有的，Hinton 在 <a href="https://arxiv.org/pdf/1504.00941.pdf" target="_blank" rel="noopener">IRNN</a> 这篇文章里就尝试使用一些初始化的 Trick 和 ReLU activation 来解决梯度消失的问题，并且能够取得近似 LSTM 的效果。</p><p>对于 LSTM 来说，其中有三个门，因为门的值需要在 [0, 1] 之间，所以选择 sigmoid 函数，没有问题；那么 Cell State 的计算和最后的 Hidden State 为什么不尝试使用 ReLU 呢，应该也没问题，求导还快，计算也方便，但是同时也就有两个缺点，一个是<strong>不是以 0 为中心，这个在 CS231n 中对于各种激活函数中有讲到，ReLU 虽然是 Non-saturated activation，但他的值域是大于零的 </strong>；另外一点，就是如果使用 ReLU，那么 LSTM 的输出可能会很大。所以，这里同样存在着一个 trade off，考虑到 LSTM 提出的时间以及在各个任务上性能都还不错，替换的需求不大，所以可能也就这么用下来了。所以，使用 <strong>ReLU 或者 Leaky ReLU</strong> 是可行的。</p><p>既然如此，那么 IndRNN 用上 ReLU 更加没什么问题，所以作者也提出能够借鉴 CNN 中的一些方法，使用类似 ResNet 进行堆叠，得到更深的 RNN：</p><p><img src="/img/deep_rnn.png" alt="Deep RNN"></p><p>对于 ResNet 的了解不多，所以这里先暂时搁置，有待后面补上。但看到 Batch Normalization 以及 ResNet，也是在提醒我们 CNN 和 RNN 相互借鉴非常重要。</p><h2 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h2><p>非常感谢来自一位资深 NLP 算法工程师泼的冷水和指导，让我意识到自己的局限性。希望以后自己能够在以下三个方面继续努力：</p><ol><li>独立思考：现在养成了读二手再到原版资料（先论文解读，再看论文本身）这种习惯，前人的理解会禁锢我们的看法，并且每个人都会有自己的盲点。应该是一手资料现行，二手资料对比补充。</li><li>关注新进展的同时，注重基础的沉淀：IndRNN 就是最好的例子，同时还应该更多横向地对比相关的文章来获得更全面的理解；还需要补一些语言学的知识来形成更完整的体系。</li><li>代码的优化和重构：复现阶段运行优先没错，但完成之后应该回过头来重新优化和重构代码，这应该是一个计算机科班学生应有的素质，同时，重写代码也能够进一步提升 Coding 的能力。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="http://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf" target="_blank" rel="noopener">RNN Tutorial-Toronto University</a></p><p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding-LSTM</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.04831&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;IndRNN&lt;/a&gt; 是最近新提出的一种 RNN 架构，作者认为其能够更好地解决 gradient vanishing / exploding 以及具有更好的解释性。&lt;/p&gt;
&lt;h2 id=&quot;从-RNN-到-IndRNN&quot;&gt;&lt;a href=&quot;#从-RNN-到-IndRNN&quot; class=&quot;headerlink&quot; title=&quot;从 RNN 到 IndRNN&quot;&gt;&lt;/a&gt;从 RNN 到 IndRNN&lt;/h2&gt;&lt;p&gt;首先来复习一下 RNN t 时刻的 hidden state 的计算公式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/vanilla_rnn_formula.png&quot; alt=&quot;Vanilla RNN Formula&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个公式相信都是了然于心，如果画成图的话，就是下面这个样子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/RNN-rolled.png&quot; alt=&quot;RNN rolled&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;图片引用自 &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;colah’blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;但是，请问一下自己，这个 A 里面是怎么样的一个结构？如果不能一下子回答出来，那么你可能和我一样，从来都不曾真正的理解 RNN。&lt;/p&gt;
&lt;p&gt;在这个 RNN cell 的内部单元，其连接的状态是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/vanilla_rnn.png&quot; alt=&quot;Vanilla RNN&quot;&gt;&lt;/p&gt;
&lt;p&gt;注意：&lt;strong&gt;$h_{t-1}$ 和 $h_t$ 之间是一个全连接的关系&lt;/strong&gt;。 &lt;/p&gt;
&lt;p&gt;而 IndRNN 的计算公式呢，做了一个小小的修改：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/ind_rnn_formula.png&quot; alt=&quot;IndRNN Formula&quot;&gt;&lt;/p&gt;
&lt;p&gt;从矩阵乘变成了 haramard dot，也就是 element wise 的乘法，能给 RNN 带来什么样的变化呢？&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/ind_rnn.jpg&quot; alt=&quot;IndRNN&quot;&gt;&lt;/p&gt;
&lt;p&gt;全连接变成了单个神经元自身之间的传递，这带来两个变化：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;更好地解决梯度爆炸 / 消失&lt;/li&gt;
&lt;li&gt;更好的解释性&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来就从这两个方面入手来深入了解一下 IndRNN&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="RNN" scheme="https://tobiaslee.top/tags/RNN/"/>
    
      <category term="论文笔记" scheme="https://tobiaslee.top/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>论文写作的一些经验</title>
    <link href="https://tobiaslee.top/2018/04/03/How-to-Write-a-Paper/"/>
    <id>https://tobiaslee.top/2018/04/03/How-to-Write-a-Paper/</id>
    <published>2018-04-03T06:04:04.000Z</published>
    <updated>2018-04-03T12:24:09.468Z</updated>
    
    <content type="html"><![CDATA[<p>人生中第一篇论文总算接近完成，<a href="http://web.xidian.edu.cn/ysxu/" target="_blank" rel="noopener">徐老师</a>真的超级 nice，几乎是逐字逐句在帮忙修改论文，并且传授了很多论文写作的经验和技巧。这一阵子也在看杨强老师的《学术研究，你的成功之道》，也有一些收获和感悟，这篇文章就记录我这个新手在写作论文过程中的一些感想。</p><h2 id="Why-Do-Reaserch"><a href="#Why-Do-Reaserch" class="headerlink" title="Why Do Reaserch"></a>Why Do Reaserch</h2><p>思考为什么要做研究其实是很重要的问题。在本科阶段，没有什么科研的压力（不会因为发不出 Paper 而毕不了业），选择做研究其实是我的一次尝试，探索一些某一个小领域的最前沿。<strong>就个人目前体验而言，我非常推荐同学们能够去尝试一下科研的</strong>。不过，鉴于我校的师生比以及老师们的兴趣可能都在项目上，科研机会是需要同学们自己去争取的，大胆发邮件陶瓷把。另外，我原来认为美赛也算是一种短期科研，但亲身经历过美赛的四天之后，我发现美赛建模的含金量全在于报名费 100 刀，一路瞎编乱造东拼西凑出来的东西真的很难称得上是一篇”论文“。</p><p>但是话说回来，尝试这一个动机是不足以撑起一个人的研究生或者是博士生生涯的。如果真的选择要去做一名科研工作者，我觉得我现在的准备还是不足的。真的做研究需要 passion，而很多做研究的人又会因为各种压力而失去消磨原本就不多的激情，所以能找个舒服的地方无压力的做研究估计也是人生一大乐事了（逃</p><h2 id="How-to-Reaserch"><a href="#How-to-Reaserch" class="headerlink" title="How to Reaserch"></a>How to Reaserch</h2><p>关于如何做研究，我就记录一下个人的经历：</p><ol><li>明确方向：这个也可能是最难的一步了，因为计算机是一个很大的领域，哪怕是现在最火的深度学习也是有无数的子问题亟待解决，因而找到自己感兴趣、并且合适的问题方向是关键的一步。</li><li>学习基础知识：本科学的东西距离前沿的一些进展还是有一定距离，推荐利用好各种网课，像 Coursera 以及 YouTube 上各种 Standford、UCB 等名校的课程的视频，都能够帮我们迅速具备探索前沿内容的知识储备。</li><li>阅读<strong>大量</strong>文献：有了方向和一定的基础之后，就需要对特定领域已有的工作进行深挖，知道别人在做什么，对于某个问题有哪些常见的做法，后续做研究中写 Reference 、Related Work 就需要有大量的文献储备。一开始可能会读的比较慢，但是到了后面熟悉套路之后，看一下模型就大概作者在做什么，也能够快速地筛选出对自己有帮助的内容。所以，一定要读，并且要关注最新的成果。</li><li>代码实战：Talk is cheap,  show me the code. 这句至理名言无论是做科研还是做项目都是适用的。有些论文开源了代码，下载下来跑一下，看一下别人代码的实现，帮助是很大的。提升最快的自然是自己<strong>复现模型</strong>，这就要求对框架比如 TensorFlow 足够熟悉，并且对论文原理也很熟悉才能做到。代码和论文二者是相辅相成的，所以这一步和上一步要结合着进行。</li><li>站在巨人的肩膀上，进行一些创新：<a href="https://blog.csdn.net/malefactor/article/details/50583474" target="_blank" rel="noopener">以Attention Model为例谈谈两种研究创新模式- CSDN博客</a>，当初在学习文本分类的时候看到了这篇文章，对于我帮助很大。文章中说创新主要是两种方式：应用创新和模型创新。应用创新是指在已有模型，一般是最新的模型，然后尝试去解决一个尚未使用新方法解决的问题，结果如果比以前都出色的话，那就是一篇很不错的文章，我见过的就有发了 ACL 以及 EMNLP 的两篇应用创新的文章；另外就是难度比较大的模型创新，不过难度也和创新程度息息相关，如果只是 increment 的创新，那么难度可能会相对小一点，但如果是开创一个领域的创新，像 GAN / ResNet 这种，就需要深刻的积累啦。</li><li>验证 idea，修改 model：在 5、6 这两步之间迭代着更新自己的模型，并用在先前的代码积累之上进行验证，对实验的现象结果做好充分的记录和分析总结。</li></ol><h2 id="How-to-Write-a-Paper"><a href="#How-to-Write-a-Paper" class="headerlink" title="How to Write a Paper"></a>How to Write a Paper</h2><p>徐老师把写文章称之为”做后感“，我也挺认可这个说法。但是实验做好之后，如何把成果凝练成一篇能够发表的 paper 而不是写成 technique report，是需要技巧和打磨的。</p><ol><li><p>Sell Your Story：写文章其实是要讲一个故事，把你的故事讲好了，审稿人才会买你的帐。而讲故事的套路就是要突出你工作的重要性，字里行间中都需要有一种<strong>世界没有我的工作就不行啦</strong>的赶脚。</p></li><li><p>Structure：这是一个很容易被忽视的问题，但注意到这一点之后论文的写作也会一下子有了支撑。杨强老师的书里说:</p><blockquote><p>摘要是较高层面对论文工作的阐述，类似一个电梯演讲，在几分钟之内吸引到你的读者；</p><p>引言则是摘要的详细版，可以看做是扩写的摘要，对论文的各个部分进一步的解释，介绍问题背景、研究动机等等，和摘要一样，需要回避技术细节</p></blockquote><p>之后的部分则是 Related Work 和我们自己的工作 Model、Experiment 等了，就需要突出<strong>我们</strong>的工作，多用 <strong>we</strong>，<strong>our</strong> 这样的词语；需要注意的是，Model 部分和 Experiment 同样需要结构上的对应，对模型介绍的顺序和实验的顺序也需要保持一致。我写的时候常常写着写着就 stuck 进去，不见庐山真面目，没有考虑到整体的结构布局。</p></li><li><p>语法/词汇/句法：语法错误是比较低级的错误，自己多读几遍，看看主语到底是人还是物，尽可能避免犯遮掩的错误；而词汇，我是一个对重复用词有着近乎偏执追求的人… 所以我一般会尽可能避免使用同样的词很多次，但老师说没关系… whatever；句法，大约就是不要写太长的句子，让句子结构有些变化，错落有致就可以了。</p></li><li><p>Feed your Idea to Readers：这个点也在杨强老师里的书中有强调，新手总是<strong>认为读懂文章是审稿人的义务</strong>，事实上审稿人就像一个婴儿，需要你把所有东西一口一口喂给它，最好是嚼烂了，这样他更高兴。所以对于所有你认为显而易见的结论和结果，请<strong>一定要明明白白地写出来</strong>，告诉你的读者，嘿在这里，我都写啦。</p></li><li><p>不要给别人留下能够批评你的漏洞：能做的实验别漏做，比如参数的对比实验；以及各类符号的说明以及解释，不要给审稿人任何可趁之机。</p></li></ol><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>感谢您阅读到这里，如果觉得有什么问题，欢迎随时与我交流~</p><p>改论文去了，还有 MaskGAN 的代码要看！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;人生中第一篇论文总算接近完成，&lt;a href=&quot;http://web.xidian.edu.cn/ysxu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;徐老师&lt;/a&gt;真的超级 nice，几乎是逐字逐句在帮忙修改论文，并且传授了很多论文写作的经验和技巧。
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>MaskGAN 学习笔记</title>
    <link href="https://tobiaslee.top/2018/04/01/MaskGAN-Notes/"/>
    <id>https://tobiaslee.top/2018/04/01/MaskGAN-Notes/</id>
    <published>2018-04-01T08:02:00.000Z</published>
    <updated>2018-04-23T12:40:07.584Z</updated>
    
    <content type="html"><![CDATA[<p>MaskGAN 是 Goodfellow 组的新作，已经被 ICLR 2018 接收，标题很是风骚，<a href="https://openreview.net/pdf?id=ByOExmWAb" target="_blank" rel="noopener">MaskGAN: Better Text Generation via Filling in the  <strong><strong>____</strong></strong></a>，这个下划线的操作真是… astounding。代码也已<a href="https://github.com/tensorflow/models/tree/master/research/maskgan" target="_blank" rel="noopener">开源</a>。这篇文章依旧是熟悉的套路，从模型 + 代码来解读论文，走起！</p><h2 id="SeqGAN-To-MaskGAN"><a href="#SeqGAN-To-MaskGAN" class="headerlink" title="SeqGAN To MaskGAN"></a>SeqGAN To MaskGAN</h2><h3 id="SeqGAN-的缺点"><a href="#SeqGAN-的缺点" class="headerlink" title="SeqGAN 的缺点"></a>SeqGAN 的缺点</h3><p>上一篇讲 SeqGAN 的时候我们提到，SeqGAN 开创了 GAN 在 Text Generation 的先河，但是，实验结果证明，其 Idea 是能 Work（通过强化学习解决 GAN 无法在离散文本上梯度回传），合成数据中的 loss 确实有下降，但是在真实的古诗数据集上，其<strong>生成的文本质量不如人意</strong>。我利用全唐诗做了实验，不过囿于设备和时间原因，并没有充分的训练和调优，摘录部分生成结果如下：</p><blockquote><p>霞畅拍起妇 已煦肃兢恼 鶋仝棚愕迷 啼肃次念云 </p><p>岂阳孤任帐 因伊牧掩牢 人原马槎问 弥章斗天钓 </p><p>鸡行肩始昏 晨刺重云千 指瘼山月堂 一似蕃德率 </p><p>有足偶有欲 威飏欢浩潋 戏鸟靓簪粘 性负觉狄至 </p></blockquote><p>有没有一种狗屁不通的感觉… 反正我是很绝望。</p><p>也就是说 SeqGAN 效果不是很好（也有实验室做过实验，其中生成质量较好的古诗基本都是训练集中的），而 MaskGAN 可能为提升生成文本的质量指出了一个方向，其和 SeqGAN 有两点主要的区别：</p><ol><li>增加额外的 Information，Masked Sequence $m(x)$，这也导致了其使用的模型架构变成 Seq2Seq，而非 SeqGAN 中 LSTM（Generator）和 CNN（Discriminator）</li><li>使用 Actor-Critic 来进行强化学习，而非 SeqGAN 中的 Policy Gradient + Monte Carlo </li></ol><p>接下来我们就从这两点不同入手，来讲解 MaskGAN。</p><h3 id="Masked-Token"><a href="#Masked-Token" class="headerlink" title="Masked Token"></a>Masked Token</h3><p>MaskGAN 在文中指出了 GAN 的两个问题，一是 Mode Collapse，即可能出现少数的生成样本种类占据了整个生成集，缺乏多样性；二是训练不稳定，GAN 难调试是出了名的。文章解决这两个问题的方案是：不再让生成器来生成的完整的文本，而是做“完形填空”，不过关于为什么能解决，他们是这么说的：</p><blockquote><p> We believe the in-filling may mitigate the problem of severe mode-collapse. </p></blockquote><p>一个<code>believe</code> 再来 <code>may</code> 加上一个 <code>mitigate</code>，这就是论文的表述的艺术啊。解决训练不稳定的方法呢就是从 Policy Gradient 转换为 Actor-Critic，后面再说。</p><p>“完形填空”相信大家高中都做过，就是把文章挖空然后让你选一个正确的单词填进去，MaskGAN 就是这么干的，对于一个输入序列 $x = （x_1,…, x_T)$，经过一个 mask： $m=(m_1,…m_T)$，其中 $m_i$ 的取值为 0 或者 1，0 就代表挖掉，1 就意味着保留。经过挖空的操作之后呢，我们就得到了 Masked Token $m(x)$，并将它交给我们以 Seq2Seq 为架构的 Generator 来进行生成，模型见下： </p><p><img src="/img/maskgan_model.png" alt="MaskGAN"></p><p>需要注意一点就是：生成的 token 不一定会作为下一个生成的 pre-token，而是取决于是否被挖空，如有原；这也是一个重要的细节，因为一个错误的答案可能会导致一整篇文章都是错误的，所以，如果有参考答案还是用参考答案。</p><a id="more"></a><p>Discriminator 的架构也是采用的 Seq2Seq，只不过是 many2one，即最后生成的每个 token 为真的概率。除了有填好的句子做为输入以外，$m(x)$ 也作为 Discriminator 的输入，文章是这么解释这么做的原因的：对于一个生成的句子 <code>the director director guided the series</code> ，如果没有 $m(x)$ 的话，那么判别器无法分别到底前一个 <code>director</code> 是原文呢还是后一个是，因为句子有可能是 <code>the *associate* director guided the series</code> 或者是 <code>the director *expertly* guided the series</code>，因此是有必要给判别器关于原文的信息，从而做出更好的判断。生成器和判别器的公式如下：</p><p><img src="/img/mask_gen.png" alt="MaskGAN Generator"></p><p><img src="/img/mask_dis.png" alt="MaskGAN Discriminator"></p><h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>前面的文章谈到了，AC 的做法相比 Policy Gradient，很大的区别就在是<strong>单步更新</strong>，以及用一个 NN 来拟合 Advantage Function 来指导生成器生成更加逼真的文本。MaskGAN 的单步reward $r_t$ 设置为了 log probablity，也就是：</p><p>$$ r_t = log D_\phi(\hat{x_t}|\hat{x}_{0:T}, \ \textbf{m(x)})$$</p><p>总的 reward $R_t​$ 则为这一时刻到句子结束 T 时之和：</p><p>$$R_t = \sum_{s=t}^T \gamma ^s r_s$$</p><p>我们通过减去一个 critic 产生的 baseline $b_t$ 来降低 variance，更新梯度的计算就变成：</p><p><img src="/img/ac_gradient.png" alt="Gradient"></p><p>其中 $b_t$ 由一个 NN 来拟合，MaskGAN 选择使用  Discriminator 的前半部分来估计 $b_t$，详细说明需要结合代码来进行。</p><h2 id="Code-Matters"><a href="#Code-Matters" class="headerlink" title="Code Matters"></a>Code Matters</h2><p>代码永远是一个很好的学习材料，也是检验论文到底是不是糊弄人的试金石。</p><h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p>从代码里我们可以看到，作者实现了很多种 Generator 的架构，有 CNN、RNN 和 Seq2Seq，所以 Seq2Seq 应该是经过对比之后选出来效果比较好的一种。</p><p>先来看 Encoder 部分，其作用是把 Masked Token 交给一个 LSTM：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_encoder</span><span class="params">(hparams, inputs, targets_present, is_training, reuse=None)</span>:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># We will use the same variable from the decoder: get word embedding matrix</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'encoder'</span>, reuse=reuse):</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">()</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell() <span class="comment">#... </span></span><br><span class="line"></span><br><span class="line">    attn_cell = lstm_cell</span><br><span class="line">    <span class="keyword">if</span> is_training <span class="keyword">and</span> hparams.gen_vd_keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">attn_cell</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="comment"># .. Add variational dropout on the cell</span></span><br><span class="line"></span><br><span class="line">    cell = tf.contrib.rnn.MultiRNNCell(</span><br><span class="line">        [attn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(hparams.gen_num_layers)],</span><br><span class="line">        state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    initial_state = cell.zero_state(FLAGS.batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行 Mask 操作</span></span><br><span class="line">    real_inputs = inputs</span><br><span class="line">    masked_inputs = transform_input_with_is_missing_token(</span><br><span class="line">        inputs, targets_present)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">      hidden_states = []</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Split the embedding into two parts so that we can load the PTB</span></span><br><span class="line">      <span class="comment"># weights into one part of the Variable.</span></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.seq2seq_share_embedding:</span><br><span class="line">        embedding = tf.get_variable(<span class="string">'embedding'</span>,</span><br><span class="line">                                    [FLAGS.vocab_size, hparams.gen_rnn_size])</span><br><span class="line">      missing_embedding = tf.get_variable(<span class="string">'missing_embedding'</span>,</span><br><span class="line">                                          [<span class="number">1</span>, hparams.gen_rnn_size])</span><br><span class="line">      embedding = tf.concat([embedding, missing_embedding], axis=<span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">      real_rnn_inputs = tf.nn.embedding_lookup(embedding, real_inputs)</span><br><span class="line">      masked_rnn_inputs = tf.nn.embedding_lookup(embedding, masked_inputs)</span><br><span class="line"></span><br><span class="line">      state = initial_state</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">make_mask</span><span class="params">(keep_prob, units)</span>:</span></span><br><span class="line">        random_tensor = keep_prob</span><br><span class="line">        <span class="comment"># 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)</span></span><br><span class="line">        random_tensor += tf.random_uniform(</span><br><span class="line">            tf.stack([FLAGS.batch_size, <span class="number">1</span>, units]))</span><br><span class="line">        <span class="keyword">return</span> tf.floor(random_tensor) / keep_prob</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> is_training:</span><br><span class="line">        output_mask = make_mask(hparams.gen_vd_keep_prob, hparams.gen_rnn_size)</span><br><span class="line"></span><br><span class="line">      hidden_states, state = tf.nn.dynamic_rnn(</span><br><span class="line">          cell, masked_rnn_inputs, initial_state=state, scope=scope)</span><br><span class="line">      <span class="keyword">if</span> is_training:</span><br><span class="line">        hidden_states *= output_mask</span><br><span class="line"></span><br><span class="line">      final_masked_state = state</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 在未 mask 的输入上再来一次 encode 操作</span></span><br><span class="line">      real_state = initial_state</span><br><span class="line">      _, real_state = tf.nn.dynamic_rnn(</span><br><span class="line">          cell, real_rnn_inputs, initial_state=real_state, scope=scope)</span><br><span class="line">      final_state = real_state</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (hidden_states, final_masked_state), initial_state, final_state</span><br></pre></td></tr></table></figure><p>思路是这样的：输入的 Inputs，根据 targets_present（一个 bool 向量指示是否 mask）进行 mask 操作，然后丢进 RNN 里面，得到最后的 state 作为输出。</p><p>但这个代码里看到了几个 tricks：</p><ol><li>在 RNN Cell 外再包了一层 Variational Dropout，每个 Unit 的 Dropout Rate 也是随机产生的，而不再是定值。推测是想要加强 Regularization 的作用，学到了。</li><li>在 Masked 的 Inputs 上进行一次 Encode 得到一个 final_masked_state 后，又在 Origin Input 上做了一次 Encode 得到 final_state，还不知道是干嘛用的，稍后再看。</li></ol><p>接下来是 Decoder 部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_decoder</span><span class="params">(hparams,</span></span></span><br><span class="line"><span class="function"><span class="params">                inputs,</span></span></span><br><span class="line"><span class="function"><span class="params">                targets,</span></span></span><br><span class="line"><span class="function"><span class="params">                targets_present,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoding_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_validating,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse=None)</span>:</span></span><br><span class="line">  <span class="string">"""Define the Decoder graph. The Decoder will now impute tokens that</span></span><br><span class="line"><span class="string">      have been masked from the input seqeunce.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  gen_decoder_rnn_size = hparams.gen_rnn_size</span><br><span class="line"></span><br><span class="line">  targets = tf.Print(targets, [targets], message=<span class="string">'targets'</span>, summarize=<span class="number">50</span>)</span><br><span class="line">  <span class="keyword">if</span> FLAGS.seq2seq_share_embedding:</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'decoder/rnn'</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">      embedding = tf.get_variable(<span class="string">'embedding'</span>,</span><br><span class="line">                                  [FLAGS.vocab_size, hparams.gen_rnn_size])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'decoder'</span>, reuse=reuse):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 准备工作 定义 lstm_cell / attn_cell</span></span><br><span class="line">    <span class="comment"># 获取 hidden_states 和 final_state</span></span><br><span class="line">    hidden_vector_encodings = encoding_state[<span class="number">0</span>]</span><br><span class="line">    state_gen = encoding_state[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add variational Droppout ...</span></span><br><span class="line"><span class="comment"># Generate Tokens</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn'</span>):</span><br><span class="line">      sequence, logits, log_probs = [], [], []</span><br><span class="line">      <span class="comment"># 利用 word embedding matrix 作为 Softmax_W 的 Matrix</span></span><br><span class="line">      softmax_w = tf.matrix_transpose(embedding)</span><br><span class="line">      softmax_b = tf.get_variable(<span class="string">'softmax_b'</span>, [FLAGS.vocab_size])</span><br><span class="line"></span><br><span class="line">      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)</span><br><span class="line">        </span><br><span class="line">      rnn_outs = []</span><br><span class="line"></span><br><span class="line">      fake = <span class="keyword">None</span></span><br><span class="line">      <span class="keyword">for</span> t <span class="keyword">in</span> xrange(FLAGS.sequence_length):</span><br><span class="line">        <span class="keyword">if</span> t &gt; <span class="number">0</span>:</span><br><span class="line">          tf.get_variable_scope().reuse_variables()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Input to the Decoder.</span></span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">          <span class="comment"># Always provide the real input at t = 0.</span></span><br><span class="line">          rnn_inp = rnn_inputs[:, t]</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          real_rnn_inp = rnn_inputs[:, t]</span><br><span class="line">      <span class="comment"># MLE 或者是 validating 时</span></span><br><span class="line">          <span class="keyword">if</span> is_validating <span class="keyword">or</span> FLAGS.gen_training_strategy == <span class="string">'cross_entropy'</span>:</span><br><span class="line">            rnn_inp = real_rnn_inp</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)</span><br><span class="line">            <span class="comment"># 如果有 real token 用 real，没用就用先前生成的 token</span></span><br><span class="line">            rnn_inp = tf.where(targets_present[:, t - <span class="number">1</span>], real_rnn_inp,</span><br><span class="line">                               fake_rnn_inp)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># RNN. run one step</span></span><br><span class="line">        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> FLAGS.attention_option <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">          rnn_out = attention_construct_fn(rnn_out, attention_keys,</span><br><span class="line">                                           attention_values)</span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">          rnn_out *= output_mask</span><br><span class="line"></span><br><span class="line">        rnn_outs.append(rnn_out)</span><br><span class="line">        <span class="keyword">if</span> FLAGS.gen_training_strategy != <span class="string">'cross_entropy'</span>:</span><br><span class="line">          logit = tf.nn.bias_add(tf.matmul(rnn_out, softmax_w), softmax_b)</span><br><span class="line"></span><br><span class="line">          <span class="comment"># Decoder 的输出，如果有 real token 则输出 real token，没有则输出 fake token</span></span><br><span class="line">          real = targets[:, t]</span><br><span class="line"></span><br><span class="line">          categorical = tf.contrib.distributions.Categorical(logits=logit)</span><br><span class="line">          <span class="keyword">if</span> FLAGS.use_gen_mode:</span><br><span class="line">            fake = categorical.mode()</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            fake = categorical.sample() <span class="comment"># sample a token based on the distribution</span></span><br><span class="line">          log_prob = categorical.log_prob(fake)</span><br><span class="line">          output = tf.where(targets_present[:, t], real, fake)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          real = targets[:, t]</span><br><span class="line">          logit = tf.zeros(tf.stack([FLAGS.batch_size, FLAGS.vocab_size]))</span><br><span class="line">          log_prob = tf.zeros(tf.stack([FLAGS.batch_size]))</span><br><span class="line">          output = real</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add to lists.</span></span><br><span class="line">        sequence.append(output)</span><br><span class="line">        log_probs.append(log_prob)</span><br><span class="line">        logits.append(logit)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> FLAGS.gen_training_strategy == <span class="string">'cross_entropy'</span>:</span><br><span class="line">        <span class="comment"># Code for MLE pre-training </span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        logits = tf.stack(logits, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (tf.stack(sequence, axis=<span class="number">1</span>), logits, tf.stack(log_probs, axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>Decoder 的思路也是很直接，就是用 Encoder 传入的 state tuple，进行 token 的生成。有几点需要注意的是：</p><ol><li>和论文中<strong>一样</strong>，如果有 real token，那么 real token 就会作为下一个 token 的 input，而非使用生成的 token</li><li>作者在设计的时候考虑到了使用 MLE 进行预训练的情况，这时候就全部使用 real tokens，并基于此生成一句话</li></ol><h3 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h3><p>文章说 Discriminator 的架构和 Generator 架构是一样的，只是最后输出是一个 scalar:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'dis'</span>, reuse=reuse):</span><br><span class="line">  encoder_states = dis_encoder(</span><br><span class="line">      hparams,</span><br><span class="line">      masked_inputs,</span><br><span class="line">      is_training=is_training,</span><br><span class="line">      reuse=reuse,</span><br><span class="line">      embedding=embedding)</span><br><span class="line">  predictions = dis_decoder(</span><br><span class="line">      hparams,</span><br><span class="line">      sequence,</span><br><span class="line">      encoder_states,</span><br><span class="line">      is_training=is_training,</span><br><span class="line">      reuse=reuse,</span><br><span class="line">      embedding=embedding)</span><br></pre></td></tr></table></figure><p>确实，<code>dis_encoder</code> 部分的代码和 <code>gen_encoder</code> 是一致的，实现也是类似的；</p><p>而 <code>dis_decoder</code> ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn'</span>) <span class="keyword">as</span> vs:</span><br><span class="line">    predictions = []</span><br><span class="line"></span><br><span class="line">    rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> xrange(FLAGS.sequence_length):</span><br><span class="line">      <span class="keyword">if</span> t &gt; <span class="number">0</span>:</span><br><span class="line">        tf.get_variable_scope().reuse_variables()</span><br><span class="line"><span class="comment"># </span></span><br><span class="line">      rnn_in = rnn_inputs[:, t]</span><br><span class="line">      rnn_out, state = cell_dis(rnn_in, state)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Prediction is linear output for Discriminator.</span></span><br><span class="line">      pred = tf.contrib.layers.linear(rnn_out, <span class="number">1</span>, scope=vs)</span><br><span class="line">      predictions.append(pred)</span><br></pre></td></tr></table></figure><p>对于输入 <code>sequence</code> ，进行 embedding 后，拿出里面的每一个 token，交给 RNN，输出一个 probability，没问题！</p><h2 id="Critic"><a href="#Critic" class="headerlink" title="Critic"></a>Critic</h2><p>论文中有提到一嘴，就是说 AC 这个算法是后来审稿人提出意见之后再加的。我一开始还担心代码里没有，但 Google 还是做的很不错的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">critic_seq2seq_vd_derivative</span><span class="params">(hparams, sequence, is_training, reuse=None)</span>:</span></span><br><span class="line"></span><br><span class="line">  sequence = tf.cast(sequence, tf.int32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># parameter setting ...</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># reuse decoder's variables</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(</span><br><span class="line">      <span class="string">'dis/decoder/rnn/multi_rnn_cell'</span>, reuse=<span class="keyword">True</span>) <span class="keyword">as</span> dis_scope:</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">()</span>:</span></span><br><span class="line">      <span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</span><br><span class="line">          hparams.dis_rnn_size,</span><br><span class="line">          forget_bias=<span class="number">0.0</span>,</span><br><span class="line">          state_is_tuple=<span class="keyword">True</span>,</span><br><span class="line">          reuse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    attn_cell = lstm_cell</span><br><span class="line">    <span class="keyword">if</span> is_training <span class="keyword">and</span> hparams.dis_vd_keep_prob &lt; <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">attn_cell</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> variational_dropout.VariationalDropoutWrapper(</span><br><span class="line">            lstm_cell(), FLAGS.batch_size, hparams.dis_rnn_size,</span><br><span class="line">            hparams.dis_vd_keep_prob, hparams.dis_vd_keep_prob)</span><br><span class="line"></span><br><span class="line">    cell_critic = tf.contrib.rnn.MultiRNNCell(</span><br><span class="line">        [attn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(hparams.dis_num_layers)],</span><br><span class="line">        state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">'critic'</span>, reuse=reuse):</span><br><span class="line">    state_dis = cell_critic.zero_state(FLAGS.batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_mask</span><span class="params">(keep_prob, units)</span>:</span></span><br><span class="line">      <span class="comment"># .. </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      output_mask = make_mask(hparams.dis_vd_keep_prob, hparams.dis_rnn_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn'</span>) <span class="keyword">as</span> vs:</span><br><span class="line">      values = []</span><br><span class="line"></span><br><span class="line">      rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> t <span class="keyword">in</span> xrange(FLAGS.sequence_length):</span><br><span class="line">        <span class="keyword">if</span> t &gt; <span class="number">0</span>:</span><br><span class="line">          tf.get_variable_scope().reuse_variables()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">          rnn_in = tf.zeros_like(rnn_inputs[:, <span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          rnn_in = rnn_inputs[:, t - <span class="number">1</span>]</span><br><span class="line">        rnn_out, state_dis = cell_critic(rnn_in, state_dis, scope=dis_scope)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">          rnn_out *= output_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prediction is linear output for Discriminator.</span></span><br><span class="line">        value = tf.contrib.layers.linear(rnn_out, <span class="number">1</span>, scope=vs)</span><br><span class="line"></span><br><span class="line">        values.append(value)</span><br><span class="line">  values = tf.stack(values, axis=<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.squeeze(values, axis=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>和文中所说的 <code>head of discriminator</code> 一致，代码中 Critic 的实现就是前半部分的 Discriminator，并且复用了 Discriminator 的参数，最后输出也就是一个 scalar，每个 token 的奖励 value；</p><h3 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h3><p>我一开始以为公式中的 $r_t$ 是要计算每个 time step 的，但论文中的注释中说：</p><blockquote><p>The REINFORCE objective should only be on the tokens that were missing.  Specifically, the final Generator reward should be based on the Discriminator predictions on missing tokens.<br>The log probaibilities should be only for missing tokens and the baseline should be calculated only on the missing tokens.</p></blockquote><p>也就是说，只在 missing tokens 上计算相应的 reward。这也很简单，对输出的进行一个 mask 就行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generator rewards are log-probabilities.</span></span><br><span class="line">  eps = tf.constant(<span class="number">1e-7</span>, tf.float32)</span><br><span class="line">  dis_predictions = tf.nn.sigmoid(dis_predictions)</span><br><span class="line">  rewards = tf.log(dis_predictions + eps)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Apply only for missing elements.</span></span><br><span class="line">  zeros = tf.zeros_like(present, dtype=tf.float32)</span><br><span class="line">  log_probs = tf.where(present, zeros, log_probs)</span><br><span class="line">  rewards = tf.where(present, zeros, rewards)</span><br></pre></td></tr></table></figure><p>代码中还实现了很多种 baseline，这里我们只看 Critic 作为 baseline 的情况:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> FLAGS.baseline_method == <span class="string">'critic'</span>:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># critic loss，只在 missing tokens 上计算</span></span><br><span class="line">  critic_loss = create_critic_loss(cumulative_rewards, estimated_values,</span><br><span class="line">                                   present)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 通过 estimated_values(critic 产生的结果) 来得到 baselines</span></span><br><span class="line">  baselines = tf.unstack(estimated_values, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">## Calculate the Advantages, A(s,a) = Q(s,a) - \hat&#123;V&#125;(s).</span></span><br><span class="line">  advantages = []</span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(FLAGS.sequence_length):</span><br><span class="line">    log_probability = log_probs_list[t]</span><br><span class="line">    cum_advantage = tf.zeros(shape=[FLAGS.batch_size])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> xrange(t, FLAGS.sequence_length):</span><br><span class="line">      cum_advantage += missing_list[s] * np.power(gamma,</span><br><span class="line">                                                  (s - t)) * rewards_list[s]</span><br><span class="line">    cum_advantage -= baselines[t]</span><br><span class="line">    <span class="comment"># Clip advantages.</span></span><br><span class="line">    cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping,</span><br><span class="line">                                     FLAGS.advantage_clipping)</span><br><span class="line">    advantages.append(missing_list[t] * cum_advantage)</span><br><span class="line">    final_gen_objective += tf.multiply(</span><br><span class="line">        log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))</span><br><span class="line"></span><br><span class="line">  maintain_averages_op = <span class="keyword">None</span></span><br><span class="line">  baselines = tf.stack(baselines, axis=<span class="number">1</span>)</span><br><span class="line">  advantages = tf.stack(advantages, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这段代码就对应上面的 $R_t - b_t$，只是这里的 $b_t$ 是由 Critic 产生的；剩下的几种 $b_t$ 里还有一半 Monte Carlo，一半 Critic 的情况，就不再细说。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>训练的套路呢也是类似的，先让预训练 Generator，再是进入 GAN 的一个对抗训练过程之中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pretraining </span></span><br><span class="line">fwd_cross_entropy_loss = tf.reduce_mean(fwd_cross_entropy_losses)</span><br><span class="line">    gen_pretrain_op = model_optimization.create_gen_pretrain_op(</span><br><span class="line">        hparams, fwd_cross_entropy_loss, global_step)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Generator Training</span></span><br><span class="line">gen_loss = (fwd_RL_loss + inv_RL_loss) / <span class="number">2.</span> <span class="comment"># average of forward and backward</span></span><br><span class="line">[gen_train_op, gen_grads,gen_vars] = model_optimization.create_reinforce_gen_train_op(</span><br><span class="line">         hparams, learning_rate, gen_loss, fwd_averages_op, inv_averages_op,</span><br><span class="line">         global_step)</span><br><span class="line"><span class="comment"># Discriminator</span></span><br><span class="line">dis_train_op, dis_grads, dis_vars = model_optimization.create_dis_train_op(</span><br><span class="line">      hparams, dis_loss, global_step)</span><br></pre></td></tr></table></figure><p>值得一提的是，loss 是 forward 和 backword 的平均值，这点似乎论文中并没有提到，算是作者的一个小心机？2333</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这篇文章存了有一个礼拜才写完，总算是赶完了；代码部分读的还是很粗糙，接下来会继续把几篇 GAN + NLP 的文章好好读一下写笔记，跑个 Demo，然后试着写一篇 Overview 出来看看能不能忽悠住大家（逃</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MaskGAN 是 Goodfellow 组的新作，已经被 ICLR 2018 接收，标题很是风骚，&lt;a href=&quot;https://openreview.net/pdf?id=ByOExmWAb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MaskGAN: Better Text Generation via Filling in the  &lt;strong&gt;&lt;strong&gt;____&lt;/strong&gt;&lt;/strong&gt;&lt;/a&gt;，这个下划线的操作真是… astounding。代码也已&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/maskgan&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;开源&lt;/a&gt;。这篇文章依旧是熟悉的套路，从模型 + 代码来解读论文，走起！&lt;/p&gt;
&lt;h2 id=&quot;SeqGAN-To-MaskGAN&quot;&gt;&lt;a href=&quot;#SeqGAN-To-MaskGAN&quot; class=&quot;headerlink&quot; title=&quot;SeqGAN To MaskGAN&quot;&gt;&lt;/a&gt;SeqGAN To MaskGAN&lt;/h2&gt;&lt;h3 id=&quot;SeqGAN-的缺点&quot;&gt;&lt;a href=&quot;#SeqGAN-的缺点&quot; class=&quot;headerlink&quot; title=&quot;SeqGAN 的缺点&quot;&gt;&lt;/a&gt;SeqGAN 的缺点&lt;/h3&gt;&lt;p&gt;上一篇讲 SeqGAN 的时候我们提到，SeqGAN 开创了 GAN 在 Text Generation 的先河，但是，实验结果证明，其 Idea 是能 Work（通过强化学习解决 GAN 无法在离散文本上梯度回传），合成数据中的 loss 确实有下降，但是在真实的古诗数据集上，其&lt;strong&gt;生成的文本质量不如人意&lt;/strong&gt;。我利用全唐诗做了实验，不过囿于设备和时间原因，并没有充分的训练和调优，摘录部分生成结果如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;霞畅拍起妇 已煦肃兢恼 鶋仝棚愕迷 啼肃次念云 &lt;/p&gt;
&lt;p&gt;岂阳孤任帐 因伊牧掩牢 人原马槎问 弥章斗天钓 &lt;/p&gt;
&lt;p&gt;鸡行肩始昏 晨刺重云千 指瘼山月堂 一似蕃德率 &lt;/p&gt;
&lt;p&gt;有足偶有欲 威飏欢浩潋 戏鸟靓簪粘 性负觉狄至 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有没有一种狗屁不通的感觉… 反正我是很绝望。&lt;/p&gt;
&lt;p&gt;也就是说 SeqGAN 效果不是很好（也有实验室做过实验，其中生成质量较好的古诗基本都是训练集中的），而 MaskGAN 可能为提升生成文本的质量指出了一个方向，其和 SeqGAN 有两点主要的区别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加额外的 Information，Masked Sequence $m(x)$，这也导致了其使用的模型架构变成 Seq2Seq，而非 SeqGAN 中 LSTM（Generator）和 CNN（Discriminator）&lt;/li&gt;
&lt;li&gt;使用 Actor-Critic 来进行强化学习，而非 SeqGAN 中的 Policy Gradient + Monte Carlo &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来我们就从这两点不同入手，来讲解 MaskGAN。&lt;/p&gt;
&lt;h3 id=&quot;Masked-Token&quot;&gt;&lt;a href=&quot;#Masked-Token&quot; class=&quot;headerlink&quot; title=&quot;Masked Token&quot;&gt;&lt;/a&gt;Masked Token&lt;/h3&gt;&lt;p&gt;MaskGAN 在文中指出了 GAN 的两个问题，一是 Mode Collapse，即可能出现少数的生成样本种类占据了整个生成集，缺乏多样性；二是训练不稳定，GAN 难调试是出了名的。文章解决这两个问题的方案是：不再让生成器来生成的完整的文本，而是做“完形填空”，不过关于为什么能解决，他们是这么说的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; We believe the in-filling may mitigate the problem of severe mode-collapse. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一个&lt;code&gt;believe&lt;/code&gt; 再来 &lt;code&gt;may&lt;/code&gt; 加上一个 &lt;code&gt;mitigate&lt;/code&gt;，这就是论文的表述的艺术啊。解决训练不稳定的方法呢就是从 Policy Gradient 转换为 Actor-Critic，后面再说。&lt;/p&gt;
&lt;p&gt;“完形填空”相信大家高中都做过，就是把文章挖空然后让你选一个正确的单词填进去，MaskGAN 就是这么干的，对于一个输入序列 $x = （x_1,…, x_T)$，经过一个 mask： $m=(m_1,…m_T)$，其中 $m_i$ 的取值为 0 或者 1，0 就代表挖掉，1 就意味着保留。经过挖空的操作之后呢，我们就得到了 Masked Token $m(x)$，并将它交给我们以 Seq2Seq 为架构的 Generator 来进行生成，模型见下： &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/maskgan_model.png&quot; alt=&quot;MaskGAN&quot;&gt;&lt;/p&gt;
&lt;p&gt;需要注意一点就是：生成的 token 不一定会作为下一个生成的 pre-token，而是取决于是否被挖空，如有原；这也是一个重要的细节，因为一个错误的答案可能会导致一整篇文章都是错误的，所以，如果有参考答案还是用参考答案。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
      <category term="论文笔记" scheme="https://tobiaslee.top/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>SeqGAN —— GAN + RL +NLP</title>
    <link href="https://tobiaslee.top/2018/03/11/SeqGAN/"/>
    <id>https://tobiaslee.top/2018/03/11/SeqGAN/</id>
    <published>2018-03-11T12:46:33.000Z</published>
    <updated>2018-04-19T15:25:05.842Z</updated>
    
    <content type="html"><![CDATA[<p>SeqGAN 是第一篇用 GAN 在 NLP 上做出一些成果的 model，<a href="http://lantaoyu.com/cv/" target="_blank" rel="noopener">Lantao Yu</a> 的 CV 看的我也是羡慕不已，想着同龄人已经有如此出色的成绩，不由得感觉自己还是太菜了。</p><h2 id="GAN-for-NLP"><a href="#GAN-for-NLP" class="headerlink" title="GAN for NLP"></a>GAN for NLP</h2><p>先前写过一篇文章，<a href="http://tobiaslee.top/2017/12/10/Thoughts-about-adversarial-training-methods-in-NLP/">关于 Adversarial Training 在 NLP 的一些思考</a>，Adversarial Training 算是 GAN 的源头，而 GAN 算是其在生成器领域的一大成就。但这个 GAN 在文本领域一直不怎么 work，先前我也做过用 GAN 中 Discriminator 作为 Classifier，希望对抗训练能够提升其分类准确度，然而并不成功。原因当时也总结了一下，记录在<a href="https://www.zhihu.com/question/68390841/answer/264504804" target="_blank" rel="noopener">这里</a>，一句话概括就是 Generator 太垃圾根本不能忽悠 Discriminator，反而 Discriminator 因为训练轮数少了（和 Generator 交替训练）被拖了后腿。那么问题又回到了 GAN 来生成文本的问题上了，图像和文本的核心区别在于<strong>图像的 Pixel 表示是连续的，而文本是由离散的 token 组成</strong>。因而，Goodfellow 也在 reddit 上说: </p><blockquote><p>You can make slight changes to the synthetic data only if it is based on continuous numbers. If it is based on discrete numbers, there is no way to make a slight change.</p><p>If you output the word “penguin”, you can’t change that to “penguin + .001” on the next step, because there is no such word as “penguin + .001”. You have to go all the way from “penguin” to “ostrich”.</p></blockquote><p>参数的微小改变不能对结果产生影响，或者说影响的方向也不对，这就导致 Discriminator 的梯度回传变得没有意义。再进一步的关于 GAN for NLP 的讨论，建议可以阅读胡杨的<a href="https://zhuanlan.zhihu.com/p/29168803" target="_blank" rel="noopener">文章</a>。前面说的这些都是为了凸显 SeqGAN 这一工作的重要性，毕竟，GAN 在 NLP 上这么难搞，还能搞出来，肯定了不得。确实，<strong>SeqGAN 是了 RL + GAN 用于文本生成的一大创举</strong>，接下来，一睹风采。</p><h2 id="Seq-GAN"><a href="#Seq-GAN" class="headerlink" title="Seq GAN"></a>Seq GAN</h2><p>Seq GAN 的模型很简洁：</p><p><img src="/img/seqgan.png" alt="SeqGAN model"></p><p>沿用 GAN 的架构， Generator 来生成文本和 Discriminator 来判别文本是真实的还是生成的，那么是怎么解决更新 Generator 参数的这个问题的呢？用 Policy Gradient！如果我们把 <strong>Generator 看成是一个 Agent，他在每一 time step 上生成的 word token 作为 action，此前生成的所有 tokens 作为 state</strong>，我们就可以设计一个 reward function 来指导 Generator 生成更真实的句子。因为 Discriminator 输出是真实句子的概率 0-1，直接拿来作为 reward，于是就有了下面的式子:</p><p>$$ Q_{D_\phi}^{G_\theta}=(a = y_{T}, s= Y_{1:T-1} = D_{\phi}(Y_{1:T}))$$</p><p>但是对于生成了一半的句子怎么评估其真实性呢？用蒙特卡洛（蒙特卡洛就是随机采样）搜索在一个 Generator 的拷贝上（为了避免搜索过程引入对梯度的影响）补全句子，然后再交给 Discriminator 评估。于是，完成的 Q function 如下：</p><p><img src="/img/seqreward.png" alt="Q function"></p><p>讲到这里，整个模型的架构已经讲完了，是不是很简单粗暴明了。接下来实作上还有一些小 tricks，文章的算法摘录如下：</p><p><img src="/img/seq_algorithm.png" alt="SeqGAN Algorithm"></p><a id="more"></a><h3 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h3><p>有些经验性的东西能 work 但是说不出个由头，也就成了所谓的玄学，文章用到的一些 tricks 如下：</p><ol><li><p>CNN 做 Discriminator，LSTM 做 Generator：LSTM 做 Generator 没什么话说，Seq2Seq 里面也差不多是这么做的，为什么用 CNN 做 Discriminator 而不是 RNN，我觉得一个很重要的原因是 CNN 比 RNN，其他的就不清楚了。</p></li><li><p>还有就是 Reddit 上一个用户的 trick，也在 SeqGAN 中出现了：</p><blockquote><p><em>a). Train a generator (G0) as normal using max-likelihood.</em></p><p><em>b). Train your discriminator to discriminate between inputs of this generator (G0) and real data.</em></p><p><em>c). Start with a fresh generator (G1) and use the GAN architecture to train it using the same discriminator.</em></p></blockquote><p>先用 MLE 来训练 G，再用 G 生成的文本和真实文本预训练一下 D，再用全新的 G1 开始 Adversarial Training，可以理解为以后的拳击运动做热身运动吧。不过文章没有用一个全新的 G1，而是在 G 的基础上继续 G vs D 的过程。</p></li><li><p>$G_\beta$ 的参数并不是完全和 $G_\theta$ 保持完全一致，而是稍稍有些滞后更新，这是一个 weight decay 的操作，目的是用于 regularization，后面会详细的说。</p></li></ol><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>SeqGAN 的代码已经<a href="https://github.com/LantaoYu/SeqGAN" target="_blank" rel="noopener">开源</a>，但网上很少有结合代码对照论文讲解的。我向来是奉行 <code>talk is cheap, show me the code</code> 这一朴素的原则，因此，我来尝试着对论文的核心代码做一些小小的解释。</p><h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p>前面说了，Generator 是用一个 LSTM 做的，TensorFlow 本身有着 LSTM cell 的封装，但是不好用，为什么呢？因为 token 是要一步一步生成的，而 LSTM cell 是无法参与 iteration 这一过程的，因此需要手搓一个 LSTM cell 和循环生成 token 的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorArray for storing results</span></span><br><span class="line">gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,dynamic_size=<span class="keyword">False</span>, infer_shape=<span class="keyword">True</span>)</span><br><span class="line">gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length, dynamic_size=<span class="keyword">False</span>, infer_shape=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_g_recurrence</span><span class="params">(i, x_t, h_tm1, gen_o, gen_x)</span>:</span></span><br><span class="line">h_t = self.g_recurrent_unit(x_t, h_tm1)  <span class="comment"># hidden_memory_tuple</span></span><br><span class="line">o_t = self.g_output_unit(h_t)  <span class="comment"># batch x vocab , xw + b logits not prob</span></span><br><span class="line">log_prob = tf.log(tf.nn.softmax(o_t))</span><br><span class="line"> <span class="comment"># 根据 log_prob 采样下一个 word token</span></span><br><span class="line">next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, <span class="number">1</span>), [self.batch_size]), tf.int32)</span><br><span class="line"><span class="comment"># map next token to word embedding</span></span><br><span class="line">x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  <span class="comment"># batch x emb_dim</span></span><br><span class="line"><span class="comment"># save prob of the select token</span></span><br><span class="line">gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.num_emb, <span class="number">1.0</span>, <span class="number">0.0</span>),                                                   tf.nn.softmax(o_t)), <span class="number">1</span>))  <span class="comment"># [batch_size] , prob</span></span><br><span class="line">    gen_x = gen_x.write(i, next_token)  <span class="comment"># indices, batch_size , save token generated</span></span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>, x_tp1, h_t, gen_o, gen_x</span><br><span class="line"><span class="comment"># while loop for generating tokens by time step</span></span><br><span class="line">_, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(</span><br><span class="line">            cond=<span class="keyword">lambda</span> i, _1, _2, _3, _4: i &lt; self.sequence_length,</span><br><span class="line">            body=_g_recurrence,</span><br><span class="line">            loop_vars=(tf.constant(<span class="number">0</span>, dtype=tf.int32),</span><br><span class="line">                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x))</span><br><span class="line">self.gen_x = self.gen_x.stack()  <span class="comment"># seq_length x batch_size</span></span><br><span class="line">self.gen_x = tf.transpose(self.gen_x, perm=[<span class="number">1</span>, <span class="number">0</span>])  <span class="comment"># batch_size x seq_length</span></span><br></pre></td></tr></table></figure><p>LSTM 的代码就不放了。这里用 TensorArray 来做结果的保存让我学到了，以前我都一直不知道用什么 TensorFlow 类型能不断的往里面写和读（Variable、Constant不能写，placeholder 要你传进去）。</p><p>至于预训练阶段，我们的生成是要基于真实的 token 的，所以代码有所区别:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># supervised loss</span></span><br><span class="line">g_predictions = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,dynamic_size=<span class="keyword">False</span>, infer_shape=<span class="keyword">True</span>)</span><br><span class="line">ta_emb_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length)</span><br><span class="line"><span class="comment"># embedded x : seq * batch_size *  emb_size</span></span><br><span class="line">ta_emb_x = ta_emb_x.unstack(self.processed_x) </span><br><span class="line"></span><br><span class="line"><span class="comment"># using the same lstm cell to generate tokens</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_pretrain_recurrence</span><span class="params">(i, x_t, h_tm1, g_predictions)</span>:</span></span><br><span class="line">h_t = self.g_recurrent_unit(x_t, h_tm1)</span><br><span class="line">o_t = self.g_output_unit(h_t)</span><br><span class="line">    <span class="comment"># batch x vocab_size</span></span><br><span class="line">g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  </span><br><span class="line"> <span class="comment"># using the real token to generate next token</span></span><br><span class="line">x_tp1 = ta_emb_x.read(i)</span><br><span class="line"><span class="keyword">return</span> i + <span class="number">1</span>, x_tp1, h_t, g_predictions</span><br><span class="line"><span class="comment"># while loop for supervised task </span></span><br><span class="line">_, _, _, self.g_predictions = control_flow_ops.while_loop(</span><br><span class="line">            cond=<span class="keyword">lambda</span> i, _1, _2, _3: i &lt; self.sequence_length,</span><br><span class="line">            body=_pretrain_recurrence,</span><br><span class="line">            loop_vars=(tf.constant(<span class="number">0</span>, dtype=tf.int32),</span><br><span class="line">                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token),</span><br><span class="line">                       self.h0,</span><br><span class="line">                       g_predictions)</span><br><span class="line">        )</span><br><span class="line">self.g_predictions = tf.transpose(self.g_predictions.stack(),</span><br><span class="line">                                          perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])  <span class="comment"># batch_size x seq_length x </span></span><br><span class="line"><span class="comment"># pretrain loss，基于 MLE 的 loss ， log prob * one_hot label</span></span><br><span class="line">self.pretrain_loss = -tf.reduce_sum(</span><br><span class="line">            tf.one_hot(tf.to_int32(tf.reshape(self.x, [<span class="number">-1</span>])), self.num_emb, <span class="number">1.0</span>, <span class="number">0.0</span>) *</span><br><span class="line">            tf.log(tf.clip_by_value(tf.reshape(self.g_predictions, [<span class="number">-1</span>, self.num_emb]), <span class="number">1e-20</span>, <span class="number">1.0</span>))</span><br><span class="line">        ) / (self.sequence_length * self.batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training update </span></span><br><span class="line">pretrain_opt = self.g_optimizer(self.learning_rate)</span><br><span class="line">self.pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(self.pretrain_loss, self.g_params), self.grad_clip)</span><br><span class="line">self.pretrain_updates = pretrain_opt.apply_gradients(zip(self.pretrain_grad, self.g_params))</span><br></pre></td></tr></table></figure><p>手动 optimize varibales 虽然我也不是第一次见到了，一般我都很懒的 <code>tf.train.AdamOptimizer().minimize(loss)</code>，但我查了一下，说是<strong>梯度裁剪<code>gradient clip</code> 确实很多情况下比不裁剪要好</strong>，以后也就尽量这么干吧。</p><p>最后来看一眼 Generator 的 loss :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generator loss</span></span><br><span class="line">self.g_loss = -tf.reduce_sum(</span><br><span class="line">tf.reduce_sum(</span><br><span class="line">        tf.one_hot(tf.to_int32(tf.reshape(self.x, [<span class="number">-1</span>])), self.num_emb, <span class="number">1.0</span>, <span class="number">0.0</span>) *</span><br><span class="line">tf.log(tf.clip_by_value(tf.reshape(self.g_predictions, [<span class="number">-1</span>, self.num_emb]), <span class="number">1e-20</span>, <span class="number">1.0</span>)),</span><br><span class="line">axis=<span class="number">1</span>) * tf.reshape(self.rewards, [<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure><p>就是我们上篇 blog 讲的，在 MLE loss 基础上，通过 reward 加权求和得到。</p><h3 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h3><p>文章用 CNN 做 Discriminator，实际上就是一个分类器，输出 [0, 1] 则为真实文本，[1, 0] 即为生成本文。CNN 做 Classifier 在图像领域我是见的很多，做文本听说过但还真是第一次见：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a convolution + maxpool layer for each filter size</span></span><br><span class="line"><span class="keyword">for</span> filter_size, filter_num <span class="keyword">in</span> zip(filter_sizes, num_filters):</span><br><span class="line">    <span class="comment"># Create a convolution + maxpool layer for each filter size</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"cov2d-maxpool%s"</span> % filter_size):</span><br><span class="line">        filter_shape = [filter_size, embedding_size, <span class="number">1</span>, filter_num]</span><br><span class="line">        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</span><br><span class="line">        b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[filter_num]), name=<span class="string">"b"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(</span><br><span class="line">            self.embedded_chars_expanded,</span><br><span class="line">            W,</span><br><span class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            padding=<span class="string">"VALID"</span>,</span><br><span class="line">            name=<span class="string">"conv"</span>)</span><br><span class="line">        <span class="comment"># filter_num</span></span><br><span class="line">        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=<span class="string">"relu"</span>)</span><br><span class="line">        pooled = tf.nn.max_pool(</span><br><span class="line">            h,</span><br><span class="line">            ksize=[<span class="number">1</span>, sequence_length - filter_size + <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            padding=<span class="string">'VALID'</span>,</span><br><span class="line">            name=<span class="string">"pool"</span>)  <span class="comment"># 全部池化到 1x1</span></span><br><span class="line">        <span class="comment"># print(conv.name, ": ", conv.shape , "----", pooled.name, " : " ,pooled.shape)</span></span><br><span class="line">        pooled_outputs.append(pooled)</span><br><span class="line">total_filters_num = sum(num_filters)</span><br><span class="line"><span class="comment"># 拼接卷积结果</span></span><br><span class="line">self.h_pool = tf.concat(pooled_outputs, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># batch * total_num</span></span><br><span class="line">self.h_pool_flat = tf.reshape(self.h_pool, [<span class="number">-1</span>, total_filters_num]) </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"output"</span>):</span><br><span class="line">    W = tf.Variable(tf.truncated_normal([total_filters_num, num_classes], stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_classes]), name=<span class="string">"b"</span>)</span><br><span class="line">    l2_loss += tf.nn.l2_loss(W)</span><br><span class="line">    l2_loss += tf.nn.l2_loss(b)</span><br><span class="line">    self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=<span class="string">"scores"</span>)</span><br><span class="line">    self.ypred_for_auc = tf.nn.softmax(self.scores)</span><br><span class="line">    self.predictions = tf.argmax(self.scores, <span class="number">1</span>, name=<span class="string">"predictions"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</span><br><span class="line">    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)</span><br><span class="line">    self.loss = losses + l2_reg_lambda * l2_loss</span><br></pre></td></tr></table></figure><p>把 word embedding 之后的 <code>[batch_size, seq_len, embedding_size]</code> 的向量扩充一个维度(对应 RGB channel)变成<code>[batch_size, seq_len, embedding_size, 1]</code> 我们就可以把文本看成一张图片了，然后使用不同 size 的 kernel，主要是 [1, filter_size, embedding_size, 1]，因为第一维是 batch_size 一般不做卷积操作，第二维是长度，我们会用不同长度的 filter_size 来 capture 不同长度下的特征，然后最后池化到 1x1，再把不同 kernel 卷积的结果拼起来，最后来一层全连接层输出到 num_class 搞定。</p><p>同样这里使用了手动 optimize：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.params = [param <span class="keyword">for</span> param <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> <span class="string">'discriminator'</span> <span class="keyword">in</span> param.name]</span><br><span class="line">        d_optimizer = tf.train.AdamOptimizer(<span class="number">1e-4</span>)</span><br><span class="line">        <span class="comment"># aggregation_method =2 似乎能够帮助减少内存占用</span></span><br><span class="line">        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=<span class="number">2</span>)</span><br><span class="line">        self.train_op = d_optimizer.apply_gradients(grads_and_vars)</span><br></pre></td></tr></table></figure><h3 id="G-beta"><a href="#G-beta" class="headerlink" title="G_beta"></a>G_beta</h3><p>$G_\beta$ 值得好好谈一谈，首先是要实现的功能：MC 产生完整的句子以及给出 reward。</p><p>先来看 MC 产生完整句子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  Unstack the values of a `Tensor` to the TensorArray</span></span><br><span class="line">ta_emb_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length)</span><br><span class="line">ta_emb_x = ta_emb_x.unstack(self.processed_x)  <span class="comment"># seq * emb_size</span></span><br><span class="line"></span><br><span class="line">ta_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length)</span><br><span class="line">ta_x = ta_x.unstack(tf.transpose(self.x, perm=[<span class="number">1</span>, <span class="number">0</span>]))  <span class="comment"># seq * batch</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># start state </span></span><br><span class="line">self.h0 = tf.zeros([self.batch_size, self.hidden_dim])</span><br><span class="line">self.h0 = tf.stack([self.h0, self.h0])</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorArray for storing the generated tokens</span></span><br><span class="line">gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># while i &lt; given num , using the provided tokens as input</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_g_recurrence_1</span><span class="params">(i, x_t, h_tm1, given_num, gen_x)</span>:</span></span><br><span class="line">    h_t = self.g_recurrent_unit(x_t, h_tm1)</span><br><span class="line">    x_tp1 = ta_emb_x.read(i)</span><br><span class="line">    gen_x = gen_x.write(i, ta_x.read(i))</span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>, x_tp1, h_t, given_num, gen_x</span><br><span class="line"><span class="comment"># i &gt; given num, generate token using tokens generated before</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_g_recurrence_2</span><span class="params">(i, x_t, h_tm1, gen_x)</span>:</span></span><br><span class="line">    h_t = self.g_recurrent_unit(x_t, h_tm1)</span><br><span class="line">    o_t = self.g_output_unit(h_t)  <span class="comment"># logits  : batch x vocab</span></span><br><span class="line">    log_prob = tf.log(tf.nn.softmax(o_t))  <span class="comment"># log prob</span></span><br><span class="line">    next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, <span class="number">1</span>), [self.batch_size]), tf.int32)</span><br><span class="line">    x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)</span><br><span class="line">    gen_x = gen_x.write(i, next_token)</span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>, x_tp1, h_t, gen_x</span><br><span class="line"></span><br><span class="line">i, x_t, h_tm1, given_num, self.gen_x = control_flow_ops.while_loop(</span><br><span class="line">    cond=<span class="keyword">lambda</span> i, _1, _2, given_num, _4: i &lt; given_num,</span><br><span class="line">    body=_g_recurrence_1,</span><br><span class="line">    loop_vars=(</span><br><span class="line">        tf.constant(<span class="number">0</span>, dtype=tf.int32),</span><br><span class="line">        tf.nn.embedding_lookup(self.g_embeddings, self.start_token),</span><br><span class="line">        self.h0, self.given_num, gen_x</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"><span class="comment"># generate the complete sentence</span></span><br><span class="line">_, _, _, self.gen_x = control_flow_ops.while_loop(</span><br><span class="line">    cond=<span class="keyword">lambda</span> i, _1, _2, _3: i &lt; self.sequence_length,</span><br><span class="line">    body=_g_recurrence_2,</span><br><span class="line">    loop_vars=(i, x_t, h_tm1, self.gen_x)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里有两个循环，是针对两种情况，一个是已经生成的部分，直接用已经给出的 tokens 作为输入，但是考虑到后面继续生成未完成的部分我们还需要最后一个给定 tokens 所产生的 hidden_state，所以我们依旧进行 time step 的循环；到了没有 tokens 的时候，根据最后一个 hidden_state 继续生成完整的句子，就和 Generator 的代码类似了。</p><p>给出 reward，就是基于已经补充完整的句子，通过 Discriminator 计算他的真实性，用真实性概率作为 reward：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_reward</span><span class="params">(self, sess, input_x, rollout_num, discriminator)</span>:</span></span><br><span class="line">    rewards = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rollout_num):</span><br><span class="line">        <span class="keyword">for</span> given_num <span class="keyword">in</span> range(<span class="number">1</span>, self.sequence_length):</span><br><span class="line">            feed = &#123;self.x: input_x, self.given_num: given_num&#125;</span><br><span class="line">            samples = sess.run(self.gen_x, feed)</span><br><span class="line">            feed = &#123;discriminator.input_x: samples, discriminator.dropout_keep_prob: <span class="number">1.0</span>&#125;</span><br><span class="line">            ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)</span><br><span class="line">            ypred = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> ypred_for_auc])</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                rewards.append(ypred)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rewards[given_num - <span class="number">1</span>] += ypred</span><br><span class="line"></span><br><span class="line">        <span class="comment"># the last token reward</span></span><br><span class="line">        feed = &#123;discriminator.input_x: input_x, discriminator.dropout_keep_prob: <span class="number">1.0</span>&#125;</span><br><span class="line">        ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)</span><br><span class="line">        ypred = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> ypred_for_auc])</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            rewards.append(ypred)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rewards[self.sequence_length] += ypred</span><br><span class="line"><span class="comment"># batch_size x seq_length</span></span><br><span class="line">    rewards = np.transpose(np.array(rewards)) / (<span class="number">1.0</span> * rollout_num)  </span><br><span class="line">    <span class="keyword">return</span> rewards</span><br></pre></td></tr></table></figure><p>因为我们的 Discriminator 的输出是类似 <code>[0.1, 0.9]</code> 这样的概率，所以句子为真实文本的概率即为 <code>item[1]</code> 的值，我们采样 <code>rollout_num</code> 次，对最后的 reward 取平均得到最终的 reward，这也就是前面 Q function 的实际实现。PS：原版的实现这里直接把 self.sequence_length 代成了 20，为此我还提了一个 PR，不知道 LantaoYu 会不会 merge， 2333.</p><p>然后是其参数更新的手段，由于篇幅，只展示最后输出单元的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_output_unit</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># weight decay</span></span><br><span class="line">    self.Wo = self.update_rate * self.Wo + (<span class="number">1</span> - self.update_rate) * tf.identity(self.lstm.Wo)</span><br><span class="line">    self.bo = self.update_rate * self.bo + (<span class="number">1</span> - self.update_rate) * tf.identity(self.lstm.bo)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unit</span><span class="params">(hidden_memory_tuple)</span>:</span></span><br><span class="line">        hidden_state, c_prev = tf.unstack(hidden_memory_tuple)</span><br><span class="line">        logits = tf.matmul(hidden_state, self.Wo) + self.bo</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">    <span class="keyword">return</span> unit</span><br></pre></td></tr></table></figure><p>文章里说 $G_\beta$ 是和 $G_\theta$ 完全相同的，但这里参数的更新并不是直接拷贝，而是根据 update rate 来计算需要更新的部分，也可以理解，因为 SGD 训练的更新必然是 variance 的，而限制每次更新的幅度能够起到 regularizaiton 的作用。</p><h3 id="Training-Process"><a href="#Training-Process" class="headerlink" title="Training Process"></a>Training Process</h3><p>训练的过程就按照算法描述的部分，这边就摘录一些核心的片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># avoid occupy all the memory of the GPU</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>这是我以前一直不知道的用法，因为我没有 GPU（贫穷限制了我的想象力），因为 TensorFlow 默认会占完所有显存，所以如果和别人共用机子，写上这样的配置可以按需分配显存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> total_batch <span class="keyword">in</span> range(TOTAL_BATCH):</span><br><span class="line">    <span class="comment"># train generator once</span></span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line">        samples = G.generate(sess)</span><br><span class="line">        rewards = g_beta.get_reward(sess, samples, sample_time, D)</span><br><span class="line">        feed = &#123;G.x: samples, G.rewards: rewards&#125;</span><br><span class="line">        _ = sess.run(G.g_update, feed_dict=feed)</span><br><span class="line">    g_beta.update_params()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train the discriminator</span></span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        generate_samples(sess, G, BATCH_SIZE, generated_num, negative_file)</span><br><span class="line">        dis_data_loader.load_train_data(positive_file, negative_file)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            dis_data_loader.reset_pointer()</span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> range(dis_data_loader.num_batch):</span><br><span class="line">                x_batch, y_batch = dis_data_loader.next_batch()</span><br><span class="line">                feed = &#123;</span><br><span class="line">                    D.input_x: x_batch,</span><br><span class="line">                    D.input_y: y_batch,</span><br><span class="line">                    D.dropout_keep_prob: dis_dropout_keep_prob</span><br><span class="line">                &#125;</span><br><span class="line">                _ = sess.run(D.train_op, feed_dict=feed)</span><br></pre></td></tr></table></figure><p>核心的 Adversarial Training 的代码就在上面了，在一次迭代中生成一些 fake data，根据 reward 来更新 G 的参数，同时滞后更新 $G_\beta$ 的参数，然后再训练 Discriminator。这里是 Discriminator 训练的次数多于 Generator，这也是 GAN 里面常用的手段了。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>SeqGAN 的代码是用的合成数据做实验，输出只有 loss，无法直观的感受其生产的结果，但实际真实的文本生成还没有对应的开源代码。论文有提到说拿来测试过古诗生成，所以接下来我会尝试用 SeqGAN 做一下古诗生成的任务，看看效果到底怎么样。</p><p>这个代码看了我三天，收获到了很多原本不知道的 TensorFlow 的用法，所以说，代码还是多看多写。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SeqGAN 是第一篇用 GAN 在 NLP 上做出一些成果的 model，&lt;a href=&quot;http://lantaoyu.com/cv/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lantao Yu&lt;/a&gt; 的 CV 看的我也是羡慕不已，想着同龄人已经有如此出色的成绩，不由得感觉自己还是太菜了。&lt;/p&gt;
&lt;h2 id=&quot;GAN-for-NLP&quot;&gt;&lt;a href=&quot;#GAN-for-NLP&quot; class=&quot;headerlink&quot; title=&quot;GAN for NLP&quot;&gt;&lt;/a&gt;GAN for NLP&lt;/h2&gt;&lt;p&gt;先前写过一篇文章，&lt;a href=&quot;http://tobiaslee.top/2017/12/10/Thoughts-about-adversarial-training-methods-in-NLP/&quot;&gt;关于 Adversarial Training 在 NLP 的一些思考&lt;/a&gt;，Adversarial Training 算是 GAN 的源头，而 GAN 算是其在生成器领域的一大成就。但这个 GAN 在文本领域一直不怎么 work，先前我也做过用 GAN 中 Discriminator 作为 Classifier，希望对抗训练能够提升其分类准确度，然而并不成功。原因当时也总结了一下，记录在&lt;a href=&quot;https://www.zhihu.com/question/68390841/answer/264504804&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;，一句话概括就是 Generator 太垃圾根本不能忽悠 Discriminator，反而 Discriminator 因为训练轮数少了（和 Generator 交替训练）被拖了后腿。那么问题又回到了 GAN 来生成文本的问题上了，图像和文本的核心区别在于&lt;strong&gt;图像的 Pixel 表示是连续的，而文本是由离散的 token 组成&lt;/strong&gt;。因而，Goodfellow 也在 reddit 上说: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can make slight changes to the synthetic data only if it is based on continuous numbers. If it is based on discrete numbers, there is no way to make a slight change.&lt;/p&gt;
&lt;p&gt;If you output the word “penguin”, you can’t change that to “penguin + .001” on the next step, because there is no such word as “penguin + .001”. You have to go all the way from “penguin” to “ostrich”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;参数的微小改变不能对结果产生影响，或者说影响的方向也不对，这就导致 Discriminator 的梯度回传变得没有意义。再进一步的关于 GAN for NLP 的讨论，建议可以阅读胡杨的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/29168803&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文章&lt;/a&gt;。前面说的这些都是为了凸显 SeqGAN 这一工作的重要性，毕竟，GAN 在 NLP 上这么难搞，还能搞出来，肯定了不得。确实，&lt;strong&gt;SeqGAN 是了 RL + GAN 用于文本生成的一大创举&lt;/strong&gt;，接下来，一睹风采。&lt;/p&gt;
&lt;h2 id=&quot;Seq-GAN&quot;&gt;&lt;a href=&quot;#Seq-GAN&quot; class=&quot;headerlink&quot; title=&quot;Seq GAN&quot;&gt;&lt;/a&gt;Seq GAN&lt;/h2&gt;&lt;p&gt;Seq GAN 的模型很简洁：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/seqgan.png&quot; alt=&quot;SeqGAN model&quot;&gt;&lt;/p&gt;
&lt;p&gt;沿用 GAN 的架构， Generator 来生成文本和 Discriminator 来判别文本是真实的还是生成的，那么是怎么解决更新 Generator 参数的这个问题的呢？用 Policy Gradient！如果我们把 &lt;strong&gt;Generator 看成是一个 Agent，他在每一 time step 上生成的 word token 作为 action，此前生成的所有 tokens 作为 state&lt;/strong&gt;，我们就可以设计一个 reward function 来指导 Generator 生成更真实的句子。因为 Discriminator 输出是真实句子的概率 0-1，直接拿来作为 reward，于是就有了下面的式子:&lt;/p&gt;
&lt;p&gt;$$ Q_{D_\phi}^{G_\theta}=(a = y_{T}, s= Y_{1:T-1} = D_{\phi}(Y_{1:T}))$$&lt;/p&gt;
&lt;p&gt;但是对于生成了一半的句子怎么评估其真实性呢？用蒙特卡洛（蒙特卡洛就是随机采样）搜索在一个 Generator 的拷贝上（为了避免搜索过程引入对梯度的影响）补全句子，然后再交给 Discriminator 评估。于是，完成的 Q function 如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/seqreward.png&quot; alt=&quot;Q function&quot;&gt;&lt;/p&gt;
&lt;p&gt;讲到这里，整个模型的架构已经讲完了，是不是很简单粗暴明了。接下来实作上还有一些小 tricks，文章的算法摘录如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/seq_algorithm.png&quot; alt=&quot;SeqGAN Algorithm&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="https://tobiaslee.top/tags/Reinforcement-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
      <category term="论文笔记" scheme="https://tobiaslee.top/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Actor-Critic - A improvement of Policy Gradient</title>
    <link href="https://tobiaslee.top/2018/03/09/Actor-Critic/"/>
    <id>https://tobiaslee.top/2018/03/09/Actor-Critic/</id>
    <published>2018-03-09T13:59:56.000Z</published>
    <updated>2019-03-09T02:15:22.233Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Improvement-of-Policy-Gradient"><a href="#Improvement-of-Policy-Gradient" class="headerlink" title="Improvement of Policy Gradient"></a>Improvement of Policy Gradient</h2><p>上篇 Blog 中讲到，我们对于 Agent 参数的更新是基于 reward function 对最大似然 loss 加权得到的 Objective Function，这里有两个问题：</p><ol><li>reward function 需要在完整的一次 trajectory 后才能够计算，也就是说，是<strong>回合更新</strong>的。这样更新的频率更新就比较慢，相应地，训练就会缓慢一些。</li><li>因为每次我们都是在当前的一个回合上计算 reward，但每次回合 Agent 所采取的策略都会有所不同，这样子就会导致我们的方差（variance）比较大。</li></ol><p>对于第一个问题，我们的解决方法是，根据每个 Agent 的每次 Action 进行参数的更新，也就是我们接下来要说的 Actor-Critic；第二个问题，也就是我们最好能够对所有的样本进行估计来计算我们的 reward（也就是 reward 的数学期望），以此来降低方差。</p><p>具体地，我们可以采取以下几种 reward function:</p><p><img src="/img/pg2.png" alt="Different Reward Function"></p><p><em>图源自知乎，详见 Reference</em></p><p>第一种也就是最为普通的 reward function，各步的累加；第二种则是不考虑先前步的 reward，这里有一个 casualty 的性质，即今天的参数更新是无法影响昨天以及之前获得的 reward的，因而可以只考虑今天之后的 reward；第三种则是加入了 baseline，在基准之上衡量 reward，能够更好地指示动作的好坏，基准一般会采取所有回合的 reward 均值。值得强调的是，这三种方式都是<strong>对 reward 的无偏估计 (unbiased)</strong>，但天上不会掉馅饼，尽管第二种第三种方式对第一种方式修改后 variance 有所降低，但整体的方差还是比较大的。</p><p>而后面三种，则是希望通过<strong>再用一个 NN，来评估当前的状态能够获得 reward</strong>。这样的估计，必然是 biased 的，因为这个 NN 基本不可能是 Perfect 的；当然，这样做的好处就能够降低 variance，所以这是一个 trade off 的问题，后面三种的详细式子如下：</p><p><img src="/img/value_fit.png" alt="Value Functions"></p><p>Q 是对当前状态 $s_t$和采取的动作$a_t$能够获得的 reward 进行估计；而 V 仅仅考虑对状态 $s_t$ 能获得的 reward，也就是对于所有 $a_t$ 的 Q 的数学期望；A 又称 Advantage Function，是衡量 $a_t$ 好坏的一个指标，从 $ A= Q-V$ 就能看出来。那么问题来了，这三个函数彼此相互关联，我们用 Neural Network(NN) 来拟合哪一个呢？</p><p>答案其实也已经在上面的那种图里了，通过一些约等式，我们能够通过拟合 $V^{\pi}(s)$ 来得到 Advantage Function，实际应用中主要就是采取 Advantage Function 作为对当前所采取动作的评估函数。</p><h2 id="Actor-Critic-Network"><a href="#Actor-Critic-Network" class="headerlink" title="Actor Critic Network"></a>Actor Critic Network</h2><p>上面的 Advantage Function 起到的作用类似一个教练（或者说是批评家），对于我们的 Actor 当前的状态和采取的动作做出指示，帮助他更好的达到目标，这也就是 Actor-Critic 算法的名字由来。我个人对 AC 算法的认识就是基于对 Policy Gradient 的优化而得到的，Actor 就是我们之前谈到的 Policy，其核心就在于<strong>用 NN 来扮演这个 Critic 的角色</strong>。</p><p>既然是 <code>NN fit V function</code>，那么训练集是什么呢？即<strong>我们如何根据当前的状态，来估计整个回合获得的 reward</strong>呢？这里有两种做法：</p><ol><li>Monte Carlo evaluation：蒙特卡洛搜索，就是对未来可能采取的动作进行搜索，计算获得的 reward</li><li>通过 V 本身来计算，获得的 reward 等于当前这一动作能获得的 r 和下一状态用 V 估计值之和，即 $ reward = r(s_t, a_t) + V (s_{t+1})$ </li></ol><p><img src="/img/target.png" alt="Two methods of evaluation of V"></p><p>一般我们都会采用第二种方法，也称为 <code>bootstrapped estimate</code>，由此，我们就可以构造出相应的 training data，用 NN 解决一个 regression 问题：</p><p><img src="/img/critic.png" alt="Regression Problem">   </p><h2 id="Program"><a href="#Program" class="headerlink" title="Program"></a>Program</h2><p>又到了喜闻乐见的代码实现部分，其实也简单，两个 NN 嘛，有点 GAN 的感觉。实际上，有论文将 Actor 和 Critic 两个 NN 整合成一个 NN，并且做出了不错的效果，好处就是能够共享特征(shared features)，不好的地方就是，训练可能不是很稳定。这次就不贴完整代码，上一篇 Blog 和这篇以及后续 RL 相关的代码都会上传到 <a href="https://github.com/TobiasLee/ReinforcementLearningPractice" target="_blank" rel="noopener">GitHub</a> 上，这里只讲解部分核心的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Actor 类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sess, n_features, n_actions, lr=<span class="number">1e-3</span>)</span>:</span> </span><br><span class="line"><span class="comment"># 相应的初始化，创建 placeholder</span></span><br><span class="line"><span class="comment"># 构建全连接层</span></span><br><span class="line"><span class="comment"># 设置 Optimizer 等</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, s, a, td)</span>:</span></span><br><span class="line">     <span class="comment"># 根据 state, action, td error(即 Advantage) 更新参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span> <span class="comment"># choose action based on the state</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Critic 类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sess, n_features, lr=<span class="number">1e-2</span>, gamma=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化 </span></span><br><span class="line">    <span class="comment"># 构建 NN，</span></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"td_square_error"</span>): <span class="comment"># Regression problem </span></span><br><span class="line">            <span class="comment"># 这里我们同样加入了 discount factor</span></span><br><span class="line">            self.td_error = self.r + gamma * self.v_ - self.v</span><br><span class="line">            <span class="comment"># 用 square 作为 loss function</span></span><br><span class="line">            self.loss = tf.square(self.td_error)</span><br><span class="line">        <span class="comment"># 设置 Optimizer </span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"train"</span>):</span><br><span class="line">            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn_state</span><span class="params">(self, s, r, s_)</span>:</span></span><br><span class="line">        <span class="comment"># advantage function A = r + V_t+1 - V_t</span></span><br><span class="line">        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]</span><br><span class="line">        v_ = self.sess.run(self.v, &#123;self.s: s_&#125;) <span class="comment"># v_ : V_t+1</span></span><br><span class="line">        <span class="comment"># 根据 training data 更新 Critic</span></span><br><span class="line">        td_error, _ = self.sess.run(</span><br><span class="line">            [self.td_error, self.train_op],</span><br><span class="line">            &#123;self.s: s,self.r: r,self.v_: v_&#125;)</span><br><span class="line">        <span class="keyword">return</span> td_error</span><br></pre></td></tr></table></figure><p>训练部分，和 Policy Gradient 大同小异：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 创建 Actor Critic 实例</span></span><br><span class="line">actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)</span><br><span class="line">critic = Critic(sess, n_features=N_F, gamma=GAMMA)</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="comment"># 训练 MAX_EPISODE </span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(MAX_EPISODE):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    track_r = list()</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">if</span> RENDER: env.render()</span><br><span class="line">        a = actor.choose_action(s)</span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            r = <span class="number">-20</span></span><br><span class="line">        track_r.append(r)</span><br><span class="line">        <span class="comment"># critic learn td_error</span></span><br><span class="line">        td_error = critic.learn_state(s, r, s_)</span><br><span class="line">        <span class="comment"># actor update behave based on the td_error</span></span><br><span class="line">        actor.learn(s, a, td_error)</span><br><span class="line"></span><br><span class="line">        s = s_ <span class="comment"># update state</span></span><br><span class="line">        t += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="comment"># if done or t &gt;= MAX_EP_STEPS:</span></span><br><span class="line">            ep_rs_sum = sum(track_r)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"running_reward"</span> <span class="keyword">not</span> <span class="keyword">in</span> globals():</span><br><span class="line">                running_reward = ep_rs_sum</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                running_reward = running_reward * <span class="number">0.95</span> + ep_rs_sum*<span class="number">0.05</span></span><br><span class="line">            <span class="keyword">if</span> running_reward &gt; DISPLAY_REWARD_THRESHOLD: <span class="comment"># render the game</span></span><br><span class="line">                RENDER = <span class="keyword">True</span></span><br><span class="line">            print(<span class="string">"episode :"</span>, i_episode, <span class="string">" reward: "</span>, int(running_reward))</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>最后，我们来看一下结果：</p><p><img src="/img/ret.gif" alt="Result"></p><p>一个很有趣的现象就是，第一次玩出界了，第二次能够在快出界的时候纠偏回来，性能确实比简单的 Policy Gradient 好很多。不过，这个训练似乎并不收敛，跑了 200 epoch 之后又会因为一次错误的尝试而导致后面一系列的错误… 果不其然是玄学啊。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/29486661" target="_blank" rel="noopener">知乎- Actor-Critic 算法小结</a></p><p><a href="https://www.youtube.com/watch?v=tWNpiNzWuO8&amp;index=4&amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" target="_blank" rel="noopener">CS294-112 9/6/17</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Improvement-of-Policy-Gradient&quot;&gt;&lt;a href=&quot;#Improvement-of-Policy-Gradient&quot; class=&quot;headerlink&quot; title=&quot;Improvement of Policy Gradient&quot;&gt;
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="https://tobiaslee.top/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Policy Graident 从数学到实现</title>
    <link href="https://tobiaslee.top/2018/03/06/Reinforcement-Learning1/"/>
    <id>https://tobiaslee.top/2018/03/06/Reinforcement-Learning1/</id>
    <published>2018-03-06T14:29:33.000Z</published>
    <updated>2018-03-07T13:59:56.026Z</updated>
    
    <content type="html"><![CDATA[<p>Text Generation 是我非常感兴趣的一个问题，传统的 Seq2Seq 能有不错的效果，但如前一篇 Blog 所说，我相信未来的文本生成必然是与 GAN 以及 Reinforcement Learning （RL）相结合的，上交的大佬 Lantao Yu 就有一篇很出色的 SeqGAN 的工作，以及最近的 MaskGAN。接下来的一段时间，尝试实现 SeqGAN 和 MaskGAN，根据需要学习相关的 RL 知识。这篇 Blog 就算是一个前哨，主要讲简单的 Policy Gradient。</p><h2 id="Brief-Introduction-of-RL"><a href="#Brief-Introduction-of-RL" class="headerlink" title="Brief Introduction of RL"></a>Brief Introduction of RL</h2><p>强化学习是一类机器学习算法的统称，其核心思想是让计算机通过尝试，学会特定行为模式以达到期望目标。前一阵子大火的 AlphaGo 以及各类打游戏的 AI 都是强化学习的产物。强化学习中把计算机程序抽象成一个 Agent（Policy），其所处的环境抽象为 Environment，学习就发生在 Agent 和 Environment 的不断交互之中，示意图如下。</p><p><img src="/img/agent.png" alt="Agent and Environment"></p><p>还有一个重要的概念叫做 state，是关于我们所获知的发生的一切（过去所有的 observation，采取的 action，和获得的 reward） 的函数，在一个观测完全（fully observed）的环境中，我们可以认为 agent 的 state 和环境的 observation 是一样的。事实上，我们需要的 Policy，就是一个 <strong>state -&gt; action 的映射</strong>。而 Deep Reinforcement Learning，就是通过深层 NN 来构建这个映射。</p><p>强化学习算法一般为两类：</p><ol><li>Model - free Approach</li><li>Model- based Approach</li></ol><p>这里的 Model 指的是环境，Model-free 算法不需要对环境进行建模，Policy 通过环境的 reward （奖赏）来指导学习；而 Model-based 类算法则有着对环境的建模，我们可以利用这个 Model 来搜索预判可能发生的情况，并据此为 Policy 选择行动策略。<br><a id="more"></a></p><h2 id="Deep-Policy-Gradient"><a href="#Deep-Policy-Gradient" class="headerlink" title="Deep Policy Gradient"></a>Deep Policy Gradient</h2><p>Policy Gradient 是一类很重要的 RL 算法，其核心步骤如下：</p><p><img src="/img/dpg.png" alt="DPG"></p><p>我们用 <strong>u</strong> 来控制 NN 的参数，并且通过 SGD 来更新这个 <strong>u</strong>，使得能够获得更多 reward  的 action 能够更可能被采纳。因为 reward 是随时间步而累积的，我们通过一个 discount factor $\gamma$ ，来控制 reward 的衰减，避免出现无限的情况。如果我们用 $x$ 来表示一次完整的行动（trajectory），$f(x)$ 来表示 total reward function， $p(x)$ 表示我们的 Policy ，$\theta$ 表示控制 NN 的参数。那么我们的目标就是让 <strong>reward function 的数学期望最大</strong>，SGD 更新，则使数学期望对求导过程如下：</p><p><img src="/img/pg.png" alt="Policy Gradient"></p><p>注意，<strong>$f(x)$ 和 $\theta$ 是无关的 </strong>，也就是说，我们只要求 $log\ p(x)$ 对 $\theta $ 的导数，再乘上 reward function 就可以了。实际实现中，我们肯定是不求数学期望而是采样 N 个 samples 然后求平均，和最大似然的方法相比，发现只需要在 Objective Function（这里的函数称为 loss 有一些不合适） 上乘上一个由 reward 进行加权的函数即可：</p><p><img src="/img/loss.png" alt="Object Function"></p><h2 id="Program"><a href="#Program" class="headerlink" title="Program"></a>Program</h2><p>数学之后，就是编程实现了。OpenAI 提供了一个用于训练强化学习的库 gym，提供了一些接口，非常可以通过传递 action 来得到对应的 state 和 reward，并且里面有很多游戏，可以拿来让我们的 agent 学着玩。这里我们拿 CartPole 游戏做测试，游戏的玩法如下所示：</p><p><img src="/img/game.gif" alt="Game"></p><p>游戏的目标就是保持一根杆子一直竖直向上，杆子会因为重力而摆动。允许摆动范围是 -15° ~ 15°，并且左右移动也有一个限制，不能移过头，否则就是 Game Over。使用 gym 接口的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">"CartPole-v0"</span>)</span><br><span class="line">env.seed(<span class="number">1</span>) <span class="comment"># 设置随机数种子 方便调试</span></span><br><span class="line">env = env.unwrapped</span><br><span class="line"></span><br><span class="line">print(env.action_space) <span class="comment"># 输出 Discrete(2,)</span></span><br><span class="line">print(env.observation_space) <span class="comment"># 输出 Box(4,) </span></span><br><span class="line"></span><br><span class="line">observation, reward, done, info = env.step(action) <span class="comment">#</span></span><br></pre></td></tr></table></figure><p>可以看到，我们的 action 有两个选择：向左；向右； observation 则是一个 4 维向量，猜测是杆子两个端点的坐标。另外，我们可以通过 <code>env.step(action)</code> 来执行一步操作，并且获得对应的返回值。第三个 <code>done</code> 是指示游戏是否 Over 的量，如果为 <code>True</code> 就说明游戏结束了。</p><p>gym 接口的使用就这些，接下来就要搓一个 Agent 来玩游戏了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyGradient</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span><span class="params">(self, s, a, r)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_discount_and_norm_reward</span><span class="params">(self)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self)</span>:</span></span><br></pre></td></tr></table></figure><p>一个一个函数来吧，首先是这个类的构造函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">             n_actions,</span></span></span><br><span class="line"><span class="function"><span class="params">             n_features,</span></span></span><br><span class="line"><span class="function"><span class="params">             lr=<span class="number">0.01</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">             reward_decay=<span class="number">0.95</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">             output_graph=False)</span>:</span></span><br><span class="line">    self.n_actions = n_actions</span><br><span class="line">    self.n_features = n_features</span><br><span class="line">    self.lr = lr</span><br><span class="line">    self.gamma = reward_decay</span><br><span class="line">    self.sess = tf.Session()</span><br><span class="line"></span><br><span class="line">    self.ep_obs, self.ep_as, self.ep_rs = [], [], []</span><br><span class="line">    self._build_net()  <span class="comment"># 构建网络</span></span><br><span class="line">    self.sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>我们传入一些训练的参数，包括 action 的数目，以及 observation 的 feature，对于CartPole，就是 2 和 4。还有就是 discount factor，以及学习率 lr。最后两行，我们构建了一个网络并且对变量进行初始化。</p><p>然后就是最核心却也非常简单的 net:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_build_net</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># input</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</span><br><span class="line">        self.tf_obs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, self.n_features]) <span class="comment">#</span></span><br><span class="line">        self.tf_acts = tf.placeholder(tf.int32, [<span class="keyword">None</span>, ], name=<span class="string">'actions_num'</span>)</span><br><span class="line">        self.tf_vt = tf.placeholder(tf.float32, [<span class="keyword">None</span>, ], name=<span class="string">"action_valuie"</span>)</span><br><span class="line">    <span class="comment"># fc1</span></span><br><span class="line">    layer = tf.layers.dense(inputs=self.tf_obs,</span><br><span class="line">                            units=<span class="number">10</span>,</span><br><span class="line">                            activation=tf.nn.tanh,</span><br><span class="line">                            kernel_initializer=tf.random_normal_initializer(mean=<span class="number">0</span>, stddev=<span class="number">0.3</span>),</span><br><span class="line">                            bias_initializer=tf.constant_initializer(<span class="number">0.1</span>),</span><br><span class="line">                            name = <span class="string">'fc1'</span>)</span><br><span class="line">    <span class="comment"># fc2</span></span><br><span class="line">    all_act = tf.layers.dense(</span><br><span class="line">        inputs=layer,</span><br><span class="line">        units=self.n_actions,</span><br><span class="line">        activation=<span class="keyword">None</span>,</span><br><span class="line">        kernel_initializer=tf.random_normal_initializer(mean=<span class="number">0</span>, stddev=<span class="number">0.3</span>),</span><br><span class="line">        bias_initializer=tf.constant_initializer(<span class="number">0.1</span>),</span><br><span class="line">        name=<span class="string">'fc2'</span>)</span><br><span class="line"></span><br><span class="line">    self.all_act_prob = tf.nn.softmax(all_act, name=<span class="string">'act_prob'</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</span><br><span class="line">        <span class="comment"># to maximize total reward log p * R = minimize - log_p *R</span></span><br><span class="line">        neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts)</span><br><span class="line">        loss = tf.reduce_mean(neg_log_prob * self.tf_vt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"train"</span>):</span><br><span class="line">        self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)</span><br></pre></td></tr></table></figure><p>熟悉神经网络模型的话就知道实质上我们就是用了两层全连接层，最后输出一个 Action 的概率；这里的 loss 就是我们的 Objective Function，先是一个 cross_entropy，然后再通过 <code>tf_vt</code> ，也就是我们的 reward 进行加权求和。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">    prob_weights = self.sess.run(self.all_act_prob, feed_dict=&#123;</span><br><span class="line">        self.tf_obs: observation[np.newaxis, :]</span><br><span class="line">    &#125;)</span><br><span class="line">    action = np.random.choice(range(prob_weights.shape[<span class="number">1</span>]), p=prob_weights.ravel()) <span class="comment"># selction action w.r.t the actions prob</span></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><p>第三个函数，这里和平常的直接 <code>argmax</code> 作为输出不一样，而是根据我们的输出的概率进行 random 选择动作。第四个函数 <code>store_trainsition()</code>，对我们一次游戏中的各个 state 进行保存： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">store_transition</span><span class="params">(self, s, a, r)</span>:</span></span><br><span class="line">    self.ep_as.append(a)</span><br><span class="line">    self.ep_rs.append(r)</span><br><span class="line">    self.ep_obs.append(s)</span><br></pre></td></tr></table></figure><p>然后是计算我们的 reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_discount_and_norm_reward</span><span class="params">(self)</span>:</span> <span class="comment"># reward decay and normalize</span></span><br><span class="line">    discount_ep_rs = np.zeros_like(self.ep_rs)</span><br><span class="line">    running_add = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(<span class="number">0</span>, len(self.ep_rs))):</span><br><span class="line">        running_add = running_add * self.gamma + self.ep_rs[t]</span><br><span class="line">        discount_ep_rs[t] = running_add</span><br><span class="line"></span><br><span class="line">    <span class="comment"># normalize episode rewards</span></span><br><span class="line">    discount_ep_rs -= np.mean(discount_ep_rs)</span><br><span class="line">    discount_ep_rs /= np.std(discount_ep_rs)</span><br><span class="line">    <span class="keyword">return</span> discount_ep_rs</span><br></pre></td></tr></table></figure><p>我们取出一次游戏的每一步的 reward 并且计算经过衰减的 reward，并且做一次 normalize（PS:不做 normalize 操作的话学习的很慢，也反应了 RL 训练其实是很 tricky 的）。</p><p>最后，根据我们的 reward，我们对 agent 的网络进行一次更新，再一次强调，这个更新的目的是让<strong>能获得更高奖赏的动作出现的概率增大，让 bad choice 的概率变小</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self)</span>:</span></span><br><span class="line">    discount_ep_rs_norm = self._discount_and_norm_reward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train on episode</span></span><br><span class="line">    self.sess.run(</span><br><span class="line">        self.train_op, feed_dict= &#123;</span><br><span class="line">            self.tf_obs: np.vstack(self.ep_obs), <span class="comment"># [None, n_obs]</span></span><br><span class="line">            self.tf_acts: np.array(self.ep_as), <span class="comment"># [None, ]</span></span><br><span class="line">            self.tf_vt: discount_ep_rs_norm</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    self.ep_obs.clear()</span><br><span class="line">    self.ep_rs.clear()</span><br><span class="line">    self.ep_as.clear()</span><br><span class="line">    <span class="keyword">return</span> discount_ep_rs_norm</span><br></pre></td></tr></table></figure><p>好了，我们的 Agent 已经写完了，接下来就是让他边玩边学：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">threshold = <span class="number">400</span> <span class="comment"># 展示阈值</span></span><br><span class="line">render = <span class="keyword">False</span> <span class="comment"># 是否展示 bool </span></span><br><span class="line">RL = PolicyGradient( <span class="comment"># 实例化一个 Agent</span></span><br><span class="line">    n_actions = env.action_space.n,</span><br><span class="line">    n_features = env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">    lr= <span class="number">0.01</span>,</span><br><span class="line">    reward_decay = <span class="number">0.99</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>): <span class="comment"># 玩 1000 次</span></span><br><span class="line">    observation = env.reset() <span class="comment"># 重置游戏</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>: </span><br><span class="line">        <span class="keyword">if</span> render:  env.render() <span class="comment"># 如果超过阈值 那么就展示玩的过程 </span></span><br><span class="line">        action = RL.choose_action(observation) <span class="comment"># 让 agent 根据 Observation 选择 action</span></span><br><span class="line">        observation_, reward, done, info = env.step(action) <span class="comment"># 执行一步 Action</span></span><br><span class="line">        RL.store_transition(observation, action, reward) <span class="comment"># 储存状态</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> done: <span class="comment"># 如果 game over 我们进行一次更新</span></span><br><span class="line">            ep_rs_sum = sum(RL.ep_rs)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'running_reward'</span> <span class="keyword">not</span> <span class="keyword">in</span> globals():</span><br><span class="line">                running_reward = ep_rs_sum</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                running_reward = running_reward * <span class="number">0.99</span> + ep_rs_sum</span><br><span class="line">            <span class="keyword">if</span> running_reward &gt; threshold: </span><br><span class="line">                render = <span class="keyword">True</span>  <span class="comment"># rendering</span></span><br><span class="line">            print(<span class="string">"episode:"</span>, i, <span class="string">"  reward:"</span>, int(running_reward))</span><br><span class="line">            vt = RL.learn() <span class="comment"># 更新 </span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        observation = observation_ <span class="comment"># 更新 observation</span></span><br></pre></td></tr></table></figure><p>大功告成，跑一个试试？</p><p>哈哈哈哈其实结果早就放出来了，那个游戏示意图就是大概迭代了 50 次之后电脑自己玩的结果。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>事实上 RL 还是有很多缺点，比如训练 tricky，对样本量需求巨大，这个游戏因为简单所以没有体现出来；以及 Policy Gradient 也存在着一些问题，同时也有对应解决的方案。囿于篇幅，和我们最终的目标是文本生成，就不做深入的讨论，以后有需要再细细研究。我觉得兴趣驱动，任务驱动，自上而下，用什么学什么的方法是最好的。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://karpathy.github.io/2016/05/31/rl/" target="_blank" rel="noopener">Deep Reinforcement Learning: Pong from Pixels</a></p><p><a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf" target="_blank" rel="noopener">Deep Reinforcement Learning Tutorial</a></p><p><a href="https://www.youtube.com/watch?v=tWNpiNzWuO8&amp;t=0s&amp;index=4&amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" target="_blank" rel="noopener">CS294 Fall 2017</a></p><p><a href="https://morvanzhou.github.io/" target="_blank" rel="noopener">莫烦强化学习教程</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Text Generation 是我非常感兴趣的一个问题，传统的 Seq2Seq 能有不错的效果，但如前一篇 Blog 所说，我相信未来的文本生成必然是与 GAN 以及 Reinforcement Learning （RL）相结合的，上交的大佬 Lantao Yu 就有一篇很出色的 SeqGAN 的工作，以及最近的 MaskGAN。接下来的一段时间，尝试实现 SeqGAN 和 MaskGAN，根据需要学习相关的 RL 知识。这篇 Blog 就算是一个前哨，主要讲简单的 Policy Gradient。&lt;/p&gt;
&lt;h2 id=&quot;Brief-Introduction-of-RL&quot;&gt;&lt;a href=&quot;#Brief-Introduction-of-RL&quot; class=&quot;headerlink&quot; title=&quot;Brief Introduction of RL&quot;&gt;&lt;/a&gt;Brief Introduction of RL&lt;/h2&gt;&lt;p&gt;强化学习是一类机器学习算法的统称，其核心思想是让计算机通过尝试，学会特定行为模式以达到期望目标。前一阵子大火的 AlphaGo 以及各类打游戏的 AI 都是强化学习的产物。强化学习中把计算机程序抽象成一个 Agent（Policy），其所处的环境抽象为 Environment，学习就发生在 Agent 和 Environment 的不断交互之中，示意图如下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/agent.png&quot; alt=&quot;Agent and Environment&quot;&gt;&lt;/p&gt;
&lt;p&gt;还有一个重要的概念叫做 state，是关于我们所获知的发生的一切（过去所有的 observation，采取的 action，和获得的 reward） 的函数，在一个观测完全（fully observed）的环境中，我们可以认为 agent 的 state 和环境的 observation 是一样的。事实上，我们需要的 Policy，就是一个 &lt;strong&gt;state -&amp;gt; action 的映射&lt;/strong&gt;。而 Deep Reinforcement Learning，就是通过深层 NN 来构建这个映射。&lt;/p&gt;
&lt;p&gt;强化学习算法一般为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model - free Approach&lt;/li&gt;
&lt;li&gt;Model- based Approach&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里的 Model 指的是环境，Model-free 算法不需要对环境进行建模，Policy 通过环境的 reward （奖赏）来指导学习；而 Model-based 类算法则有着对环境的建模，我们可以利用这个 Model 来搜索预判可能发生的情况，并据此为 Policy 选择行动策略。&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="https://tobiaslee.top/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Taste of Paper</title>
    <link href="https://tobiaslee.top/2018/02/18/Taste-of-Paper/"/>
    <id>https://tobiaslee.top/2018/02/18/Taste-of-Paper/</id>
    <published>2018-02-18T10:28:54.000Z</published>
    <updated>2018-02-20T11:07:13.305Z</updated>
    
    <content type="html"><![CDATA[<p>好久没更新 Blog，随便写点这段时间写文章的感想吧。</p><h2 id="Paper-的种类"><a href="#Paper-的种类" class="headerlink" title="Paper 的种类"></a>Paper 的种类</h2><p>琢磨 Paper 的 Experiment 怎么写的时候，想起老师说的，Paper 大体上分三种：</p><ol><li>Paper for paper：为了写而写，这是我们尽可能需要避免的一类文章。似乎挺多综述文章可以归到这一类了，并不是说总结前人工作没有意义，而是一部分领域已经有较好综述类文章的还写… 就难免刻意之嫌。</li><li>Paper for application: 这应该是目前 CS 或者说是 DL 领域最多的文章了。大家把一开始用在 CV 上的技术或者是模型套到 NLP 上看看能不能 work，或者是拿着第三档的 idea 在一些更实际的问题上跑一个 state of art 的结果。有些人把这些文章也归入灌水的行列，我觉得有失偏驳。首先这类文章对落地，工业界出产品很有帮助的，理论/模型提出 -&gt; 在细分问题上做出好的结果 -&gt; 工业现实应用大致是这么个流程，没了第二步直接转化是不显示的；另外一方面，能起到倒逼创新的作用，像 CNN 的几种架构在 ImageNet 上狂刷 Accuracy，大家怎么调参都无法进步的时候自然回想着去探索一些新的思路。</li><li>Paper for idea/innovation: 说实在话，这一档的文章基本上都是大牛的作品了，算是挖出一个巨坑然后无数人前仆后继的往里跳，比如 Goodfellow 的 GAN 和 Adversarial 一系列作品。 </li></ol><p>三种的 Paper 客观存在，不能说好坏吧。毕竟科研工作者也是人，也要混饭吃，谁能保证一个大牛一开始不是靠糊墙支撑到他挖坑呢？但就本科生来讲，我觉得第二种性价比比较高的。第一种 Paper 可能会被你未来的老板看穿然后认为你就是个科研混子而拒绝了你，第三种势必要有对某一领域的足够积累之后才可能，毕竟这么多人盯着呢谁不想做下一个大牛去 FAIR 当主任… 拍脑袋想出的觉得能改变世界的 idea 不是没有，只不过如我这样的普通人可能第二天起来仔细分析一下就给自己判死刑了。第二种要求你对特定领域问题有深入了解，能看懂最新的 Paper，同时要能够实现相关的模型以及具备调优的能力，大约是一个准 PhD 应该具备的简单版技能树吧。这样老师可能觉得：“诶这学生还不错，培养培养可能是个搬砖的能手。”然后就把你收了（逃</p><h2 id="Taste-Of-Paper"><a href="#Taste-Of-Paper" class="headerlink" title="Taste Of Paper"></a>Taste Of Paper</h2><p>即使是同一档次的文章，其作者的 taste 也是有着蛮大差别的，拿在读的两篇文章来举个例子。</p><h3 id="Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification"><a href="#Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification" class="headerlink" title="Adversarial Training Methods for Semi-Supervised Text Classification"></a>Adversarial Training Methods for Semi-Supervised Text Classification</h3><p>前面已经写过详细写过这篇文章了，可以算是首次 Adversarial Training 应用在 NLP 领域吧。Adversarial Training 在 CV 领域已经用的蛮多了，自然地想到能不能迁移到 NLP，于是在 Word Embedding 层面搞了事情，做出了Semi-Supervised Text Classification 的 SoA。我个人对这篇文章是很有好感的，能算作第二种里面比较接近第三类的，挖了一个 Adversarial Training 在 NLP 的小坑吧，后面的一篇文章就是用这个技术在相对而言比较小众一点的 Relation Extraction 问题上做了工作。文章的 Taste 不错，体现在以下几个方面：</p><ol><li>对于 Adversarial Training（AT） 的定位：在 CV 领域， AT 是作为一种对抗 Adversarial Example 的手段，作为防御机制而提出的，这篇文章认为 AT 是一种 Regularization，通过 Word Embedding 质量的提升来达到避免过拟合的目的，并且通过可视化 good 和 bad 的几个 neighbor 词语来实证；进一步地，对比了 AT 和加 Noise 的效果，有信服力地说明了 AT 是一种更强力的 Regularization</li><li>对实验结果的分析：Paper 本身就是对结果在做解释，但实验结果中出现的意料之外的情况还是需要作者的阐述的，文章提出的一个方法在其中一个数据集上性能输于 baseline， 也给出了较合理的解释。</li><li>参数的选择：这点应该是很多 SoA 的文章都欠缺的，这篇似乎也没能给出比较详尽的解释参数选择的原因。反正能 Work 嘛，反正是玄学（逃</li></ol><h3 id="Adversarial-Training-for-Relation-Extraction"><a href="#Adversarial-Training-for-Relation-Extraction" class="headerlink" title="Adversarial Training for Relation Extraction"></a>Adversarial Training for Relation Extraction</h3><p>这篇就是典型的第二类的文章了，应该是受了前一篇文章的启发，在一个新的问题上评估 AT 的性能。这篇的 Taste 可能就不如前一篇，我觉得一个是很多讨论都过于浅显，没有继续深挖；另一个实验做的也不是很全，可能是囿于篇幅和时间吧。整体来说，灌水的可能性比较大。</p><p>惭愧地说，我在写的东西水平和这篇差不多，甚至还要低一点（不然我也能发顶会了不是）。但我相信，我以后肯定能够完成出色的工作的。</p><h2 id="Supervised-to-Unsupervised"><a href="#Supervised-to-Unsupervised" class="headerlink" title="Supervised to Unsupervised"></a>Supervised to Unsupervised</h2><p>由监督学习到非监督学习，应该也是一个趋势。</p><p>看了 Goodfellow 组最新的 GAN 在文本生成上的一篇文章，《Maskgan: Better Text Generation Via Filling in the <strong>____</strong>》 标题把下划线进去，一开始我都以为这是漏写或者是 Bug，读过之后发现不然。文章大致是用类似初高中完形填空的方法来让 GAN 学习生成文本，但原先 GAN 在文本领域的尝试失败说是梯度无法回传（？存疑，具体有待研究），这里加入了 Reinforcement Learning （RL，强化学习）的机制，来传递梯度更新 Generator 的参数。</p><p>讲上面的例子是想说明一点， RL 是更加接近人类学习过程的一种手段，Reward 函数的设置真的很像一个人活着，为了某一个目标而不断奋斗的这样一个过程。相信未来很多如今是 Supervised 的问题都会与 RL 结合产生新的碰撞。</p><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p>接下来的半个寒假的计划大概就是：</p><ol><li>继续完善手头的工作</li><li>细读 Goodfellow 新作，写一篇笔记，尝试跑代码</li><li>学一波 RL，尝试写个打游戏的 Demo</li><li>刷 PAT</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;好久没更新 Blog，随便写点这段时间写文章的感想吧。&lt;/p&gt;
&lt;h2 id=&quot;Paper-的种类&quot;&gt;&lt;a href=&quot;#Paper-的种类&quot; class=&quot;headerlink&quot; title=&quot;Paper 的种类&quot;&gt;&lt;/a&gt;Paper 的种类&lt;/h2&gt;&lt;p&gt;琢磨 Pape
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>动态规划学习笔记1</title>
    <link href="https://tobiaslee.top/2018/02/02/Dynamic-Programming-Learning/"/>
    <id>https://tobiaslee.top/2018/02/02/Dynamic-Programming-Learning/</id>
    <published>2018-02-02T10:46:04.000Z</published>
    <updated>2018-02-15T13:43:28.136Z</updated>
    
    <content type="html"><![CDATA[<p>菜鸡还是直面自己的弱点才能成长啊，算法还是一块巨大的短板，需要好好补一补。从自己觉得很玄的动态规划（Dynamic Programming，DP） 开始，从几个经典的题型入手，主要参考刘汝佳的《算法竞赛入门经典》。</p><h2 id="Longest-Increasing-Subsequence"><a href="#Longest-Increasing-Subsequence" class="headerlink" title="Longest Increasing Subsequence"></a>Longest Increasing Subsequence</h2><p>POJ 2533 这道最长上升子序列问题是非常经典的 DP 问题，问题描述摘录如下:</p><blockquote><p>A numeric sequence of <em>ai</em> is ordered if <em>a<sub>1</sub></em> &lt; <em>a<sub>2</sub></em> &lt; … &lt; <em>a<sub>N</sub></em>. Let the subsequence of the given numeric sequence (<em>a1<sub>1</sub>, </em>a<sub>2</sub>, …, <em>a<sub>N</sub></em>) be any sequence (<em>a<sub>i1</sub></em>, <em>a<sub>i2</sub></em>, …, <em>a<sub>iK</sub></em>), where 1 &lt;= <em>i1</em> &lt; <em>i2</em> &lt; … &lt; <em>iK</em> &lt;= <em>N</em>. For example, sequence (1, 7, 3, 5, 9, 4, 8) has ordered subsequences, e. g., (1, 7), (3, 4, 8) and many others. All longest ordered subsequences are of length 4, e. g., (1, 3, 5, 8).</p><p>Your program, when given the numeric sequence, must find the length of its longest ordered subsequence.</p></blockquote><p>题意很简单，怎么做呢？简单粗暴的穷举必然是一种方法，找出所有的子串，验证其单调性，复杂度为O(2^n)。</p><p>另外一种普遍的解法就是动态规划了，设 <code>d[i]</code> 是 <code>以 i 位置结尾的数能够成的上升子序列的长度最大值</code> ，那么必然有：</p><p><strong>$d[i] = max(d[i],  d[j]+1) \ if \ a[j] &lt; a[i] \ and \ i &lt; j$</strong></p><p>（PS：刘汝佳的书上的公式给的是 <code>d[i] = max(0, d[j]) + 1</code>，有问题。</p><p>有了这个式子，我们就不难写出下面这段核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; i++) d[i] = <span class="number">1</span>; <span class="comment">// 初始化</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i)</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>;  j &lt; i; ++j) </span><br><span class="line"><span class="keyword">if</span>(arr[j] &lt; arr[i]) </span><br><span class="line">d[i] = max(d[j] + <span class="number">1</span>, d[i]); <span class="comment">// 状态转移方程</span></span><br></pre></td></tr></table></figure><p>上面的那个式子叫做状态转移方程，而 <code>d[i]</code> 叫做位置 <code>i</code> 的状态。很显然，因为<strong>自身构成一个 LIS，长度为 1</strong>，所以我们需要把所有状态都初始化为 1。最后的解雇就是 <code>max(d[i])</code>。复杂度是O(n^2)，还存在着 O(nlogn) 的优化方法，就不展开了。</p><p>从这个例子我们能够发现，动态规划最重要的就是<strong>设计合适的状态和状态转移方程</strong>，从而把问题变小，也是一种分而治之的思想。</p><h2 id="0-1-背包问题"><a href="#0-1-背包问题" class="headerlink" title="0-1 背包问题"></a>0-1 背包问题</h2><p>我一开始听到 0-1 背包就觉得是个厉害的不行的东西肯定很难，其实也还是一个动态规划问题，问题描述如下：</p><blockquote><p>0-1 背包</p><p>有一个窃贼在偷窃一家商店时发现有n件物品，第i件物品价值为vi元，重量为wi，假设vi和wi都为整数。他希望带走的东西越值钱越好，但他的背包中之多只能装下C磅的东西，W为一整数。他应该带走哪几样东西？</p></blockquote><p>这里的核心就是设计状态转移方程，因为每一件物品只有两种状态，取或者不取，所以利用这一点，对于第 i 件物品，考虑当前剩余容量 j，那么用 <strong>d(i, j) 表示取第 i 个，i+1，…，n 件物品能取得的最大价值</strong>，则有</p><blockquote><p> d(i, j) = max( d(i+1, j), d(i+1, j-W<sub>i</sub>) + V<sub>i</sub>)</p></blockquote><p>d(i+1, j) 的含义即为不取第 i 个物品，考虑下一个；d(i+1, j-W<sub>i</sub>) + V<sub>i</sub> 则是取了第 i 个，相应的重量减少为 <code>j - W&lt;sub&gt;i&lt;/sub&gt;</code>继续考虑第 i+1 个，因此我们可以写出下面的这段核心代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = n ; i &gt;= <span class="number">1</span>; i-- )  <span class="comment">// 从后向前考虑</span></span><br><span class="line"><span class="keyword">for</span>(j = C ; j &gt;= <span class="number">0</span> ; j --)&#123;</span><br><span class="line">d[i][j] = (i == n ? <span class="number">0</span> : d[i+<span class="number">1</span>][j]);<span class="comment">// 边界情况 </span></span><br><span class="line"><span class="keyword">if</span>( j &gt;= W[i]) <span class="comment">// 容量允许</span></span><br><span class="line">d[i][j] = max(d[i][j], d[i+<span class="number">1</span>][j-W[i]] + V[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中有一点需要注意：N 从后向前考虑，也就是最简单的情况，保证 i+1 能够继续递推，最终的答案即为 d(1, C) ；如果要从前向后的话，即定义 <strong>f(i, j) 为把前 i 个物品装入容量为 j 的最大价值</strong>，类似我们可以得到 </p><blockquote><p>f(i, j) = max( f(i-1, j), f(i-1, j - W<sub>i</sub>) + V<sub>i</sub>)</p></blockquote><p>这里的边界情况也对应的修改为 <code>i=0</code> 时，价值为 0。这种从前向后的的代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ; j &lt;= W; j++) &#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; W &gt;&gt; V; <span class="comment">// 直接计算 不用保存</span></span><br><span class="line">        f[i][j] = (i == <span class="number">0</span> ? <span class="number">0</span> : f[i<span class="number">-1</span>][j]);</span><br><span class="line">        <span class="keyword">if</span>(j &gt;= V)</span><br><span class="line">            f[i][j] = max(f[i][j], f[i<span class="number">-1</span>][j-W]+V);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>其带来的一个显著的好处就是不用对数据进行保存。</p><p>然而使用二维数组，其大小 N*M 是一种巨大的浪费，甚至会超出一些题目的限制，因此我们可以使用滚动数组来节约空间的开销，即吧 f 数组变成一维的</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">memset</span>(f, <span class="number">0</span>, <span class="keyword">sizeof</span>(f)); <span class="comment">// tips: memset 对int数组只设置为 0/-1 </span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i&lt;= n ; i ++) &#123;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; W &gt;&gt; V;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = C; j&gt;= <span class="number">0</span>; j --) </span><br><span class="line">        <span class="keyword">if</span>(j &gt;= V)</span><br><span class="line">            f[j] = max(f[j], f[j-W] + V);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这边我们先把 j 的顺序改成了逆序，在前面两种方法中，j 枚举的顺序是没关系的。这样，f 数组的计算顺序就是从上到下，从右往左。在计算 <code>f(i, j)</code> 之前，<code>f[j]</code> 里保存着<code>f(i-1, j)</code> ，而 <code>f[j- W]</code> 里保存着的是 <code>f(i-1, j - W)</code>，因为 <code>f(i, j)</code> 的计算所依赖的 <code>f(i-1, j)</code> 和 <code>f(i-1, j - W)</code> 都已经在之前计算出来，所以我们可以利用先前的值更新并且覆盖，达到节约空间的效果。</p><p>所以，可以得出一个经验，<strong>使用滚动数组在一些旧值使用不多的情况下，可以大幅减少内存</strong>。当然，滚动数组也会带来打印结果不便的困难，再一次证明了“天下没有免费的午餐”这一定律。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>DP 的学习才刚刚开始，但这篇文章已经拖了很久，所以决定在年三十先告一段落吧。再小小的总结一下：</p><ol><li>DP 思想，是分治的一种，核心是设计好的状态转移方程</li><li>何时用滚动数组，优点缺点</li><li>memset 的使用注意点，对 int 数组只能设置为 0 或 -1（保证高字节和低字节相同）</li><li>边界情况的考虑</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;菜鸡还是直面自己的弱点才能成长啊，算法还是一块巨大的短板，需要好好补一补。从自己觉得很玄的动态规划（Dynamic Programming，DP） 开始，从几个经典的题型入手，主要参考刘汝佳的《算法竞赛入门经典》。&lt;/p&gt;
&lt;h2 id=&quot;Longest-Increasin
      
    
    </summary>
    
    
      <category term="算法" scheme="https://tobiaslee.top/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ACM" scheme="https://tobiaslee.top/tags/ACM/"/>
    
  </entry>
  
  <entry>
    <title>LaTeX Learning Notes</title>
    <link href="https://tobiaslee.top/2018/01/22/Latex-Learning/"/>
    <id>https://tobiaslee.top/2018/01/22/Latex-Learning/</id>
    <published>2018-01-22T07:24:37.000Z</published>
    <updated>2018-02-01T07:20:05.234Z</updated>
    
    <content type="html"><![CDATA[<p>LaTeX 是论文写作中非常重要的一个排版工具，用这篇文章记录下学习过程中的一些语法、注意事项，以便日后查阅。</p><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>配置 MacOS 上的 LaTeX 环境还是比较轻松愉快的</p><ol><li><p>安装 <a href="https://www.sublimetext.com/" target="_blank" rel="noopener">Sublime Text</a></p></li><li><p>下载 <a href="http://www.tug.org/mactex/" target="_blank" rel="noopener">MacTeX</a>，选择最全的版本 3.1 G</p></li><li><p>安装 <a href="https://skim-app.sourceforge.io/" target="_blank" rel="noopener">Skim</a>，用来预览，并且在<code>选项-同步</code>中选择  <code>Sublime Text</code> ，如下所示：</p><p><img src="/img/skim.png" alt="Skim Setting"></p></li><li><p>在 Sublime Text 中安装 LaTexTools 和配置编译路径，首先安装 Sublime Text 的包管理器，点击 <code>View&gt;Show Console</code> ，在控制台中输入下列 Python 代码安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request,os,hashlib; h = <span class="string">'6f4c264a24d933ce70df5dedcf1dcaee'</span> + <span class="string">'ebe013ee18cced0ef93d5f746d80ef60'</span>; pf = <span class="string">'Package Control.sublime-package'</span>; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( <span class="string">'http://packagecontrol.io/'</span> + pf.replace(<span class="string">' '</span>, <span class="string">'%20'</span>)).read(); dh = hashlib.sha256(by).hexdigest(); print(<span class="string">'Error validating download (got %s instead of %s), please try manual install'</span> % (dh, h)) <span class="keyword">if</span> dh != h <span class="keyword">else</span> open(os.path.join( ipp, pf), <span class="string">'wb'</span> ).write(by)</span><br></pre></td></tr></table></figure><p>安装成功之后，在 Sublime 中使用<code>Command+Shift+P</code>，输入 <code>install</code> ，按下<code>enter</code>，加载仓库后，再输入LatexTools，回车确认后等待安装完成。</p></li><li><p>但因为路径的锅，如果我们现在打开 Sublime Text 复制进去一段 LaTeX 代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%!TEX program = xelatex</span><br><span class="line">\documentclass&#123;article&#125;</span><br><span class="line">\usepackage&#123;fontspec, xunicode, xltxtra&#125;</span><br><span class="line">\setmainfont&#123;Hiragino Sans GB&#125;</span><br><span class="line">\title&#123;Test&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">Hello world!</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>会发现报错：<code>CANNOT COMPILE</code></p><p><strong>解决方案</strong>：通过 Package Control 安装一下 <code>Fix Mac Path</code> 这个包就能解决，安装步骤同 LaTex Tools。</p></li></ol><p>安装完毕之后，我们就可以使用 <code>Command + B</code> 来对写好的 LaTeX 进行渲染，Skim 会弹出渲染后的窗口，并且在 Skim 窗口按住 <code>Command + Shift</code> 再点击，会自动跳转到 Sublime 中相应的部分。<br><a id="more"></a></p><h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>LaTeX 文章的结构由以下一些元素组成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\section&#123;&#125;</span><br><span class="line">\subsection&#123;&#125;</span><br><span class="line">\subsubsection&#123;&#125;</span><br><span class="line"></span><br><span class="line">\paragraph&#123;&#125;</span><br><span class="line">\subparagraph&#123;&#125;</span><br></pre></td></tr></table></figure><p>在 <code>{}</code> 内书写章节或段落的题目。LaTeX 会自动为章节和段落编号。</p><h3 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h3><p>LaTeX 可以通过 <code>\tableofcontents</code> 来根据文章整体的结构自动生成目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;document&#125;</span><br><span class="line">\tableofcontents</span><br><span class="line">\newpage</span><br><span class="line">\maketitle&#123;&#125;</span><br><span class="line">\pagenumbering&#123;gobble&#125;</span><br><span class="line">\maketitle&#123;&#125;</span><br><span class="line">\newpage % for a summry</span><br><span class="line">\pagenumbering&#123;arabic&#125;</span><br><span class="line">\section&#123;Restatement of the Problem&#125;</span><br><span class="line">It&apos;s a problem restatements.</span><br><span class="line">\subsection&#123; problem 1&#125;</span><br><span class="line">It&apos;s problem 1</span><br><span class="line">\subsection&#123; problem 2&#125;</span><br><span class="line">It&apos;s problem 2</span><br><span class="line">\section&#123;Assumptions&#125;</span><br><span class="line">\section &#123;The Model&#125;</span><br><span class="line">\section &#123;Results&#125;</span><br><span class="line">\section &#123;Strengths and Weaknesses&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>编译之后产生的结果如下：</p><p><img src="/img/contents.png" alt="Contens"></p><p>类似的，使用 <code>\listoftables</code> 和 <code>listoffigures</code> 可以产生关于表格和图片的一张目录。</p><p>目录的深度可以通过 <code>\setcounter{tocdepth}{depth}</code> 来设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\setcounter&#123;tocdepth&#125;&#123;1&#125; % Show sections</span><br><span class="line">%\setcounter&#123;tocdepth&#125;&#123;2&#125; % + subsections</span><br><span class="line">%\setcounter&#123;tocdepth&#125;&#123;3&#125; % + subsubsections</span><br><span class="line">%\setcounter&#123;tocdepth&#125;&#123;4&#125; % + paragraphs</span><br><span class="line">%\setcounter&#123;tocdepth&#125;&#123;5&#125; % + subparagraphs</span><br></pre></td></tr></table></figure><h3 id="Pagenumber"><a href="#Pagenumber" class="headerlink" title="Pagenumber"></a>Pagenumber</h3><p>页码的设置也很简单: <code>\pagenumberring{property}</code></p><p>property 有以下三种：</p><ul><li>gobble - no numbers </li><li><em>arabic</em> - arabic numbers 阿拉伯数字</li><li><em>roman</em> - roman numbers 罗马数字</li></ul><h2 id="Formula"><a href="#Formula" class="headerlink" title="Formula"></a>Formula</h2><p>公式也是很重要的一部分，LaTeX 对数学公式的支持真的是非常给力。</p><p>最简单的 <code>$$ F = ma $$</code>，就能够打出 $$ F = ma $$ ，独占一行。</p><p>行内公式只要使用单个美元符号即可 <code>$ formula in line $</code>。</p><p>常用希腊字母:<code>\alpha</code>, <code>\beta</code>,<code>\gamma</code> 等</p><p>使用 <code>\begin{equation}</code> + <code>\end{equation}</code> 在之间打公式，并且<strong>会有序号标注</strong>。</p><p>多个等式需要对齐，在导言部分通过  <code>usepackage{amsmath}</code>  导入 <code>amsmath</code> 包，通过该包中的 <code>align*</code> 可以实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;align*&#125;</span><br><span class="line">1 + 2 &amp;=3 \\ % 换行</span><br><span class="line">1 &amp;= 3 -2 \\ % 等式对齐 &amp; 控制需要对齐的符号</span><br><span class="line">f(x) &amp;= \frac&#123;1&#125;&#123;x&#125; \\ % frac&#123;分子&#125;&#123;分母&#125;</span><br><span class="line">g(x) &amp;= \int^a_b \frac&#123;1&#125;&#123;4&#125;x^4 % int^上限_下限</span><br><span class="line">\end&#123;align*&#125;</span><br></pre></td></tr></table></figure><p>矩阵：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\left[</span><br><span class="line">\begin&#123;matrix&#125;</span><br><span class="line">    1 &amp; 0 \\</span><br><span class="line">8 &amp; 1</span><br><span class="line">\end&#123;matrix&#125;</span><br><span class="line">\right]</span><br></pre></td></tr></table></figure><p>另外，如上面代码里的 <code>\left[</code> 和 <code>\right\</code>，可以显式的声明括号需要包括的内容，LaTeX 会对括号进行自动的放缩以包含所需的内容。</p><h2 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h2><p>插入图片:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[h!] % [h!] 强制图片出现在代码所在位置 否则 LaTeX 会自动调整 </span><br><span class="line">\includegraphics[width=\linewidth] &#123;boat.jpg&#125;</span><br><span class="line">\caption&#123; A boat&#125;</span><br><span class="line">\label &#123;fig:boat1&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>用 <code>\includegraphics[width=\linewidth]{IMAGE.jpg}</code> 来设置图片 <code>[width=\linewidth]</code> 控制高度</p><ul><li>h (here) - same location</li><li>t (top) - top of page</li><li>b (bottom) - bottom of page</li><li>p (page) - on an extra page</li><li>! (override) - will force the specified location</li></ul><h3 id="Multi-Images"><a href="#Multi-Images" class="headerlink" title="Multi Images"></a>Multi Images</h3><p>使用 Subfigure package 来设置多张图片</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;graphicx, subcaption&#125;</span><br><span class="line">\begin&#123;figure&#125;[h!]</span><br><span class="line">\begin&#123;subfigure&#125;[b]&#123;0.2\linewidth&#125;</span><br><span class="line">\includegraphics[width=\linewidth] [image.jpg]</span><br><span class="line">\end&#123;subfigure&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Citation 是论文的重要组成部分，LaTeX 配合 Google Scholar 可以非常便捷的进行文献的引用，主要步骤如下：</p><ol><li>在 <code>.tex</code> 同一目录下建立同名 <code>.bib</code> 文件</li><li>在 <a href="https://scholar.google.com" target="_blank" rel="noopener">Google Scholar</a> 搜索需要引用的文献，然后点击对应文献的 <code>&quot;</code> 按钮，选择 <code>BibTex</code> 格式，将内容复制到 <code>.bib</code> 文件中。 e.g. </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@inproceedings&#123;mikolov2013distributed,</span><br><span class="line">  title=&#123;Distributed representations of words and phrases and their compositionality&#125;,</span><br><span class="line">  author=&#123;Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff&#125;,</span><br><span class="line">  booktitle=&#123;Advances in neural information processing systems&#125;,</span><br><span class="line">  pages=&#123;3111--3119&#125;,</span><br><span class="line">  year=&#123;2013&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>在 <code>.tex</code> 文件中 <code>\end{document}</code> 前输入：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">% .... </span><br><span class="line">\bibliography&#123;bib_name&#125; % 输入 .bib 文件名</span><br><span class="line">\bibliographystyle&#123;ieeetr&#125;  % referene style  引用格式</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><ol><li>在需要引用的地方，使用 <code>\cite{reference_name}</code> 来进行引用 e.g. <code>\cite{mikolov2013distributed}</code></li></ol><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>LaTeX 用熟练了之后还是很方便的，写出来的文章也很漂亮。但这篇文章并不能覆盖所有 LaTeX 的内容，比如很重要的使用 <code>tikz</code> 包来进行绘图。很多东西记不住，也都是即查即用，学会 Google 才是上策。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LaTeX 是论文写作中非常重要的一个排版工具，用这篇文章记录下学习过程中的一些语法、注意事项，以便日后查阅。&lt;/p&gt;
&lt;h2 id=&quot;Installation&quot;&gt;&lt;a href=&quot;#Installation&quot; class=&quot;headerlink&quot; title=&quot;Installation&quot;&gt;&lt;/a&gt;Installation&lt;/h2&gt;&lt;p&gt;配置 MacOS 上的 LaTeX 环境还是比较轻松愉快的&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;安装 &lt;a href=&quot;https://www.sublimetext.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sublime Text&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;下载 &lt;a href=&quot;http://www.tug.org/mactex/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MacTeX&lt;/a&gt;，选择最全的版本 3.1 G&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装 &lt;a href=&quot;https://skim-app.sourceforge.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Skim&lt;/a&gt;，用来预览，并且在&lt;code&gt;选项-同步&lt;/code&gt;中选择  &lt;code&gt;Sublime Text&lt;/code&gt; ，如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/skim.png&quot; alt=&quot;Skim Setting&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在 Sublime Text 中安装 LaTexTools 和配置编译路径，首先安装 Sublime Text 的包管理器，点击 &lt;code&gt;View&amp;gt;Show Console&lt;/code&gt; ，在控制台中输入下列 Python 代码安装：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; urllib.request,os,hashlib; h = &lt;span class=&quot;string&quot;&gt;&#39;6f4c264a24d933ce70df5dedcf1dcaee&#39;&lt;/span&gt; + &lt;span class=&quot;string&quot;&gt;&#39;ebe013ee18cced0ef93d5f746d80ef60&#39;&lt;/span&gt;; pf = &lt;span class=&quot;string&quot;&gt;&#39;Package Control.sublime-package&#39;&lt;/span&gt;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &lt;span class=&quot;string&quot;&gt;&#39;http://packagecontrol.io/&#39;&lt;/span&gt; + pf.replace(&lt;span class=&quot;string&quot;&gt;&#39; &#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;%20&#39;&lt;/span&gt;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&lt;span class=&quot;string&quot;&gt;&#39;Error validating download (got %s instead of %s), please try manual install&#39;&lt;/span&gt; % (dh, h)) &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; dh != h &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; open(os.path.join( ipp, pf), &lt;span class=&quot;string&quot;&gt;&#39;wb&#39;&lt;/span&gt; ).write(by)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;安装成功之后，在 Sublime 中使用&lt;code&gt;Command+Shift+P&lt;/code&gt;，输入 &lt;code&gt;install&lt;/code&gt; ，按下&lt;code&gt;enter&lt;/code&gt;，加载仓库后，再输入LatexTools，回车确认后等待安装完成。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;但因为路径的锅，如果我们现在打开 Sublime Text 复制进去一段 LaTeX 代码：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;%!TEX program = xelatex&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;\documentclass&amp;#123;article&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;\usepackage&amp;#123;fontspec, xunicode, xltxtra&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;\setmainfont&amp;#123;Hiragino Sans GB&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;\title&amp;#123;Test&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;\begin&amp;#123;document&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Hello world!&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;\end&amp;#123;document&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;会发现报错：&lt;code&gt;CANNOT COMPILE&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：通过 Package Control 安装一下 &lt;code&gt;Fix Mac Path&lt;/code&gt; 这个包就能解决，安装步骤同 LaTex Tools。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;安装完毕之后，我们就可以使用 &lt;code&gt;Command + B&lt;/code&gt; 来对写好的 LaTeX 进行渲染，Skim 会弹出渲染后的窗口，并且在 Skim 窗口按住 &lt;code&gt;Command + Shift&lt;/code&gt; 再点击，会自动跳转到 Sublime 中相应的部分。&lt;br&gt;
    
    </summary>
    
    
      <category term="LaTeX" scheme="https://tobiaslee.top/tags/LaTeX/"/>
    
  </entry>
  
  <entry>
    <title>Goodbye 2017, Hello 2018</title>
    <link href="https://tobiaslee.top/2017/12/29/Goodbye-2017-Hello-2018/"/>
    <id>https://tobiaslee.top/2017/12/29/Goodbye-2017-Hello-2018/</id>
    <published>2017-12-29T14:53:22.000Z</published>
    <updated>2017-12-30T00:59:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>2017年的余额还剩了两天，对照着去年的规划，梳理下今年吧。</p><p>17年上半年，基本是按着写的规划在走：继续学习 Android，深入研究一些底层；研究 Java 的一些特性；读了一些畅销书；拿到了一笔奖学金，给爸妈买了礼物。</p><p>下半年，是转变很大的一年，个人觉得比较有意义的事情有：</p><ol><li>Coursera，刷完 Machine Learning、Deep Learning（还有最后一门 Sequence Model，应该是今天开课）</li><li>跟老师一起做 NLP</li><li>去深圳暑实，感觉受到冷落</li><li>加入掘金翻译计划，校对和翻译的文章和文档加起来大约二十篇</li><li>接过 SSSTA 主席的锅，为续一年继续努力</li><li>坚持写 Blog</li></ol><p>觉得比较遗憾的事情：</p><ol><li>没拿到国奖</li><li>没找到女朋友</li></ol><p>其实这两件遗憾的事情也都算不上遗憾了，国奖是技不如人，甘拜下风，唯有再努力；</p><p>女朋友也是可遇不可求，已经做好了单身到毕业的准备，只是有些时候看着他们的热闹，确实有些落寞。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><ol><li>越长大越能体会父母的辛苦，但中国国情，很少会当面说 I love you 这样的词汇，但心里还是非常感激的，希望能不辜负他们的期望吧</li><li>想法真的会随着人的经历而改变，看着一年前信誓旦旦写下的毕业就业的规划，倒不是打脸，只是我觉得这样的调整确实是正常的，也是有必要的。</li><li><strong>打造个人的品牌</strong>，self-branding 很重要。对于一个有大概率在未来成为技术人员的人来说，如何不依靠专业知识来增加收入。我觉得通过知识的输出，让圈子里的人能够知道你，认可你，这样无论是在今后求职也好，深造也好，还有增加“睡后收入”上都能够有所帮助</li><li>自律很重要，回想这一年，保持雷打不动的作息，还有阅读，学习新知识的习惯，对各个方面帮助都很大</li></ol><h2 id="规划"><a href="#规划" class="headerlink" title="规划"></a>规划</h2><ol><li>把手头的题目做完，发表论文</li><li>暑假到深度学习公司实习</li><li>构建个人品牌，提升影响力</li><li>在竞赛方面增加投入</li><li>保持 GPA</li></ol><blockquote><p>Stay Hungry, Stay Foolish.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2017年的余额还剩了两天，对照着去年的规划，梳理下今年吧。&lt;/p&gt;
&lt;p&gt;17年上半年，基本是按着写的规划在走：继续学习 Android，深入研究一些底层；研究 Java 的一些特性；读了一些畅销书；拿到了一笔奖学金，给爸妈买了礼物。&lt;/p&gt;
&lt;p&gt;下半年，是转变很大的一
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Virtual Methods in C++ Learning Notes</title>
    <link href="https://tobiaslee.top/2017/12/14/Virtual-Methods-in-C-Learning-Notes/"/>
    <id>https://tobiaslee.top/2017/12/14/Virtual-Methods-in-C-Learning-Notes/</id>
    <published>2017-12-14T08:52:35.000Z</published>
    <updated>2017-12-14T13:38:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>早就听闻 C++ 中虚函数的大名，今天 OOP 课上老师讲了，感觉还有一些不清晰的地方，上课老师几个比较坑的例子也有一些细节需要注意。回来翻看了一下《C++ Primer Plus》，做笔记以供日后复习。</p><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><p>在学习虚函数之前需要一些 C 语言和基本的 C++ 知识，这里不展开，简单带过：</p><ol><li>函数调用的方式：在 C/C++ 中，函数调用，实质上是<strong>通过地址作为入口</strong>找到对应的代码块，进行执行。</li><li>继承：子类继承父类之后，会获得父类所有的成员，并且，在构造函数（constructor)的调用链中，以<strong>父类在前，子类在后</strong>的顺序依次调用；析构（deconstructor）函数恰好相反，<strong>子类在先，父类在后</strong>。</li></ol><h2 id="静态-动态联编"><a href="#静态-动态联编" class="headerlink" title="静态/动态联编"></a>静态/动态联编</h2><blockquote><p>编译器负责将源代码中的函数调用解释为执行特定的函数代码块，这一过程被称为函数名联编（binding）。在编译过程（Compile time）进行联编称为静态联编（static binding），而虚函数的存在使得编译器有时需要在运行时（Run time）选择正确的虚方法的代码，称为动态联编。</p><p>——《C++ Primer Plus》</p></blockquote><p>似乎还是有点抽象，需要用代码来解释，在此之前，再讨论一个小问题：指针和引用类型的兼容性。</p><a id="more"></a><h3 id="指针和引用类型的兼容性"><a href="#指针和引用类型的兼容性" class="headerlink" title="指针和引用类型的兼容性"></a>指针和引用类型的兼容性</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">double</span> x = <span class="number">2.4</span>;</span><br><span class="line"><span class="keyword">int</span> * pi = &amp; x; <span class="comment">// 非法赋值，指针类型不匹配</span></span><br><span class="line"><span class="keyword">long</span> &amp; rl = x; <span class="comment">// 非法赋值，引用类型不匹配</span></span><br></pre></td></tr></table></figure><p>上面这段代码的错误显而易见，类型不匹配，但是在继承之中：<strong>指向基类的引用或指针可以引用派生类对象</strong>，而不必进行显式的类型转换，比如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseClass</span> &#123;</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DerivedClass</span> :</span> <span class="keyword">public</span> BaseClass &#123;&#125;</span><br><span class="line">DerivedClass dc;</span><br><span class="line">BaseClass * pb = &amp;dc; <span class="comment">// 正确</span></span><br><span class="line">BaseClass &amp; rb = dc; <span class="comment">// 正确</span></span><br></pre></td></tr></table></figure><p>这种将派生类的指针或引用转换为基类的指针或指针的过程，称为向上强制转换（upcasting）。其实也非常好理解，继承就是一个 <code>is-a</code> 的关系，既然 <code>DerivedClass</code> 的实例 <code>dc</code> 是一个 <code>BaseClass</code>，那么其指针和引用自然也能够认为是 <code>BaseClass</code>的指针和引用。</p><p>相反地，如果想要把基类的指针或引用转换为派生类指针或引用，称为（downcasting），如果不用显式类型转换，是不允许的，举个最简单的例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BaseClass bc;</span><br><span class="line"><span class="comment">// DerivedClass &amp;dc =  bc; 错误</span></span><br><span class="line">DerivedClass &amp;dc = (DerivedClass) bc;<span class="comment">// 显式类型转换</span></span><br></pre></td></tr></table></figure><p>同样很好理解，<code>is-a</code>  关系不可逆，如果我的派生类里有父类没有的成员，那基类怎么给你变出一个不存在的成员来？所以自然不行。</p><p>接下来，就进入到虚函数的部分。</p><h2 id="虚函数"><a href="#虚函数" class="headerlink" title="虚函数"></a>虚函数</h2><p>先来看一个例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TradePerson</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">say</span><span class="params">()</span> </span>&#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">"Just Hi\n"</span>;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tinker</span> :</span> <span class="keyword">public</span> TradePerson &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">say</span><span class="params">()</span> </span>&#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">"Hi Tinker\n"</span>;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>我们定义了两个类，<code>Tinker</code> 继承自 <code>TradePerson</code>，接下来：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tinker tinker;</span><br><span class="line">TradePerson * tp;</span><br><span class="line">tp = &amp; tinker;</span><br><span class="line">tp-&gt;say();</span><br></pre></td></tr></table></figure><p>结果是 <code>Just Hi</code>，<code>tp-&gt;say()</code> 根据指针类型 <code>TradePerson</code> 调用了 <code>TradePerson::say()</code>，没有毛病，在编译是就能够确定这个调用的地址，是静态联编。</p><p>而如果我们在 <code>TradePerson</code> 的 <code>say()</code> 方法前加上 <code>virtual</code> 关键字，声明为虚函数（注：声明为虚函数的方法在基类及所有派生类，包括派生类的派生类中都是虚的）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TradePerson</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">say</span><span class="params">()</span> </span>&#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">"Just Hi\n"</span>;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这时的输出结果就会变成 <code>Hi Tinker</code>，这是为什么呢？这个时候 <code>tp-&gt;say()</code> 就会根据对象类型 <code>Tinker</code> 调用 <code>Tinker::say()</code>。而在实际应用中，这个对象类型只有在运行的时候才会确定，也就是我们所说的动态联编。</p><p>似乎是很神奇，那么虚函数是怎么工作的呢？</p><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p>通常编译器处理虚函数的方法是，为每个对象添加一个隐藏成员，这个隐藏成员中保存了一个指向函数地址数组的指针，这个数组称为虚函数表，看下面这个例子：</p><p><img src="/img/vtable.png" alt="Virtual Function Table"></p><p>基类是 <code>Scientist</code> 类，其中隐藏的成员 <code>vptr</code> 指向一个函数地址数组；同样，派生类<code>Physicity</code> 也有一个 <code>vptr</code> 指向另一个函数地址数组。值得注意的是，对于所有声明为虚函数的函数，如果其没有在派生类中被重新定义，<strong>则派生类中该函数的指向和基类相同</strong>，也就是说，我们会去调用父类的这个函数；而如果被重新定义了，则相应的更新其地址。</p><p>上面的例子已经非常详细了，另外还有一点值得注意，就是不管有多少的虚函数，我们都只需要在对象中添加<strong>一个地址成员 vptr</strong>，区别仅仅在于 <strong>vptr </strong>所指向的地址表的大小而已。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>对于使用基类引用或指针作为参数的函数调用，将进行向上转换，这一点很重要，请看下面的例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Brass</span>&#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">ViewAcct</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrassPlus</span> :</span> <span class="keyword">public</span> Brass&#123;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">ViewAcct</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fr</span><span class="params">(Brass &amp; rb)</span> </span>; <span class="comment">// uses rb.ViewAcct()</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fp</span><span class="params">(Brass * pb)</span> </span>; <span class="comment">// uses pb-&gt;ViewAcct()</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fv</span><span class="params">(Brass b)</span> </span>; <span class="comment">// uses b.ViewAcct()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Brass b;</span><br><span class="line">  BrassPluss bp;</span><br><span class="line">  fr(b);<span class="comment">// 调用 Brass::ViewAcct()</span></span><br><span class="line">  fr(bp);<span class="comment">//  调用 BrassPlus::ViewAcct()</span></span><br><span class="line">  fp(b);<span class="comment">// 调用 Brass::ViewAcct()</span></span><br><span class="line">  fp(bp); <span class="comment">//  调用 BrassPlus::ViewAcct()</span></span><br><span class="line">  fv(b);  <span class="comment">// 调用 Brass::ViewAcct()</span></span><br><span class="line">  fv(bp); <span class="comment">// 调用 Brass::ViewAcct()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前面四个输出结果没有问题，前面的几个 <code>bp</code> 都通过 upcasting 调用了 <code>BrassPlus::ViewAcct()</code>，最后两个为什么都是 <code>Brass::ViewAcct()</code>？因为<strong>值传递只把 <code>BrassPlus</code> 的 <code>Brass</code> 部分给了 <code>fv()</code>，只能调用 <code>Brass::ViewAcct()</code></strong>。</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol><li><p>构造函数没有虚函数。因为没有意义，我们在构造子类的时候必然会调用父类的构造函数。</p></li><li><p>一般我们都会把<strong>基类的析构函数声明为虚函数</strong>，Why？看下面的例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line">    <span class="keyword">char</span>* p;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    A() &#123;<span class="built_in">cout</span>&lt;&lt; <span class="string">"A constructor"</span> &lt;&lt; <span class="built_in">endl</span>;  p = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">5</span>];&#125;</span><br><span class="line">    ~A() &#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">"A deconstructor"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="keyword">delete</span>[] p;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Z</span> :</span> <span class="keyword">public</span> A &#123;</span><br><span class="line">    <span class="keyword">char</span> * q;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Z() &#123;<span class="built_in">cout</span>&lt;&lt; <span class="string">"Z constructor!"</span> &lt;&lt; <span class="built_in">endl</span>;  q = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">50</span>];&#125;</span><br><span class="line">    ~Z() &#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">"Z deconstructor"</span> &lt;&lt; <span class="built_in">endl</span>; <span class="keyword">delete</span>[] q;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A * ptr = <span class="keyword">new</span> Z();</span><br><span class="line">    <span class="keyword">delete</span> ptr;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  f();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果是什么呢？</p><blockquote><p>A constructor<br>Z constructor!<br>A deconstructor</p></blockquote><p>没有调用 <code>Z</code> 的析构函数，也就意味着，有申请的内存没有被释放。原因是 <code>delete</code> 会只调用 <code>~A()</code>，解决的办法很简单，就是把基类 <code>A</code> 声明为虚函数。<strong>通常我们会给基类提供虚析构函数，即使它并不需要析构函数</strong>，其原因就在于这样能够调用子类对应的析构函数，来进行资源的释放。 </p></li><li><p>友元不能是虚函数，因为友元不是类成员，而只有成员才能是虚函数。</p></li><li><p>如果派生类没有重新定义虚函数，就会使用该函数的基类版本。如果派生类在派生链之中，则使用最新的虚函数版本（即最晚定义的版本，太爷爷声明虚函数，爷爷没重新定义，爸爸重新定义了，儿子没有定义，那么儿子会调用爸爸的版本）。</p></li><li><p>重新定义将会隐藏方法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>&#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">int</span> a)</span> <span class="keyword">const</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Z</span> :</span> <span class="keyword">public</span> A&#123;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这可能会报错，如果不报错，这段代码意味着:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Z z;</span><br><span class="line">z.print(); <span class="comment">// 正确</span></span><br><span class="line">z.print(<span class="number">5</span>); <span class="comment">// 错误，父类的版本被隐藏</span></span><br></pre></td></tr></table></figure><p><strong>重新定义继承的方法不是重载</strong>，而会将所有同名的基类方法隐藏。</p><p>这告诉我们两点：</p><p>如果重新定义继承的方法，确保与原来的原型完全相同。</p><p>如果基类声明被重载了，则需要在派生类中重新定义所有的基类版本。如果之定义一个版本，则其余版本会被隐藏，无法使用</p><h2 id="感受"><a href="#感受" class="headerlink" title="感受"></a>感受</h2><p>静态联编和动态联编和 JVM 的静态分配合动态分配很类似，不过不同的就是 Java 中的继承和重载比 C++ 简单了很多，更加解放了程序员，而不必操心这些有的没的（逃</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;早就听闻 C++ 中虚函数的大名，今天 OOP 课上老师讲了，感觉还有一些不清晰的地方，上课老师几个比较坑的例子也有一些细节需要注意。回来翻看了一下《C++ Primer Plus》，做笔记以供日后复习。&lt;/p&gt;
&lt;h2 id=&quot;前置知识&quot;&gt;&lt;a href=&quot;#前置知识&quot; class=&quot;headerlink&quot; title=&quot;前置知识&quot;&gt;&lt;/a&gt;前置知识&lt;/h2&gt;&lt;p&gt;在学习虚函数之前需要一些 C 语言和基本的 C++ 知识，这里不展开，简单带过：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;函数调用的方式：在 C/C++ 中，函数调用，实质上是&lt;strong&gt;通过地址作为入口&lt;/strong&gt;找到对应的代码块，进行执行。&lt;/li&gt;
&lt;li&gt;继承：子类继承父类之后，会获得父类所有的成员，并且，在构造函数（constructor)的调用链中，以&lt;strong&gt;父类在前，子类在后&lt;/strong&gt;的顺序依次调用；析构（deconstructor）函数恰好相反，&lt;strong&gt;子类在先，父类在后&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;静态-动态联编&quot;&gt;&lt;a href=&quot;#静态-动态联编&quot; class=&quot;headerlink&quot; title=&quot;静态/动态联编&quot;&gt;&lt;/a&gt;静态/动态联编&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;编译器负责将源代码中的函数调用解释为执行特定的函数代码块，这一过程被称为函数名联编（binding）。在编译过程（Compile time）进行联编称为静态联编（static binding），而虚函数的存在使得编译器有时需要在运行时（Run time）选择正确的虚方法的代码，称为动态联编。&lt;/p&gt;
&lt;p&gt;——《C++ Primer Plus》&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;似乎还是有点抽象，需要用代码来解释，在此之前，再讨论一个小问题：指针和引用类型的兼容性。&lt;/p&gt;
    
    </summary>
    
    
      <category term="C++" scheme="https://tobiaslee.top/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>关于 Adversarial Training 在 NLP 的一些思考</title>
    <link href="https://tobiaslee.top/2017/12/10/Thoughts-about-adversarial-training-methods-in-NLP/"/>
    <id>https://tobiaslee.top/2017/12/10/Thoughts-about-adversarial-training-methods-in-NLP/</id>
    <published>2017-12-10T07:40:48.000Z</published>
    <updated>2017-12-11T13:57:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>围绕 ICLR 2017 的这篇 Adversarial Training Methods for Semi-Supervised Text Classification，谈谈关于对抗训练在自然语言处理领域应用的一些想法。</p><hr><p>这篇文章的核心思想：通过 Goodfellow 提出的 Fast Gradient Sign Method (FGSM) 来计算出扰动，添加到到连续的 Word Embedding 上产生 X_adv, 再一次喂给 model，得到 Adverarial Loss，通过和原来的 Classification Loss（Cross-Entropy）做一个相加得到新的 Loss。通过优化这个 Loss，能够在文本分类任务上取得超过目前 State-of-art 的表现。<br><a id="more"></a></p><h2 id="Perturbation-on-Word-Embedding"><a href="#Perturbation-on-Word-Embedding" class="headerlink" title="Perturbation on  Word Embedding"></a>Perturbation on  Word Embedding</h2><p>这一点在前一篇阅读笔记中谈到了，和图像领域不同，组成文本的词语一般都是以 one-hot vector 或者是 word index vector 来表示的，可以视作是离散的，而非连续的 RGB 值。这就导致如果我们直接在 raw text 上进行扰动，则可能扰动的方向和大小都没有任何明确的语义对应。但文章认为 word embedding 的表示是可以认为是连续的：</p><blockquote><p>Because the set of high-dimensional one-hot vectors does not admit infinitesimal perturbation,we define the perturbation on continuousword embeddings instead of discrete word inputs.</p></blockquote><p>因而这个扰动可以在一定程度上 make sense，至于更详细的原因，后面还会谈到。</p><h2 id="Why-State-of-Art"><a href="#Why-State-of-Art" class="headerlink" title="Why State of Art"></a>Why State of Art</h2><p>文章提到，这种 Adversarial Training 和图像中有所不同，其通过 FGSM 产生的 X_adv 并不能是做一种对抗样本，因为扰动之后的 Word Embedding 有极大可能并不能映射到一个真实存在的词语上，这和图像中 RGB 的扰动能够产生人眼无法分辨区别的对抗样本是不一样的。所以作者认为，这种 Adversarial Training 更<strong>类似于一种 Regularization 的手段，能够使得 word embedding 的质量更好，避免 overfitting</strong>，从而取得出色的表现。</p><p>Adversarial Training 能够提升 Word Embedding 质量的一个原因是：</p><p>有些词与比如（good 和 bad），其在语句中 Grammatical Role 是相近的，我理解为词性相同（都是形容词），并且周围一并出现的词语也是相近的，比如我们经常用来修饰天气或者一天的情况（The weather is good/bad; It’s a good/bad day），这些词的 Word Embedding 在是非常相近的。文章中用 Good 和 Bad 作为例子，找出了其最接近的 10 个词：</p><p><img src="/img/neighbors.png" alt="Top 10 Neighbors  to &#39;good&#39; and &#39;bad&#39;"></p><p>可以发现在 Baseline 和 Random 的情况下, good 和 bad 出现在了彼此的邻近词中，而经过喂给模型经过扰动之后的 X-adv 之后，也就是 Adversarial 这一列，这种现象就没有出现，事实上， good 掉到了 bad 接近程度排第 36 的位置。</p><p>我们可以猜测，在 Word Embedding 上添加的 Perturbation 很可能会导致原来的 <code>good</code> 变成 <code>bad</code>，导致分类错误，计算的 Adversarial Loss 很大，而<strong>计算 Adversarial Loss 的部分是不参与梯度计算的，也就是说，模型（LSTM 和最后的 Dense Layer）的 Weight 和 Bias 的改变并不会影响 Adversarial Loss</strong>，模型只能通过<strong>改变 Word Embedding Weight 来努力降低它</strong>，进而如文章所说：</p><blockquote><p>Adversarial training ensures that the meaning of a sentence cannot be inverted via a small change, so these words with similar grammatical role but different meaning become separated.</p></blockquote><p>这些含义不同而语言结构角色类似的词能够通过这种 Adversarial Training 的方法而被分离开，从而提升了 Word Embedding 的质量，帮助模型取得了非常好的表现。</p><p>先前也有工作通过在输入上增加 Noise 来作为 Regularization 手段，可能会有人问，这个 Perturbation 和 Noise 有什么区别？Perturbation 和 Noise 相比有什么好处呢？ 在高维空间中，Noise 几乎和 cost 的梯度正交，没什么影响，而 Adversarial Perturbations 是对 cost 有很明显的影响的，也就是 Adversarial Cost 应该是较大的，但我在实验中做出来的 Adversarial Loss 远比 Classification Loss 要小，原因猜测是<strong>没有使用预训练的 Word Embedding 所导致</strong>。</p><h2 id="Other-Adversarial-Training-Methods"><a href="#Other-Adversarial-Training-Methods" class="headerlink" title="Other Adversarial Training Methods"></a>Other Adversarial Training Methods</h2><p>在 NLP 领域还有几种 Adversarial Training 的方法。</p><p>Papernot 提出的 Jacobian Saliency Map Approach（JSMA）原先应用在图像领域，同样有迁移到文本领域的应用，主要可以参考这篇 <a href="https://arxiv.org/abs/1604.08275" target="_blank" rel="noopener">Paper</a>，核心思想是通过计算 Jacobian Matrix 来衡量每个 Word Embedding 的改变对输出结果的影响，再依此在字典中寻找词语来进行替换</p><p>与此类似的还有这一篇 <a href="https://arxiv.org/abs/1707.02812" target="_blank" rel="noopener">Towards Crafting Text Adversarial Samples</a>，其提出了一种计算词语对最终分类标签的贡献的方式，并且构建一个候选词池（candidate pool)，根据贡献大小排序后从池中选词进行替换，或者是去除和添加。</p><h2 id="Why-Adversarial-Training"><a href="#Why-Adversarial-Training" class="headerlink" title="Why Adversarial Training"></a>Why Adversarial Training</h2><p>现在来谈谈在 NLP 领域中 Adversarial Training 是一个什么样的角色，或者说我们希望它们起到什么样的效果？</p><p>我认为，主要有以下四种主要的目的：</p><ol><li>作为 Regularization 的手段，提升模型的性能（分类准确率），防止过拟合</li><li>产生对抗样本，攻击深度学习模型，产生错误结果（错误分类）</li><li>让上述的对抗样本参与的训练过程中，提升对对抗样本的防御能力，具有更好的泛化能力</li><li>利用 GAN 来进行自然语言生成</li></ol><p>第四点并不在这篇文章的讨论范围内，而前三点事实上可以认为是两种不同的手段，而它们看似都是对模型泛化能力的提升，但其对于泛化标的是有所区别的，前者是通过提升 Word Embedding 质量，可以认为具有一定的普适性；而后者，则主要是对于对抗样本的适应和泛化。</p><p>但这两种方式各自也都存在着一些无法解释或者有些模糊不清的地方：</p><p>我们该如何理解前者的 Perturbation，仅仅是如上文所说的 <code>good</code> 和 <code>bad</code> 的问题吗？以及 Perturbation 的方向，仅仅是对 Cost 影响最大的方向，其在语言场景中有所对应吗？<strong>理解这个 Perturbation 对于 NLP 非常重要</strong>，其重要性甚至超出我们获得的所谓的“对抗样本”。</p><p>对于后者，在图像领域中个别像素点的 RGB 的值的改变如果可以认为是现实生活中常有发生的话，那么<strong>逐词的替换似乎就有点和我们创作文本的方式相悖</strong>：我会写一句 <code>I feel good today</code> 然后通过把 <code>I</code> 换成类似的 <code>He</code> 吗？首先语法上就出现了问题，其次这二者希望表达的完全不是一个意思。所以这种方式，对攻击神经网络模型很有效，但真的有意义吗？</p><p>这些问题，值得进一步的思考。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;围绕 ICLR 2017 的这篇 Adversarial Training Methods for Semi-Supervised Text Classification，谈谈关于对抗训练在自然语言处理领域应用的一些想法。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;这篇文章的核心思想：通过 Goodfellow 提出的 Fast Gradient Sign Method (FGSM) 来计算出扰动，添加到到连续的 Word Embedding 上产生 X_adv, 再一次喂给 model，得到 Adverarial Loss，通过和原来的 Classification Loss（Cross-Entropy）做一个相加得到新的 Loss。通过优化这个 Loss，能够在文本分类任务上取得超过目前 State-of-art 的表现。&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization 学习笔记</title>
    <link href="https://tobiaslee.top/2017/12/06/Batch-Normalization-Learning-Notes/"/>
    <id>https://tobiaslee.top/2017/12/06/Batch-Normalization-Learning-Notes/</id>
    <published>2017-12-06T08:27:58.000Z</published>
    <updated>2017-12-06T12:02:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>Batch Normaliza（BN）是现在几乎已经成为深度学习，特别是图像领域几乎标配的一种 Technique，常常能在论文中见到。拜读了 <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> 这篇 Citation 3000+ 的论文和李宏毅老师的<a href="https://www.bilibili.com/video/av9770302/#page=10" target="_blank" rel="noopener">深度学习</a>课程之后，写篇 Blog 作为学习笔记。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h3 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h3><p>李宏毅老师是以机器学习中常用的 Feature Scaling 来作为一开始的引入的，Feature Scaling 顾名思义，就是把各个特征的值通过计算数学期望和方差，进行一个放缩。</p><p>这个放缩有什么意义呢？如果我们的 Feature，如下图中的 X<sub>1</sub> 和 X<sub>2</sub>，其数值不在一个数量级上（1 vs 100），而如果他们对于最终结果的影响的权重是差不多的，则在我们的 Minimize Loss 的过程中，Loss 对 权重 W<sub>1</sub> 和 W<sub>2</sub> 的梯度有很大的差别，类似一个椭圆，这就会导致 Training 过程变得比较困难：<strong>因为如果要达到 Optima，我们在横/纵两个方向上要选择不同的 Learning Rate</strong>（横向较大，纵向较小），事实上这样的设置是比较困难，所以一般我们都会倾向于选择一个较小的 Learning Rate，这就导致了训练速度的下降。</p><p><img src="/img/feature_scaling.png" alt="Feature Scaling"></p><p>而经过放缩之后的梯度就比较接近一个正圆，各个方向上的梯度差不多，就能够使用较大的 Learning Rate，从而使得训练过程更快。<br><a id="more"></a></p><h3 id="Internal-Covariate-Shift"><a href="#Internal-Covariate-Shift" class="headerlink" title="Internal Covariate Shift"></a>Internal Covariate Shift</h3><p>Feature Scaling 是机器学习中的技巧，在深度学习，神经网络里，怎么去运用类似的方法呢？</p><p>Paper 作者在摘要里讲：深层神经网络复杂的一个原因是层之间的输入的分布随着训练而改变（参数发生变化），我的理解就是各层的输入可以看做是一个个的 Feature，而他们的数值不一并且随着时间变化。这一现象被作者成为 Internal Covariate Shift：</p><blockquote><p>We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training.</p></blockquote><p>这可能会导致训练中梯度消失（Gradient Saturation），造成训练困难，我们无可奈何地选择较小的 Learning Rate 以及精心设置的参数初始值以避免这种情况出现。</p><p>这个层之间输入的随训练变化而变化的过程，李宏毅老师的示意图非常形象：</p><p><img src="/img/internal_covariate_shift.png" alt="Internal Covariate Shift"></p><p>模型层与层之间的输入关系就好比一个音筒纸杯传话，彼此要对上才能达到目标。后面的人看前面的人旧的位置进行调整，而这个调整因为前面的人的自我调整导致不匹配（移动过多），因为需要不断地进行重复调整，导致这个过程非常缓慢。</p><p>解决的思路其实已经说了，就是要让输入在训练过程中保持一致，自然就想到用和 Feature Scaling 类似的手段，对这些层之间的 activations 做标准化的处理，但我们怎么得到同样随训练而变化的 mean 和 variance 呢？Batch Normalization 就是 Paper 作者提出的解决方法。</p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Batch Normalization 的思路也很简单，给每一个 Batch，计算一个 mean 和 variance，对每一个 Batch 中层之间的输入进行 Scaling 的操作：</p><p><img src="/img/BN.png" alt="Batch Normalization"></p><p>值得注意的是：<strong>是在 Z 上做 Scaling 操作，而不是经过激活函数之后得到的 A</strong>，事实上在 A 上做 Scaling 也是可以，但因为通过 Sigmoid 之类的激活函数之后再算梯度可能会出现 Saturation，这是我们不想要的，所以直接在 Z 上做可能会比较好。</p><p>同时在 Paper 中，还有两个参数 γ 和 β 用来对 normalize 之后的 x 进行一个放缩，这两个参数是相当于给模型一个自我调整 normalize 的旋钮，如果学出来 γ 等于 σ，β 等于 μ，就意味着可能我们的模型不需要 normalization，这个选择的权利交给神经网络自己。</p><p>因为 mean 和 variance 是在 batch 中计算的，<strong>这对 batch size 就提出了一定要求，不能太小，否则这个 mean 和 variance 就没有什么意义</strong>，Paper 中的算法如下：</p><p><img src="/img/bn_al.png" alt="Batch Normalization Algorithms"></p><h3 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h3><p>还有一点值得注意的是，在我们计算梯度的时候，<strong>μ 和 σ 事实上是对梯度有影响的，而不是一个常量</strong>。我们可以把 μ 和 σ 看做是一个 batch 的一个 feature，然后对其使用链式法则计算导数：</p><p><img src="/img/bn_gradients.png" alt="Gradients in BN"></p><p>对于导数中的求和符号我是这么理解的：<strong>Loss Function 是一个 batch 中各个 example 的 loss 之和，因而其导数也是对各个 example 求导后相加得到</strong>。</p><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>还有一个小细节，在我们 Test 的时候，μ 和 σ 怎么得到？其实也很简单，在训练过程中记录下每个 Batch 的 mean 和 variance，Test  的时候取平均就行。</p><h2 id="Benifits"><a href="#Benifits" class="headerlink" title="Benifits"></a>Benifits</h2><p>Batch Normalization 带来的好处有以下几点：</p><ol><li>更大的 Learning Rate，意味着更快的训练速度</li><li>避免了一些 Gradient Vanishing/Exploding 的情况</li><li>降低了对 Initialization 的要求</li><li>能够起到一定的 Regularization 作用，可以去除 Dropout 和使用更小的 L2 罚项。</li></ol><p>前面三点都是因为减轻了 Internal Covariate Shift 带来的好处，而第四点，Overfitting 实质上是 traning data 中的 noise 导致，通过 Normalization 我们可以消除这个 nosie，从而起到一定的 Regularization 作用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Batch Normaliza（BN）是现在几乎已经成为深度学习，特别是图像领域几乎标配的一种 Technique，常常能在论文中见到。拜读了 &lt;a href=&quot;https://arxiv.org/pdf/1502.03167.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 这篇 Citation 3000+ 的论文和李宏毅老师的&lt;a href=&quot;https://www.bilibili.com/video/av9770302/#page=10&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;深度学习&lt;/a&gt;课程之后，写篇 Blog 作为学习笔记。&lt;/p&gt;
&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;h3 id=&quot;Feature-Scaling&quot;&gt;&lt;a href=&quot;#Feature-Scaling&quot; class=&quot;headerlink&quot; title=&quot;Feature Scaling&quot;&gt;&lt;/a&gt;Feature Scaling&lt;/h3&gt;&lt;p&gt;李宏毅老师是以机器学习中常用的 Feature Scaling 来作为一开始的引入的，Feature Scaling 顾名思义，就是把各个特征的值通过计算数学期望和方差，进行一个放缩。&lt;/p&gt;
&lt;p&gt;这个放缩有什么意义呢？如果我们的 Feature，如下图中的 X&lt;sub&gt;1&lt;/sub&gt; 和 X&lt;sub&gt;2&lt;/sub&gt;，其数值不在一个数量级上（1 vs 100），而如果他们对于最终结果的影响的权重是差不多的，则在我们的 Minimize Loss 的过程中，Loss 对 权重 W&lt;sub&gt;1&lt;/sub&gt; 和 W&lt;sub&gt;2&lt;/sub&gt; 的梯度有很大的差别，类似一个椭圆，这就会导致 Training 过程变得比较困难：&lt;strong&gt;因为如果要达到 Optima，我们在横/纵两个方向上要选择不同的 Learning Rate&lt;/strong&gt;（横向较大，纵向较小），事实上这样的设置是比较困难，所以一般我们都会倾向于选择一个较小的 Learning Rate，这就导致了训练速度的下降。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/feature_scaling.png&quot; alt=&quot;Feature Scaling&quot;&gt;&lt;/p&gt;
&lt;p&gt;而经过放缩之后的梯度就比较接近一个正圆，各个方向上的梯度差不多，就能够使用较大的 Learning Rate，从而使得训练过程更快。&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://tobiaslee.top/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>文本预处理方法小记</title>
    <link href="https://tobiaslee.top/2017/11/17/Sentence-preprocessing-skills/"/>
    <id>https://tobiaslee.top/2017/11/17/Sentence-preprocessing-skills/</id>
    <published>2017-11-17T12:41:59.000Z</published>
    <updated>2018-07-28T07:02:29.106Z</updated>
    
    <content type="html"><![CDATA[<p>最近做 Sentiment Analysis 的问题，用 IMDB，Twitter 等 Dataset，拿到原始的一条条文本，直接喂给 Model 肯定不行，需要进行对文本进行预处理。预处理的精细程度很大程度上也会影响模型的性能。这篇 Blog 就记录一些预处理的方法。</p><h2 id="Remove-Stop-Words"><a href="#Remove-Stop-Words" class="headerlink" title="Remove Stop Words"></a>Remove Stop Words</h2><p>​    Stop Words，也叫停用词，通常意义上，停用词大致分为两类。一类是人类语言中包含的功能词，这些功能词极其普遍，与其他词相比，功能词没有什么实际含义，比如’the’、’is’、’at’、’which’、’on’等。另一类词包括词汇词，比如’want’等，这些词应用十分广泛，但是对这样的词搜索引擎无法保证能够给出真正相关的搜索结果，难以帮助缩小搜索范围，同时还会降低搜索的效率，所以通常会把这些词从问题中移去，从而提高搜索性能。</p><a id="more"></a><p>​    在对 Twitter 做 Sentiment Analysis 时，因为Twitter 的文本大部分是人们的口头语，所以停用词的使用是非常频繁的，而这些停用词对于情感极性的贡献并不大，所以一般在预处理阶段我们会将它们从文本中去除，以更好地捕获文本的特征和节省空间（Word Embedding）。Remove Stop Words 的方法有很多，Stanford NLP 组有一个工具就能够办到，Python 中也有 nltk 库来做一些常见的预处理，这里就以 nltk 为例来记录去除停用词的操作：</p><p>首先我们导入 <code>nltk.corpus</code> 中的 <code>stopwords</code> 对象，选取 <code>english</code> 的 stopwords，生成一个 set</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line">stop = set(stopwords.words(<span class="string">'english'</span>)) <span class="comment"># </span></span><br><span class="line">print(stop)</span><br></pre></td></tr></table></figure><blockquote><p>‘below’, ‘she’, ‘both’, ‘didn’, ‘his’, ‘we’, ‘they’, ‘from’, ‘themselves’, ‘more’, ‘shan’, ‘which’, ‘whom’, ‘further’, ‘needn’, ‘while’, ‘at’,  … </p></blockquote><p>以上是 stop 中的部分 stop words，确实没有什么意义，接下来定义一个函数，将原始的数据集文本中的停用词去除：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_stop</span><span class="params">(data)</span>:</span></span><br><span class="line">  total_words = <span class="number">0</span> <span class="comment"># 用于计算平均长度 以方便后面截断长度的设定</span></span><br><span class="line">    after_remove = list()</span><br><span class="line">    length = len(data) <span class="comment"># 获取数据集的大小 一般是一个 ndarray</span></span><br><span class="line">    total_words = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(length): <span class="comment"># 依次处理文本</span></span><br><span class="line">        clean_sentence = list()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> data[i].split(): <span class="comment"># 将文本 spilit 成一个个单词</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop:</span><br><span class="line">                clean_sentence.append(word)</span><br><span class="line">        total_words += len(clean_sentence)</span><br><span class="line">        afters.append(<span class="string">" "</span>.join(after))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Average length:"</span>, total_words/length)</span><br><span class="line">    <span class="keyword">return</span> np.asarray(after_remove)</span><br></pre></td></tr></table></figure><p>思路很简单，就是判断 word 是否在 stop words 的集合中，如果不在就保留下来，最后通过 <code>&quot; &quot;.join(list)</code> 将非停用词的列表生成一个字符串，这个 <code>.join</code> 非常有意思；同样，为了统计去掉停用词之后的平均句子长度，在代码中我们每次都计算一下每个句子的长度，最后求一个平均，这是为了一些需要进行截断操作的处理做准备。</p><h2 id="To-Word-Index"><a href="#To-Word-Index" class="headerlink" title="To Word Index"></a>To Word Index</h2><p>文本是无法直接交给我们模型进行训练的，我们需要把它们变成数字，在 NLP 领域很常用的一种方法就是 Sentence -&gt; Word ID -&gt; Word Embedding，而Sentence -&gt; Word ID 这一步，就是把每一个词变成一个独立的整数，比如下面的例子：</p><p>“I am a student”，“You are a student, too”</p><p>两个句子中总共有 7 个不一样的单词，我们按照出现的顺序来编号：</p><p>I：1  am: 2  a: 3  student: 4 You: 5  are: 6  too: 7</p><p>那么两个句子就会对应的被转换为：</p><p>[1 2 3 4] 和 [5 6 3 4 7] </p><p>如果我们遇到了词汇表中没有的词，一般用 0 或者 UNK（unknown）来表示，所以“You are beautiful today”的表示就是 [5 6 0 0]</p><p>如何利用 TensorFlow 来实现呢，很简单，利用 VocabularyProcessor 这个类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(</span><br><span class="line">    MAX_LENGTH, min_frequency=<span class="number">2</span>)</span><br><span class="line">x_transform_train = vocab_processor.fit_transform(train_text)</span><br><span class="line">x_transform_test = vocab_processor.transform(test_text)</span><br></pre></td></tr></table></figure><p>其主要有两个函数：<code>fit()</code> 和 <code>transform()</code>，fit 就是形成一个 word: id 的字典，transform 就是根据字典来把句子转换成 id 组成的向量，一般我们通过 fit 训练集，再根据由训练集得到的 vocab dict 来 transform 测试集。你可能会说这样岂不是会有很多 UNK？是的，这也是真实的情况，每时每刻都有新词被造出来，而我们的训练集是不可能 hold 住所有的词语的。</p><p>值得一提的是，这里 <code>VocabularyProcessor</code> 的构造函数中还有一个 <code>min_frequency</code> 参数，可以筛掉出现次数少于这个参数的词，去低频次，也是一种预处理的手段。</p><p><strong>Update</strong>: 随着 TensorFlow 版本的更新，原来 <code>contrib</code> 包里的 <code>VocabularyProcessor</code> 已经不被推荐使用了，改为使用 <code>tf.keras.preprocessing</code> 包中的相关 API 进行操作，其中最重要的一个类就是 <code>Tokenizer</code> 这个类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenizer</span></span><br><span class="line">tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=<span class="number">20000</span>， oov_token=<span class="string">'&lt;UNK&gt;'</span>)</span><br><span class="line">tokenizer.fit_on_texts(train_text)</span><br><span class="line">train_idxs = tokenizer.text_to_sequences(train_text)</span><br><span class="line">test_idxs = tokenizer.text_to_sequences(test_text)</span><br></pre></td></tr></table></figure><p><code>Tokenizer()</code> 构造函数有几个比较有用的参数：</p><ul><li>num_words: 保留的 vocab_size 的个数，即词频最高的 <code>num_words</code> 个 word 在词表中，这和之前的 <code>min_frequency</code> 的设计是恰好相反的，一个是考虑留下多少，一个是考虑去掉多少出现频率较低的。</li><li>oov_token: 超出词表（test 中有 train 中未出现的词）时，将其设置为指定的 token，<strong>这个 <code>&lt;UNK&gt;</code> 在不会出现在 <code>word_docs</code> 和 <code>word_counts</code> 中，但是会出现 <code>word_index</code> 中，所以在计算 vocab_size 时使用 <code>len(word_index)</code> 比较稳妥</strong></li></ul><p>流程也是一样的，先利用 fit_on_texts 进行词表的构建，再利用 <code>text_to_sequences()</code> 来将 word 转化为对应的 idx；Tokenizer 有三个非常有用的成员：</p><ul><li>word_docs：一个 <code>OrderedDict</code>，用于记录各个词出现的次数</li><li>word_count：一个 <code>dict</code>，用于记录各个词出现的次数</li><li>word_index：word2idx 的一个字典，我们可以根据 word 拿到对应的 index，也可以通过简单的一行代码来构建一个 idx2word 的字典用于之后将 indexes 翻译成文本 <code>idx2word = {idx: word for word, idx in zip(word2idx.keys(), word2idx.values())}</code></li></ul><p>转换成 index 序列之后，长度的截取和 padding 则需要利用 <code>tf.keras.preprocessing.sequence</code> 包中的 <code>pad_sequences</code> 函数来进行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_padded = tf.keras.preprocessing.sequence.pad_sequences(train_idxs, </span><br><span class="line">                                    maxlen=MAX_LENGTH, padding=<span class="string">'post'</span>, truncating=<span class="string">'post'</span>)</span><br></pre></td></tr></table></figure><p><code>MAX_LENGTH</code> 和之前一样，用于指定需要截取到或者补齐到的程度，而之后的 <code>padding</code> 和 <code>truncating</code> 关键字则用于指定补齐和截取是哪个方向（<code>post</code> 或者 <code>pre</code>），比如对于一个序列 <code>[1, 2, 3, 4, 5]</code> 如果我们用 <code>maxlen = 6</code> ，而 padding 选择 <code>pre</code> 得到的结果就是：</p><blockquote><p>[0, 1, 2, 3, 4, 5]</p></blockquote><p>而如果 padding 选择 <code>post</code> ，结果就是：</p><blockquote><p>[1, 2, 3, 4, 5, 0]</p></blockquote><p><code>truncating</code> 同理。</p><h2 id="Load-Pre-trained-Word-Embedding"><a href="#Load-Pre-trained-Word-Embedding" class="headerlink" title="Load Pre-trained Word Embedding"></a>Load Pre-trained Word Embedding</h2><p>我们可以使用预训练好的 Word Embedding 来加快模型的训练速度，常用的有 Stanford 的 <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> 和 Google 家的 <a href="https://code.google.com/archive/p/word2vec" target="_blank" rel="noopener">Word2Vec</a> ，这里就以 GloVe 为例，展示如何加载预训练的 Word Embedding。</p><p>GloVe 解压后会得到几个 <code>.txt</code> 文件，分别是 50/100/200/300 维的 Word Embedding，其格式是 “词语 向量” ，如下所示</p><blockquote><p>the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062</p></blockquote><p>那么我们先需要构建一个 vocab 词汇表表存放所有的单词，以及其对应的 Word Embedding：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">filename = <span class="string">'glove.6B.50d.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadGloVe</span><span class="params">(filename)</span>:</span></span><br><span class="line">    vocab = []</span><br><span class="line">    embd = []</span><br><span class="line">    file = open(filename, <span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file.readlines(): <span class="comment"># 读取 txt 的每一行</span></span><br><span class="line">        row = line.strip().split(<span class="string">' '</span>)</span><br><span class="line">        vocab.append(row[<span class="number">0</span>])</span><br><span class="line">        embd.append(row[<span class="number">1</span>:])</span><br><span class="line">    print(<span class="string">'Loaded GloVe!'</span>)</span><br><span class="line">    file.close()</span><br><span class="line">    <span class="keyword">return</span> vocab, embd</span><br><span class="line"></span><br><span class="line">vocab, embd = loadGloVe(filename)</span><br><span class="line">vocab_size = len(vocab) <span class="comment"># 词表的大小</span></span><br><span class="line">embedding_dim = len(embd[<span class="number">0</span>]) <span class="comment"># embedding 的维度</span></span><br><span class="line">print(<span class="string">"Vocab size : "</span>, vocab_size)</span><br><span class="line">print(<span class="string">"Embedding dimensions : "</span>, embedding_dim)</span><br></pre></td></tr></table></figure><p>获取了 vocab 和 embedding 之后，我们将就可以利用 VocabularyProcessor 来根据 vocab 将文本转化为对应的 Id，并且进行 Embedding 操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">print(embedding.shape)</span><br><span class="line"></span><br><span class="line">vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_LENGTH)</span><br><span class="line">pretrain = vocab_processor.fit(vocab) <span class="comment"># 根据我们的 vocab 进行 fit</span></span><br><span class="line">x_transform_train = vocab_processor.transform(x_train) <span class="comment"># train set</span></span><br><span class="line">x_transform_test = vocab_processor.transform(x_test) <span class="comment"># test set</span></span><br><span class="line"></span><br><span class="line">vocab = vocab_processor.vocabulary_</span><br><span class="line">vocab_size_after_process = len(vocab) <span class="comment"># 注意：这个 size 和前面的不一样了</span></span><br><span class="line">print(<span class="string">"Vocab size after process:"</span>, vocab_size_after_process)</span><br></pre></td></tr></table></figure><p>值得注意的是，<strong>经过 fit 后的 vocab_size 会变小</strong>，因为预训练的 Word Embedding 包括 <code>,</code> <code>!</code> 类似的符号，而在 VocabularyProcessor 的处理代码中，会忽略所有非单词的符号，这就是为什么 vocab_size 变小的原因。</p><p>接下来就是通过 Tensorflow 的 <code>embedding_look_up</code> 进行 embedding 操作了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">embeddings_var = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[vocab_size_after_process, embedding_dim]), trainable=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 创建 Embedding 变量 </span></span><br><span class="line">embedding_placeholder = tf.placeholder(tf.float32, [vocab_size_after_process, embedding_dim]) <span class="comment"># 通过 Placeholder 喂给 graph</span></span><br><span class="line">embedding_init = embeddings_var.assign(embedding_placeholder) <span class="comment"># 初始化赋值</span></span><br><span class="line"></span><br><span class="line">embedding = np.asarray(embd) <span class="comment"># 将 list 变成 ndarray 便于喂给 graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(embedding_init, feed_dict=&#123;embedding_placehoder:embedding&#125;)</span><br><span class="line">  <span class="comment"># ... 其他代码</span></span><br></pre></td></tr></table></figure><p><strong>注意：<code>embedding_var</code> trainable 属性设置为<code>False</code>（已经预训练好了）； 以及这里的 vocab_size 建议采用 after process 的较小的值，避免输入中没有符号而 Embedding Weight 中有符号，造成的计算、空间资源的浪费。</strong></p><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><p>打乱训练集也是我们经常需要做的，避免同种 label 的数据大量的出现，我们处理的数据常常是 ndarray 或者是 pandas 的 Series，这里就介绍两个 shuffle 的函数：</p><p>首先是 numpy 的，通过 np.random.shuffle 打乱数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.shuffle(train_set)</span><br><span class="line">np.random.shuffle(test_set)</span><br></pre></td></tr></table></figure><p>再就是 Pandas Series 的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = train.sample(frac=<span class="number">0.3</span>)</span><br><span class="line">test = test.sample(frac=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p><code>Series.sample(frac)</code> 可以起到 Shuffle 的作用，并且还能够通过 frac 参数来指定采样的比例，如果我们希望只用 0.3 的数据，就可以指定 <code>frac = 0.3</code>，来实现这一目的，这使用起来非常方便。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近做 Sentiment Analysis 的问题，用 IMDB，Twitter 等 Dataset，拿到原始的一条条文本，直接喂给 Model 肯定不行，需要进行对文本进行预处理。预处理的精细程度很大程度上也会影响模型的性能。这篇 Blog 就记录一些预处理的方法。&lt;/p&gt;
&lt;h2 id=&quot;Remove-Stop-Words&quot;&gt;&lt;a href=&quot;#Remove-Stop-Words&quot; class=&quot;headerlink&quot; title=&quot;Remove Stop Words&quot;&gt;&lt;/a&gt;Remove Stop Words&lt;/h2&gt;&lt;p&gt;​    Stop Words，也叫停用词，通常意义上，停用词大致分为两类。一类是人类语言中包含的功能词，这些功能词极其普遍，与其他词相比，功能词没有什么实际含义，比如’the’、’is’、’at’、’which’、’on’等。另一类词包括词汇词，比如’want’等，这些词应用十分广泛，但是对这样的词搜索引擎无法保证能够给出真正相关的搜索结果，难以帮助缩小搜索范围，同时还会降低搜索的效率，所以通常会把这些词从问题中移去，从而提高搜索性能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="TensorFlow" scheme="https://tobiaslee.top/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Some Feelings</title>
    <link href="https://tobiaslee.top/2017/11/01/Some-Feelings/"/>
    <id>https://tobiaslee.top/2017/11/01/Some-Feelings/</id>
    <published>2017-11-01T13:26:32.000Z</published>
    <updated>2017-11-01T15:31:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>起标题的时候想了很久，一开始想用“大二上”这样的标题，但一想，大二上也才过去半个学期，可能到期末的时候再写比较合适吧；然后也想用“随笔/感想”这个标题，觉得略 low，想了想还是用英文名吧23333。</p><h2 id="Feelings"><a href="#Feelings" class="headerlink" title="Feelings"></a>Feelings</h2><h3 id="大学的节奏"><a href="#大学的节奏" class="headerlink" title="大学的节奏"></a>大学的节奏</h3><p>应该是已经适应了大学生活的节奏了吧，开学之后就按部就班的上课，每天过着两点一线的生活，偶尔出去溜达一圈吃个饭。和大一的不同就是课变少了，有大块的时间空出来，期中考试也只要考一门离散数学，让我有一种上了假大学的错觉。</p><p>作业也都在按时地完成，但始终觉得学的不像上学期一样认真了，可能是自认为摸清了套路吧：平时老老实实上课，写作业，考试前突击刷一波卷子，成绩都还能过得去。似乎确实是这样的。在我看来，大学课程的意义，就是<strong>告诉你有这样的一样东西，学校教给你关于它的一些基本的知识，等到你要用的时候，知道去哪里找到它，再捡起来就好</strong>。上学期学的高数、线代，现在脑子里已经没剩下多少了，但能让我看懂论文里的公式和符号，知道公式背后的 intuition，大概这就是这些课之于我的意义吧。</p><p>所以就感觉，大学生活变得无比无趣，像是上了轨的列车，一眼就能看到终点，却要等待漫长的时间。特别希望有个时光机器能够直接穿梭到四年之后，过那个时候的生活。明知是痴心妄想，但请允许痴人说梦。<br><a id="more"></a></p><h3 id="吃鸡"><a href="#吃鸡" class="headerlink" title="吃鸡"></a>吃鸡</h3><p>吃鸡这个游戏也已经包围了我们寝室，不得不说这个游戏无论是对玩家还是旁观者，都有很强的代入感，容易让人沉浸其中。要不是我的电脑带不了，估计我也会入迷吧。毕竟生活这么无趣，这游戏又是如此刺激，当屏幕上出现“大吉大利，今晚吃鸡”的字眼的时候是多么振奋人心。</p><p>记得在某公号看到一篇文章，讲王者荣耀为什么会让人上瘾，就是因为其回报很快，击杀的快感是及时反馈的，一场游戏半个小时以内就能分出胜负。这就和学习一样东西截然不一样，无论是基本的数理课程，还是一门语言的使用，亦或是对某种技能的掌握，都需要少则几个月多则几年甚至几十年的投入，这意味着回报周期是很长的，虽然其回报可能是巨大。<strong>人们总是希望欲望尽快得到满足，所以他们总会倾向于做那些能够迅速得到回报的活动</strong>，所以有那么多人孜孜不倦地玩着王者荣耀、守望先锋（已经过期？），又或者是现在大红大紫的吃鸡。</p><p>但是啊，我们总是要看远一点，吃鸡不能让你以后天天吃得上鸡（虽然少数主播可能能做到吧），看别人吃鸡就别说了。有种别人吸烟，而我主动去吸二手烟的感觉。</p><h3 id="健康"><a href="#健康" class="headerlink" title="健康"></a>健康</h3><p>最近身体也状况频出，不像上学期那么生龙活虎了，上周末的肠炎到今天才感觉有所好转。应该是每天大量时间坐在寝室缺乏锻炼所致。打算买个新椅子？又或是办张健身卡？想每天晚上去跑步但是西安这个天气以及不能洗澡的问题让我很是犹豫。这真是个问题啊。</p><h3 id="情感"><a href="#情感" class="headerlink" title="情感"></a>情感</h3><p>既然你看不见我，那我也就当做看不见你吧。一直觉得需要找一个能够平视甚至是俯视我的人，来敦促我不断前进，但感觉现在是找不到啦。那就拉倒。</p><h2 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h2><p>把从暑假开始做的科项目研申请了国创，老师也很爽快地给了答复，现在就希望能够把剩下的部分顺利做完，遇到困难也不怕，有信心能拿下！</p><p>另外就是数模啦，期中考后有一个小比赛，再就是美赛了，队友都很强，我也要好好努力，把握住机会！</p><p>昨天还有人在朋友圈发质疑鸡汤，觉得都是用来骗人推进 GDP 的，但我天天给自己喝鸡汤、打鸡血，算是很正能量了。</p><blockquote><p>盖所以为精金者，在足色，而不在分两。所以为圣者，在纯乎天理，而不在才力也。故虽凡人，而肯为学，使此心纯乎天理，则亦可为圣人。犹一两之金，比之万镒，分两虽悬绝，而其到足色处，可以无愧。故曰‘人皆可以为尧舜’者以此。</p></blockquote><p>做自己的尧舜吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;起标题的时候想了很久，一开始想用“大二上”这样的标题，但一想，大二上也才过去半个学期，可能到期末的时候再写比较合适吧；然后也想用“随笔/感想”这个标题，觉得略 low，想了想还是用英文名吧23333。&lt;/p&gt;
&lt;h2 id=&quot;Feelings&quot;&gt;&lt;a href=&quot;#Feelings&quot; class=&quot;headerlink&quot; title=&quot;Feelings&quot;&gt;&lt;/a&gt;Feelings&lt;/h2&gt;&lt;h3 id=&quot;大学的节奏&quot;&gt;&lt;a href=&quot;#大学的节奏&quot; class=&quot;headerlink&quot; title=&quot;大学的节奏&quot;&gt;&lt;/a&gt;大学的节奏&lt;/h3&gt;&lt;p&gt;应该是已经适应了大学生活的节奏了吧，开学之后就按部就班的上课，每天过着两点一线的生活，偶尔出去溜达一圈吃个饭。和大一的不同就是课变少了，有大块的时间空出来，期中考试也只要考一门离散数学，让我有一种上了假大学的错觉。&lt;/p&gt;
&lt;p&gt;作业也都在按时地完成，但始终觉得学的不像上学期一样认真了，可能是自认为摸清了套路吧：平时老老实实上课，写作业，考试前突击刷一波卷子，成绩都还能过得去。似乎确实是这样的。在我看来，大学课程的意义，就是&lt;strong&gt;告诉你有这样的一样东西，学校教给你关于它的一些基本的知识，等到你要用的时候，知道去哪里找到它，再捡起来就好&lt;/strong&gt;。上学期学的高数、线代，现在脑子里已经没剩下多少了，但能让我看懂论文里的公式和符号，知道公式背后的 intuition，大概这就是这些课之于我的意义吧。&lt;/p&gt;
&lt;p&gt;所以就感觉，大学生活变得无比无趣，像是上了轨的列车，一眼就能看到终点，却要等待漫长的时间。特别希望有个时光机器能够直接穿梭到四年之后，过那个时候的生活。明知是痴心妄想，但请允许痴人说梦。&lt;br&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Adversarial Papers 脉络梳理</title>
    <link href="https://tobiaslee.top/2017/10/26/Adversarial-Papers-%E8%84%89%E7%BB%9C%E6%A2%B3%E7%90%86/"/>
    <id>https://tobiaslee.top/2017/10/26/Adversarial-Papers-脉络梳理/</id>
    <published>2017-10-26T11:22:52.000Z</published>
    <updated>2017-10-29T02:26:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>Adversarial Training 是如今深度学习中非常重要的一种训练方法，这段时间读了这方面的一些 Paper，在此做些梳理，帮助理清思路。<br><a id="more"></a></p><h2 id="Intriguing-properties-of-neural-networks"><a href="#Intriguing-properties-of-neural-networks" class="headerlink" title="Intriguing properties of neural networks"></a>Intriguing properties of neural networks</h2><p>这篇 Szegedy 在 2013 年的 Paper，如题目所说，阐述了神经网络的两个 intrigue 的属性：神经网络的空间含有语义信息（It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks）；另一个则就是我们提到的 Adversarial Training，其第一次（存疑）提到了神经网络对于输入的扰动（perturbation）非常敏感，可以通过对输入增加扰动来形成对抗样本（adversarial examples），导致模型做出错误的分类。</p><h2 id="Explaining-and-harnessing-adversarial-examples"><a href="#Explaining-and-harnessing-adversarial-examples" class="headerlink" title="Explaining and harnessing adversarial examples"></a>Explaining and harnessing adversarial examples</h2><p>Goodfellow 在这篇 Paper 中对 Adversarial Examples 的原理以及危害做了进一步的阐释，并且提出了一种计算扰动和利用对抗样本进行训练，从而提升模型鲁棒性的方法。并且，Goodfellow 认为 Adversarial Training 实际上是一种 Regularization 的手段，并且性能优于 Dropout。</p><p>为什么小小的扰动会产生巨大的影响：Feature 的精度有限，如果扰动 η 小于精度的话，模型的预测不应该出现误差。但因为我们的模型存在一个 W （NN 习得的权重），而 W <em> x_perturbation = W </em> x + W * η ，那么扰动就会被放大，而且空间维度越高，其产生的影响就越大。</p><p>如何计算扰动： fast gradient sign method，计算 loss 函数对 x 的导数取 sign（为了数值上的 smooth），再乘上一个我们设定的扰动大小 norm 即可。</p><h2 id="The-limitations-of-deep-learning-in-adversarial-settings"><a href="#The-limitations-of-deep-learning-in-adversarial-settings" class="headerlink" title="The limitations of deep learning in adversarial settings"></a>The limitations of deep learning in adversarial settings</h2><p>Paper 提出了 Adversarial Samples（Examples） 所需要达到目标，并进行分级。</p><p>Adversarial Goals:</p><ol><li>Confidence Reduction：降低 model 对输出的信心，导致分类 Ambiguity</li><li>Misclassification：错误分类，model 无法正确输出 input 的 label</li><li>Targeted Misclassification：指定错误分类，产生样本使得模型能够输出指定的错误 label</li><li>Source/target misclassification：使得一类输入都产生同一种输出</li></ol><p>也对产生 Adversarial Samples 方法，根据所需知识进行了分级:</p><ol><li>Training data and network architecture：这种方法知道 model 的输入、模型的架构和参数以及 loss 的设置，这是最强的一种 adversarial</li><li>Network architecture：能够知道 model 的架构和参数，足以来模拟模型</li><li>Training Data：能够从 input data 中采样一部分数据来生成对抗样本，但是不知道模型的架构</li><li>Oracle：仅仅能够通过一个模型的代理来获知模型输入和输出，能通过改变输入来观察所引起的输出的改变（作者打了个比方：在密码学里，能够看到明文和解密之后的文本，以及对应的变化）</li><li>Samples：只能看到输入和输出这样的 pair 数据，不能和 Oracle 一样来尝试改变 input 获取 output 的变化</li></ol><p>还提出了一种和 Goodfellow 不一样的 Adversarial Training 方法，通过 forward derivative（Jacobian，directly providing gradients of the output components with respect to each input component），来产生 Adversarial Examples。</p><h2 id="Crafting-Adversarial-Input-Sequences-for-Recurrent-Neural-Networks"><a href="#Crafting-Adversarial-Input-Sequences-for-Recurrent-Neural-Networks" class="headerlink" title="Crafting Adversarial Input Sequences for Recurrent Neural Networks"></a>Crafting Adversarial Input Sequences for Recurrent Neural Networks</h2><p>在上一篇文章的基础上，将计算 Adversarial Examples 的方法由图像领域迁移到文本领域，对 RNN （LSTM）产生 Adversarial Sequence。</p><p>通过替换输入句子的第 i 个词，替换的要求有：</p><ol><li>替换词来自我们的字典 D</li><li>替换词和原词的不同 （通过 sgn 函数来衡量）和 sgn（ 模型输出 logits 对 原词的偏导数），尽可能接近</li></ol><p>上面的偏导数的含义：</p><blockquote><p>This gives us a precise mapping between changes made to the word embeddings and variations of the output of the pooling layer</p></blockquote><p>结果: 在 Sentiment Classification 中，对于平均长度为 71.06 的句子，平均替换 9.18 个词语能够达到使原 model 完全无法正确预测（error 100 %）</p><h2 id="Distillation-as-a-defense-to-adversarial-perturbations-against-deep-neural-networks"><a href="#Distillation-as-a-defense-to-adversarial-perturbations-against-deep-neural-networks" class="headerlink" title="Distillation as a defense to adversarial perturbations against deep neural networks"></a>Distillation as a defense to adversarial perturbations against deep neural networks</h2><p>提出了一种防御机制“Defense Distillation”来降低 Adversarial Examples 对 DNN 的影响。</p><p>该机制核心思想是先通过一个 NN 来产生一个 Probablity Vector 作为第二个 NN 的输入 Label，其中包含的概率信息能够起到以下作用：</p><blockquote><p>Training a network with this explicit relative information about classes prevents models from fitting too tightly to the data, and contributes to a better generalization around training points.</p></blockquote><p>作者举了 MNIST 的例子：如果数字是 7，那么对应的 label 就是 [0, 0, 0, 0 ,0 , 0, 0, 1,0,0]，而我们的 probabilibty vector 则可能是 在 1 上是 0.4 在 7 上是 0.6，这就给我们的第二个 NN 一种提示，1 和 7 间有某种联系，他会尝试学到一些 1 和 7 共同的结构（比如那一竖），从而避免过于相信数字是 7，而在一些写的比较潦草的数字上做出错误的判断。</p><p>这样，第二个模型得到的 F_star 就能够对 Adversarial Samples 更加 Robust，但是在一部分模型上会带来一部分 accuracy 的损失。</p><p>作者还提出了产生 Adversarial Samples 的 Framework:</p><ol><li>Direction Sensitivity Estimation: evaluate the sensitivity of class change to each input feature 能够找到一个 data mainfold 上导致 class change 的最为敏感的方向</li><li>Perturbation Selection : use the sensitivity information to select a perturbation delta-X among the input dimensions</li></ol><p>定义了 DNN Robustness :</p><ol><li>Display good accuracy inside and outside of its training dataset </li><li>model a smooth clssifier function F which would intuitively classify inputs relatively consistently in the neighborhood</li></ol><p>衡量 Robustness 的一个 metric：使分类器产生不同分类结果所需进行扰动的平均大小。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Adversarial Training 是如今深度学习中非常重要的一种训练方法，这段时间读了这方面的一些 Paper，在此做些梳理，帮助理清思路。&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Adversarial Training Methods For Semi-Supervised Text Classification 阅读笔记</title>
    <link href="https://tobiaslee.top/2017/10/16/Adversarial-Training-Methods-For-Semi-Supervised-Text-Classification-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://tobiaslee.top/2017/10/16/Adversarial-Training-Methods-For-Semi-Supervised-Text-Classification-阅读笔记/</id>
    <published>2017-10-16T01:52:26.000Z</published>
    <updated>2017-10-22T07:03:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇 ICLR 2017 刚出的 <a href="https://arxiv.org/abs/1605.07725" target="_blank" rel="noopener">Paper</a>，这篇 blog 主要记录阅读过程中的一些想法，如有错误之处，欢迎联系指正。</p><h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><blockquote><p>We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. </p></blockquote><p>如 Paper 的摘要所说，这篇论文的核心思想就是：在 word embedding 上增加扰动(perturbation)，来增强模型的鲁棒性，从而提升分类器的准确率。</p><p>我对这一点理解是这样的：一段文本，在计算机的处理过程中通常是用一组 词 id 向量来表示的，两个词的 id 是无法体现词之间的关系的，比如 “Good” 和 “Nice”，两个单词是近义词，但是他们的 id 可能隔了十万八千里；而在图像中，也就是 RGB 值的分布，两个相近的颜色的 RGB 值就会非常接近。</p><p><img src="/img/rgb.png" alt="RGB"></p><p>我们可以在调色板上随意拖动，颜色就会连续的变化。</p><p>在文本领域则没有这样的性质，这就使得这个扰动非常难以定义，作者提出在 word embedding  层添加扰动，而不是在 input 层（图像处理就会在输入的图片上增加 noise 来达到扰动的目的），我觉得是非常聪明的。因为 word embedding 之后，词向量所在的空间可以近似地看成连续的：相同含义（甚至是不同含义）的词在 embedding 后的分布非常接近，增加一点点扰动，可能就能够恰好落在另一个同义词上或者是反义词上。</p><p>比如我们在做 Sentiment Classification 的任务，“我觉得这道菜很美味”和“我觉得这道菜很难吃”这两个句子唯一的不同就是最后的形容词，但是 sentiment 就是 positive 和 negative 两个标签，而“美味”和“难吃”在 word embedding 之后很有可能是非常相似的（直觉感受，没有验证）。我们增加扰动使得原来的“美味”变成“难吃”，如果 model 不能正确分类，就说明这个扰动（或者说 adversarial example，对抗样本)是 model 所不能够很好地适应的.文章里添加一个 adversarial loss，来衡量 model 对于扰动的适应能力，通过 minimize 这个 loss，来增强 model 对于对抗样本的能力，最终提升整体分类的准确率。</p><a id="more"></a><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h3><p><img src="/img/adv_example.png" alt="Adversarial Example"></p><p>这个例子我觉得非常形象地说明了 Adversarial Example，原本分类器比较自信（57%）地认为图片中是一直熊猫，但在给图片增加人眼都无法分辨的噪点之后，分类器却几乎肯定（99%）地认为图中是一只长臂猿。</p><p>那如何衡量模型对 Adversarial Example 的适应能力呢？可以额外定义一个 Cost Function：</p><p><img src="/img/adv_loss.png" alt="Adversarial Loss"></p><p>Paper 中，将上式 (adv_loss) 和原先分类器的 loss(cl_loss) 相加得到一个新的 total_loss，在训练过程中 minimize 这个 total_loss，就能够实现“Train classifier to be robust to the worst perturbation”的目的了。</p><h3 id="Worst-Case-Perturbation"><a href="#Worst-Case-Perturbation" class="headerlink" title="Worst Case Perturbation"></a>Worst Case Perturbation</h3><p>怎么样的扰动是 <strong>worst</strong> 的呢？我们称之为 r<sub>adv</sub>，worst 意味着模型最无法 fit ，也就意味它会使 adv_loss 最大，也就是满足下面这个式子：</p><p><img src="/img/r_adv.png" alt="r_adv"></p><p>但是因为在神经网络中这个 r 不可微分，所以我们采用一个替代的方案来近似计算 r<sub>adv</sub>：</p><p><img src="/img/alternative_radv.png" alt="Alternativer r_adv"></p><p>也就是我们计算一个 loss function 对输入 x —也就是经过 word embedding 后的句子向量—的梯度，乘一个负号和 eplison，再除上梯度的 L2 范数即可。</p><p>这个方法是大牛 Goodfellow 提出来的（文章还有待阅读），我的理解是这样的：</p><p>梯度方向是 loss 下降最快的方向，那么梯度的反向（负号），也就是 loss 上升最快的方向，换而言之，就是<strong>我们模型最害怕、最不能适应的一个方向</strong>。再经过除以 L2 范数，相当于得到一个单位向量，乘以 epsilon（我们可以通过它来控制扰动的大小），得到我们模型最害怕的一个扰动，也就是 r<sub>adv</sub>。</p><h3 id="Virtual-Adversarial-Training"><a href="#Virtual-Adversarial-Training" class="headerlink" title="Virtual Adversarial Training"></a>Virtual Adversarial Training</h3><p>前面的所提及的都是 Supevised-Learning 情况下的（loss 函数的设置），事实上文本领域还存在大量没有 label 的数据，能不能把这个方法迁移到 Semi-Supervised 情况下呢？答案是：Yes。</p><p>没有 label 的情况下，我们通过衡量扰动前后分布的 KL Divergence 来确定 r<sub>adv</sub>：</p><p><img src="/img/semi_adv.png" alt="Virtual Adverarial Training"></p><p>用类似的方法来近似计算 r<sub>adv</sub>：</p><p><img src="/img/semi_radv.png" alt="Semi-R_adv"></p><p>但因为是 Semi-Supervised Learning，不是所有的样本都有 label，所以 loss 的定义也和普通的不同，因为我主要关注 Supervised Learning，所以这一部分没有细读，有待日后补充。</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>Paper 实现的代码 Google 已经开源在 <a href="https://github.com/tensorflow/models/tree/master/research/adversarial_text" target="_blank" rel="noopener">GitHub</a> 了，我把核心的框架提出来，基于原本的 Attention-based Bi-LSTM 来做的，主要在以下几个地方做了改动：</p><h3 id="Word-Normalization"><a href="#Word-Normalization" class="headerlink" title="Word Normalization"></a>Word Normalization</h3><p>Paper 中为了避免最后数学上的病态解，于是对 word embedding 做了一个 Normalization:</p><p><img src="/img/normalization.png" alt="Normalization"></p><p>在代码里的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(emb, weights)</span>:</span></span><br><span class="line">    weights = vocab_freqs / tf.reduce_sum(vocab_freqs)</span><br><span class="line">    mean = tf.reduce_sum(weights * emb, <span class="number">0</span>, keep_dims=<span class="keyword">True</span>)</span><br><span class="line">    var = tf.reduce_sum(weights * tf.pow(emb - mean, <span class="number">2.</span>), <span class="number">0</span>, keep_dims=<span class="keyword">True</span>)</span><br><span class="line">    stddev = tf.sqrt(<span class="number">1e-6</span> + var)</span><br><span class="line">    <span class="keyword">return</span> (emb - mean) / stddev</span><br></pre></td></tr></table></figure><p>因为我是随着模型一起训练 word embedding 的，没有 pretrain，所以在每次训练中都要对 embedding 做一次 normalization。</p><h3 id="Add-Perturabation"><a href="#Add-Perturabation" class="headerlink" title="Add Perturabation"></a>Add Perturabation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scale_l2</span><span class="params">(x, norm_length)</span>:</span></span><br><span class="line">    <span class="comment"># shape(x) = (batch, num_timesteps, d)</span></span><br><span class="line">    <span class="comment"># Divide x by max(abs(x)) for a numerically stable L2 norm.</span></span><br><span class="line">    <span class="comment"># 2norm(x) = a * 2norm(x/a)</span></span><br><span class="line">    <span class="comment"># Scale over the full sequence, dims (1, 2)</span></span><br><span class="line">    alpha = tf.reduce_max(tf.abs(x), (<span class="number">1</span>, <span class="number">2</span>), keep_dims=<span class="keyword">True</span>) + <span class="number">1e-12</span></span><br><span class="line">    l2_norm = alpha * tf.sqrt(</span><br><span class="line">        tf.reduce_sum(tf.pow(x / alpha, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>), keep_dims=<span class="keyword">True</span>) + <span class="number">1e-6</span>)</span><br><span class="line">    x_unit = x / l2_norm</span><br><span class="line">    <span class="keyword">return</span>  norm_length * x_unit</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_perturbation</span><span class="params">(embedded, loss)</span>:</span></span><br><span class="line">    <span class="string">"""Adds gradient to embedding and recomputes classification loss."""</span></span><br><span class="line">    grad, = tf.gradients(</span><br><span class="line">        loss,</span><br><span class="line">        embedded,</span><br><span class="line">     aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)</span><br><span class="line">    grad = tf.stop_gradient(grad)</span><br><span class="line">    perturb = scale_l2(grad, epsilon)</span><br><span class="line">    <span class="keyword">return</span> embedded + perturb</span><br></pre></td></tr></table></figure><p>我们通过计算梯度（Google 的代码这里没有乘以 -1 作为反向？），在通过 scale_l2 进行对梯度进行求单位向量再乘以 epsilon 得到扰动， scale_l2 就是一个数值稳定（避免 0 为除数）的放缩函数。 </p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p><code>loss = ad_loss + cl_loss</code>：我们先把未增加扰动的 word embedding 交给 Bi-LSTM 得到一组 logits，计算 cl_loss；再将增加扰动后的 embedding_perturbated 交给 Bi-LSTM 得到另一组 logits，计算 ad_loss。二者相加得到我们需要优化的 total_loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logits, cl_loss = cal_loss_logit(batch_embedded, keep_prob, reuse=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">   embedding_perturbated = add_perturbation(batch_embedded, cl_loss)</span><br><span class="line">   </span><br><span class="line">   ad_logits, ad_loss = cal_loss_logit(embedding_perturbated, keep_prob, reuse=<span class="keyword">True</span>)</span><br><span class="line">   </span><br><span class="line">   loss = cl_loss + ad_loss</span><br></pre></td></tr></table></figure><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>因为计算量特别大（计算梯度），所以一开始只用了 1/5 的数据，但也花了 7 个小时，足足是无扰动情况的 20 倍（20分钟）。可能实现上还有一些问题，但是在相同参数和训练步数（大约 5 个 epoch）情况下，Adversarial Trainning 的准确率确实高了 1%（95.69 % 和 94.59 %）。</p><p>随后我用 AWS 租了一个 p2 instance（ Tesla K80 11G) 来跑全部的数据集，同样是 5 个 epoch 的情况下达到了 98.5%的准确率（接近 2 hours），比 ABBLSTM （i7 1 hour) 高了 0.2 %，<strong>效果有，但是很小</strong>。</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><p>我觉得表现不是非常好的原因如下：</p><ol><li>精度没有达到 Paper 提出的 99.3%，可能是训练次数不够，仅仅是 5 个 epoch(Update: 训练 50个 epoch 也是 98.5% 排除这个原因)</li><li>没有预先训练 embedding，而是和 Bi-LSTM 一起训练，每次都要重新进行 Normalization，训练速度慢，而且 Adversarial Example 对于 embedding 的依赖很大 所以导致效果也可能不是显著。</li><li>没有采用文章所采用的一些 Training 的 Trick，比如 gradient clip，loss 退火等</li><li>超参数没有经过 fine-tuning，比如 epsilon 选的是适合 IMDB dataset 的值，对于 DBpedia 还需要调整。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇 ICLR 2017 刚出的 &lt;a href=&quot;https://arxiv.org/abs/1605.07725&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper&lt;/a&gt;，这篇 blog 主要记录阅读过程中的一些想法，如有错误之处，欢迎联系指正。&lt;/p&gt;
&lt;h2 id=&quot;Idea&quot;&gt;&lt;a href=&quot;#Idea&quot; class=&quot;headerlink&quot; title=&quot;Idea&quot;&gt;&lt;/a&gt;Idea&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如 Paper 的摘要所说，这篇论文的核心思想就是：在 word embedding 上增加扰动(perturbation)，来增强模型的鲁棒性，从而提升分类器的准确率。&lt;/p&gt;
&lt;p&gt;我对这一点理解是这样的：一段文本，在计算机的处理过程中通常是用一组 词 id 向量来表示的，两个词的 id 是无法体现词之间的关系的，比如 “Good” 和 “Nice”，两个单词是近义词，但是他们的 id 可能隔了十万八千里；而在图像中，也就是 RGB 值的分布，两个相近的颜色的 RGB 值就会非常接近。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/rgb.png&quot; alt=&quot;RGB&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们可以在调色板上随意拖动，颜色就会连续的变化。&lt;/p&gt;
&lt;p&gt;在文本领域则没有这样的性质，这就使得这个扰动非常难以定义，作者提出在 word embedding  层添加扰动，而不是在 input 层（图像处理就会在输入的图片上增加 noise 来达到扰动的目的），我觉得是非常聪明的。因为 word embedding 之后，词向量所在的空间可以近似地看成连续的：相同含义（甚至是不同含义）的词在 embedding 后的分布非常接近，增加一点点扰动，可能就能够恰好落在另一个同义词上或者是反义词上。&lt;/p&gt;
&lt;p&gt;比如我们在做 Sentiment Classification 的任务，“我觉得这道菜很美味”和“我觉得这道菜很难吃”这两个句子唯一的不同就是最后的形容词，但是 sentiment 就是 positive 和 negative 两个标签，而“美味”和“难吃”在 word embedding 之后很有可能是非常相似的（直觉感受，没有验证）。我们增加扰动使得原来的“美味”变成“难吃”，如果 model 不能正确分类，就说明这个扰动（或者说 adversarial example，对抗样本)是 model 所不能够很好地适应的.文章里添加一个 adversarial loss，来衡量 model 对于扰动的适应能力，通过 minimize 这个 loss，来增强 model 对于对抗样本的能力，最终提升整体分类的准确率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>GAN 学习笔记</title>
    <link href="https://tobiaslee.top/2017/10/03/GAN-Notes/"/>
    <id>https://tobiaslee.top/2017/10/03/GAN-Notes/</id>
    <published>2017-10-03T06:41:07.000Z</published>
    <updated>2017-10-19T09:26:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>Generative Adversarial Networks，简称 GAN，中文叫做生成式对抗网络，是现在非常火热的神经网络模型。网上关于 GAN 的 Demo 很多，讲解也很多，但都只是在 Intuition 层面的理解，很少涉及其中的数学细节，而我在阅读 Goodfellow 的论文时对其中的公式感到一头雾水，好在最终找到了国立台湾大学的李宏毅老师的公开课视频，这篇文章就是他深度学习课程 GAN 部分的学习笔记。</p><h2 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h2><p>GAN 主要由两个模块组成，Generator（G） 和 Discriminator（D）。有个很形象的比喻就是，G 是一个造假币的，D 是一个验钞机。G 的目的就是要让 D 无法辨别自己的造出的假币，因此 G 就要不断提升自己造假币的技术，尽可能地以假乱真；而 D 自然不能轻易地让 G 得逞，于是也要不断地提升自己鉴别假钞的能力。两方这样对抗式地提升着自己，最终我们就能够得到一个造假币技术很强的 G，这就能可以应用在很多领域之中，比如图像生成，我们就能够生成逼真到足以欺骗人眼的图片，这么好的效果也难怪 GAN 如此火热了。实现的方式也很简单，我们用 NN 来做我们的 Generator 和 Discriminator（Discriminator 其实是一个 Binary Classifier），然后把他们接起来（即把 Generator 产生的 data 作为 input 交给 Discriminator），再利用 Gradient Descent 的方法来训练，就可以达到我们想要的结果了。关于 Intuition 的部分就写这么多，接下来就带着这种理解进入到数学的部分啦。<br><a id="more"></a></p><h2 id="Basic-Idea-Of-GAN"><a href="#Basic-Idea-Of-GAN" class="headerlink" title="Basic Idea Of GAN"></a>Basic Idea Of GAN</h2><h3 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h3><p><img src="/img/mle.png" alt="Maximum Likelihood Estimation"></p><p>Generator 实际在做的事情就是，对于给定样本分布 P<sub>data</sub>(x)，我们希望的我们 P<sub>G</sub>(x;θ），能够尽可能地接近 P<sub>data</sub>(x)，这里的 θ 就是控制 G 的参数，如果是神经网络的话，对应就是各层的 Weights。衡量接近的指标就是上面的 L，其含义就是如果我们从 P<sub>data</sub>(x) 中 采样 m 个 x_i，那么在给定 θ 的情况下，我们可以计算出从 P<sub>G</sub> 中 采样出 x_i 的几率，将它们连乘得到 L。我们的目的就是找到一个 θ_star，使得 L 能够最大。</p><p><img src="/img/theta.png" alt="解 Theta"></p><p>上面就是求解 θ_star 的过程，运用了一系列数学的技巧：</p><ol><li>先取 log 将连乘转化成连加，其结果约等于从 P<sub>data</sub> 中采样 x，再计算其 log 之和（其中 E<sub>x~P<sub>data</sub></sub> 的含义是从 P<sub>data</sub> 中采样 x）</li><li>把求期望值改成取遍所有 x，转化成一个积分问题</li><li>接下来再非常有技巧地（我个人看来）在后面减去一项只跟 P<sub>data</sub>有关而与 θ 无关（不影响结果）的积分，再将被积函数中的 log 部分相减得到一个式子</li><li>最后再将 log 部分上下颠倒，变成最小化问题，其最小化的目标就是 P<sub>data</sub> 和 P<sub>G</sub> 的KL Divergence，KL Divergence 是用来衡量两个分布的相似程度的指标</li></ol><h3 id="General-PG-——-Nerual-Networks"><a href="#General-PG-——-Nerual-Networks" class="headerlink" title="General PG —— Nerual Networks"></a>General P<sub>G</sub> —— Nerual Networks</h3><p><img src="/img/pgnn.png" alt="Pg is NN"></p><p>如果我们的分布 P<sub>G</sub> 是一个很普通的分布函数比如说高斯分布，那他不可能接近现实生活中如此丰富的数据的分布。而 GAN 带给我们非常重要的一个 idea 就是我们可以利用 NN（ Nerual Networks） 来作为 P<sub>G</sub>。因为理论上只要 NN 足够复杂，他是可以拟合任何函数的。在这里 NN 的输入是一个低维的 vector，<strong>如果这个 vector 是来自一个 distribution 的话，那么 NN 的输出也可以看做是一个 distribution</strong>，这样理论上我们可以通过控制 NN 的 weights，来产生任何分布了。我曾经在看 Demo 代码时对于 G 的输入是一个随机分布非常不解，现在有种醍醐灌顶之感。</p><p>利用 NN 来做 P<sub>G</sub>，它的表示就如上图所示，其中 P<sub>prior</sub> 是一个先验分布，就是我们的随机分布 z 的分布（比如是一个高斯分布），再乘上一个 Identify 函数（如果 G(z)=x 则为 1，否则为 0）。</p><p>但是这又带来了一个问题，L 现在变得非常难以计算，这样我们怎么才能通过调整 θ 来得到我们想要的 G呢，GAN 的重大贡献就在于提出了这个问题的解决方法：引入 Discriminator。</p><h2 id="V-G-D-的优化问题"><a href="#V-G-D-的优化问题" class="headerlink" title="V (G, D) 的优化问题"></a>V (G, D) 的优化问题</h2><p><img src="/img/VGD.png" alt="V(G, D)"></p><p>这里首先需要找到 D_star 使得 V(G, D) 最大，再在给定 D_star 的条件下，找到 G_star 使得 V(G, D) 最小。</p><p><img src="/img/maxVGD.png" alt="max V"></p><p>V(G, D) 的定义:</p><p>V =  E<sub>x~P<sub>data</sub></sub>[log D(x)] + E<sub>x~P<sub>G</sub></sub>[log (1-D(x))]</p><p>利用同样的技巧将求期望转化为求积分，如果假定 D(x) 可以取任意值，那么我们要求解 D_star 使得 V(G, D) 最大，只要使被积函数最大即可。</p><p>此时，P<sub>data</sub>(x) 是给定的分布，我们可以用 a 来表示，同理，对于给定的 G，我们可以用 b 来表示 P<sub>G</sub>(x)，求导解得驻点：</p><p><strong>D_star = a / a + b</strong></p><p>接下来把求得的 D_star 带入 V(G, D)，并且做一些数学上的变换（分子分母同时除以2，再拿出 log 1/2），得到结果:</p><p><img src="/img/JSD.png" alt="JS Divergence"></p><p>JSD 指的是 <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank" rel="noopener">Jensen–Shannon divergence</a>，同样是衡量分布相似程度的一个指标</p><p>也就是说在这种 V 的定义下，我们其实是在求他们的 JSD，JSD 在两个 distribution 完全没有交集的时候等于 log 2，而在完全相同的时候等于 0，也就是说 ：</p><p>V<sub>max</sub> = 0     V<sub>min</sub> = -log 4</p><p>那我们要找一个 G_star 使得 V 最小，也就是要让：</p><p>P<sub>G</sub>(x) = P<sub>data</sub>(x)</p><p>这里就达到了我们一开始说的造假币来以假乱真的目的了，<strong>我们的 Generator 产生的 distribution 和真实 data 的 distribution 完全一致</strong>。</p><p>那么怎么解 G_star 呢？如果我们把 max V(G, D) 这一部分看成是 L(G)，那么要使得 L(G)最小，就用Gradient Descent ！不过还有一个小问题，就是我们的函数中存在 max，如何求梯度呢？这个不难解决：<strong>分段函数，落在哪段函数区间则用哪段区间的微分就行</strong>。</p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p><img src="/img/algorithm.png" alt="Algorithm"></p><p>理论上的算法大致就如上图所示，不断地进行迭代即可。但是有一个问题，就是不能保证在下一次迭代中的 JSD 一定小于上一次，我们能做的也就是要求学习率比较小，尽可能避免出现 JSD 反而变大的情况。</p><p>但是在实现中呢，因为我们不能求期望值和积分，所以只能从真实样本中 sample 中 m 个样本，以 m 个样本作为全部的样本空间，再用一个 V_hat 代替 V：</p><p><img src="/img/practice.png" alt="practice"></p><p>这个 V_hat 看起来非常的像 Cross Entropy 函数，要 Maxmize V，只要 Minimize -V 即可，我们把 -V 记作 L，对应我们一般模型中的 loss，那么就相当于我们要训练一个 Binary Classifier，来分辨数据是来自P<sub>data</sub> 还是 P<sub>G</sub>。</p><p><img src="/img/real_algorithm.png" alt="Real Implementation"></p><p>最后则是如上更为详细的实现过程，但仍然存在一个小问题，因为我们如果可以画出 log(1-D(x)) 的图像，我们会发现一开始的时候斜率很小，不利于学习，所以一般会<strong>将 log(1-D(x)) 改换为 -log(D(x))</strong> 来更好的加快训练，但这样需要使得给 Generator 生成的数据打上 positive 的标签（这个细节有待日后验证）。</p><h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><h3 id="Discriminator-Loss"><a href="#Discriminator-Loss" class="headerlink" title="Discriminator Loss"></a>Discriminator Loss</h3><p>如果我们去训练我们的 Discriminator，也就是一个 Classifier，会发现它的准确率高的可怕，几乎是 100 %，即 Discriminator Loss 基本为 0。其原因主要有两个：</p><ol><li>实现上的原因：我们的采样不是全部的样本，导致两个分布实际上存在交集的分布“看上去”没有交集，而使得我们的 JSD 常常为 log2。如果我们要通过削弱 Classifier 来解决这个问题，就会导致参数非常难调整；而另一方面要想衡量 JSD，又要求 Discriminator 很强才行，这就存在了矛盾。</li><li>数据本身：P<sub>G</sub> 和 P<sub>data</sub> 本身是高维空间中的 mainfolds（流形），比如是两条曲线，其本身的交集很小，所以 JSD 通常接近 log 2，这就导致我们的算法没有足够的动力去进化。解决的方法就有 Add Noise，让 P<sub>G</sub> 和 P<sub>data</sub>  有更多的重叠的部分。比如在 Discriminator 的输入中增加 Noise，或者给输入的标签进行更换。这相当于增加了曲线的宽度，使得他们有更大部分的重叠，减小 JSD。当然这个 Noise 一般是会 decay over time。</li></ol><h3 id="Mode-collapse"><a href="#Mode-collapse" class="headerlink" title="Mode collapse"></a>Mode collapse</h3><p><img src="/img/mode_collapse.png" alt="Mode Collapse"></p><p>我们的模型会趋于保守，而无法 Cover 部分的 Distribution，在实际应用中比如图片生成，模型会趋向于生成大量重复的图片，而不愿意尝试生成的新的图片，因为这可能会带来巨大的误差。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>基本照着李宏毅老师的视频记录下了这么多内容，算是对 GAN 有一个初步的了解。但 GAN 存在着很多问题，因而也存在着许多的变种来解决这些问题。还要学习一个啊！</p><p>PS：李宏毅老师台湾腔太萌了：哼，我有写错一个地方你们有发现吗，哈哈哈哈笑死我了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Generative Adversarial Networks，简称 GAN，中文叫做生成式对抗网络，是现在非常火热的神经网络模型。网上关于 GAN 的 Demo 很多，讲解也很多，但都只是在 Intuition 层面的理解，很少涉及其中的数学细节，而我在阅读 Goodfellow 的论文时对其中的公式感到一头雾水，好在最终找到了国立台湾大学的李宏毅老师的公开课视频，这篇文章就是他深度学习课程 GAN 部分的学习笔记。&lt;/p&gt;
&lt;h2 id=&quot;Intuition&quot;&gt;&lt;a href=&quot;#Intuition&quot; class=&quot;headerlink&quot; title=&quot;Intuition&quot;&gt;&lt;/a&gt;Intuition&lt;/h2&gt;&lt;p&gt;GAN 主要由两个模块组成，Generator（G） 和 Discriminator（D）。有个很形象的比喻就是，G 是一个造假币的，D 是一个验钞机。G 的目的就是要让 D 无法辨别自己的造出的假币，因此 G 就要不断提升自己造假币的技术，尽可能地以假乱真；而 D 自然不能轻易地让 G 得逞，于是也要不断地提升自己鉴别假钞的能力。两方这样对抗式地提升着自己，最终我们就能够得到一个造假币技术很强的 G，这就能可以应用在很多领域之中，比如图像生成，我们就能够生成逼真到足以欺骗人眼的图片，这么好的效果也难怪 GAN 如此火热了。实现的方式也很简单，我们用 NN 来做我们的 Generator 和 Discriminator（Discriminator 其实是一个 Binary Classifier），然后把他们接起来（即把 Generator 产生的 data 作为 input 交给 Discriminator），再利用 Gradient Descent 的方法来训练，就可以达到我们想要的结果了。关于 Intuition 的部分就写这么多，接下来就带着这种理解进入到数学的部分啦。&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="GAN" scheme="https://tobiaslee.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>调参侠的自我修养</title>
    <link href="https://tobiaslee.top/2017/09/17/%E8%B0%83%E5%8F%82%E4%BE%A0%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"/>
    <id>https://tobiaslee.top/2017/09/17/调参侠的自我修养/</id>
    <published>2017-09-17T01:22:56.000Z</published>
    <updated>2017-10-03T07:31:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>昨天刷完了 Andrew Ng 深度学习的第二部分课程</p><blockquote><p>Improving Deep Neural Networks: Hyperparameter tuing, Regularization and Optimization</p></blockquote><p>收获还是非常大的，虽然 Andrew Ng 对一些 tricks 的数学细节一笔带过，但通过画图之类的手段来让我获得了一个很形象的 intuition（直观感受），后面数学的部分也要靠自己再去啃书了。这篇 Blog 主要就是对课程内容的一个梳理。</p><h2 id="Pre-processing-and-Regularization"><a href="#Pre-processing-and-Regularization" class="headerlink" title="Pre-processing and Regularization"></a>Pre-processing and Regularization</h2><p>对应第一周的内容，主要讲解了 Deep Learning 和传统 Machine Learning 一些细节的区别：</p><ol><li>Train / Dev (Cross Validation) / Test Sets 的分割。在 ML 里我们经常会用 6/2/2 这样一个比例，而在大数据量下的 Deep Learning，则一般会选用 <strong>98/1/1 甚至是 99.5/0.25/0.25</strong> 这样一个比例，尽可能的利用数据，来训练出一个更好的模型。另外需要注意的一点就是<strong>保证 Dev Set和 Test Set 这两个集合来自同一数据集</strong>，以确保我们能够通过在 Dev Set上的评估能正确反映在 Test Set 的性能。</li><li>Deep Learning 中 Bias 和 Variance。Bias 和 Variance 需要和一个较为客观的指标进行对比，比如识别中的猫和狗，假设人类的正确率为 99.9%，那么如果我们的模型正确率只有 99%，那么这就可以说是 Bias 了。另外，在 ML 中有时需要进行 Bias 和 Variance 的权衡，但 Deep Learning 是可以训练出一个足够好的模型而不用进行这种权衡的。</li></ol><p>Andrew Ng 也给出了“Basic Recipe for Deep Learning”：<br><a id="more"></a><br>评估模型：</p><ol><li>High Bias：用更大的模型；训练更久；更换 NN 模型。再次评估模型。</li><li>High Variane：获取更多的数据（数据增强）；Regularization。再次评估模型。</li></ol><h3 id="Pre-processing"><a href="#Pre-processing" class="headerlink" title="Pre-processing"></a>Pre-processing</h3><p>事实上数据预处理的手段很多，课程主要讲解的是 Normalization，归一化，让数据保持相对关系的同时，更具可比较性。归一化的主要目的是让梯度下降收敛的速度更快。</p><p><img src="/img/standard.png" alt="Normalization"></p><p>我们让原有数据减去平均值 <code>x拔</code>，再除以方差，得到归一化后的数据 x`。</p><p>这里有一个问题就是 Test Set 上的平均值和方差如何计算，是利用训练集上每个 batch 的 Average 和 variance 做一个 EWMA（后面会提）得到，从而保证 Test Set 的预处理和 Train Set 保持一致。</p><p><strong>数据预处理</strong>真的非常重要，我在 Kaggle 上的 MNIST 识别里仅仅将像素值归一到 [0,1] 区间，就使得模型的准确率上升了接近 1%，亲身体验告诉我，这个 trick 方便而又有效！</p><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>在 ML 中我们有 L1，L2 级别的 Regularization，深度学习中用的比较多的是 L2。</p><p>L2 Regularization 在 BP 过程中体现为在更新 dw 的时候增加了一项 decay 项，所以这使得权重不至于过大，从而使得模型能够防止过拟合。控制 L2 的是一个 Lambda 惩罚系数，是需要根据模型调整的。</p><p>还有一种在 DL 中常用的 Regularization 手段就是 Dropout，随机地使一些 hidden unit 的输出值变成 0，从而使得神经网络不会依赖特定的一个 hidden unit 或者说是 feature，而会尽可能的把权重分摊到各个 feature，使得神经网络能够具有更好的泛化能力。这里通过 keep_prob 来控制随机被置为 0 的 hidden unit 的比例，一般取为 0.5。</p><p>另外还有一些防止过拟合的手段：</p><ol><li>数据增强，通过对已有数据的变换、组合得到更多数据，比如图片旋转、翻转、裁剪</li><li>提前终止训练，通过检测 Train Error 和 Dev Error 的曲线变化来停止训练，防止继续训练是的模型过拟合</li></ol><h2 id="Optimization-Algorithms"><a href="#Optimization-Algorithms" class="headerlink" title="Optimization Algorithms"></a>Optimization Algorithms</h2><p>对应第二周的内容，主要讲解了几种主流的梯度下降算法，假设我们数据集总共的样本数为 m。</p><h3 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h3><p>一次在一个 epoch (through the whole data)，就是整个数据集（m条数据）上进行梯度下降的计算。在大数据量的深度学习之中这种算法优化的速度不是非常理想，但能够找到最优值。</p><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>一次在一组数据上进行梯度的计算，因为样本较少，无法利用矩阵来加速运算。除此之外，还会引入一些噪音，所以 Cost Function 随着迭代的进行而剧烈震荡下降。这种方法的收敛速度比较快。</p><h3 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h3><p>这就是前面两种算法的折中，1 &lt; mini-batch size &lt; m，一般会取 64，128，256（一般以填满GPU/CPU 为最佳），这也是目前深度学习中最主流的手段。</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>在介绍 Momentum 之前 Andrew 先介绍了一种处理数据使数据平滑的 trick：Exponentially Weighted Moving Average(EWMA)，公式如下：</p><p>V<sub>t</sub>=β <em> V<sub>t-1</sub> + (1 - β) </em> θ<sub>t</sub></p><p>如果把这个递推公式展开的话，V<sub>t</sub> 可以看做是一个前面 1 / (1-β) 个数据的平均值。</p><p><img src="/img/temp.png" alt="Dot"></p><p><img src="/img/EWMA.png" alt="EWMA"></p><p>效果如上图所示，用一条红色的曲线来替代蓝色的离散点图，β 越大曲线越平缓，越小震荡越厉害。</p><p>在前面几个点的处理效果上会导致平均值较小，使得整体曲线偏向下方，为了修正这部分我们有时会给 V<sub>t</sub> 除上一个 (1 - β^t)</p><p>然后借鉴这种思想，用在 Gradient Descent 上：</p><p>V<sub>dw</sub> = β <em> V<sub>dw</sub> + (1 - β) </em> dw</p><p>V<sub>db</sub> = β <em> V<sub>db</sub> + (1 - β) </em> db</p><p>W = W - α <em>  V<sub>dw</sub> ; b = b - α </em>  V<sub>db</sub></p><p><img src="/img/momentum.png" alt="Momentum"></p><p>结合上图，蓝色曲线是没有 Momentum 的梯度下降，红色是使用了 Momentum 的梯度下降曲线。从取平均这个角度来理解：纵向的梯度有正有负，平均之后，趋于 0，横向梯度都是同一方向，平均之后正负号不改变。所以我们看到红色曲线更快，更少震荡（意味着陷入局部最优的概率变小）地下降到了全局最优点。β 通常在 0.9 ~ 0.98 这个范围</p><h3 id="RMS-Prop"><a href="#RMS-Prop" class="headerlink" title="RMS Prop"></a>RMS Prop</h3><p>Root Mean Square Prop（RMS prop）是另外一种梯度下降算法，和 Momentum 比较类似：</p><p>S<sub>dw</sub> = β <em> V<sub>dw</sub> + (1 - β) </em> dw^2</p><p>S<sub>db</sub> = β <em> V<sub>db</sub> + (1 - β) </em> db^2</p><p>W = W - α <em>  dw / sqrt(S<sub>dw</sub> + epsilon) ; b = b - α </em> db/sqrt(S<sub>db</sub> + epsilon) </p><p>RMS Prop 的好处在于我们可以用更大的 Learning Rate 而不用担心步子太大扯着蛋。</p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam 结合了 Momentum和 RMS prop，计算 V 和 S（注意，<strong>两个 β 参数不同</strong>），以及相应的修正后的 V<sub>correct</sub>，和 S<sub>correct</sub>，最后在更新权重的时候，用 V 代替 dw。这是现在在大规模深度学习中非常广泛使用的一种算法。</p><h2 id="Hyperparameter-Tuning"><a href="#Hyperparameter-Tuning" class="headerlink" title="Hyperparameter Tuning"></a>Hyperparameter Tuning</h2><p>调参侠的自我修养的重点部分：</p><ol><li>Hyperparameter 调整有优先级的考量：<ol><li>Learning Rate </li><li>Momentum β， 隐藏层的单元数，mini-batch size</li><li>隐藏层数量，Learning Rate Decay</li></ol></li><li>Try random values，don’t use a grid：随机取点调参，而不要按部就班的用格点图。</li><li>在 log 级别设置范围，由大及小。比如在 0.001~1 调整学习率 ，分成 0.001~0.01、0.01 ~ 0.1、0.1~1 这样三个曲线，在三个区间内随机取点调试。</li></ol><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>最后一块就是 Batch Normalization，这一部分的思想和前面数据预处理非常类似。</p><p>我们在神经网络中如果对第 l 层的输出 z(l)（或者是a(l)） 做一次 Normalization，同样能够使得我们的模型更具泛化的能力。</p><p><img src="/img/BN.png" alt="BN"></p><p>对第 l 层里的每个单元 x(i)进行如上图所示的计算，这里的 γ 和 β 是需要我们去学习的参数。具体的原理有待读 Paper 研究。</p><p>这边记录一下 Andrew Ng 认为 Batch Norm as regularization 的观点：</p><ol><li>Batch Norm 对隐藏层之间数据做了 Normalization，因为通过归一化后，使得后面的隐藏层对前面隐藏层数据的变化适应能力更强，通过链式法则可以使得模型对于输入的变化有着更强的适应能力</li><li>Batch Norm 给 z(l) 添加了噪音，使得 a(l) 也有了噪音，更加具有泛化能力</li><li>Batch Norm 带有轻微的一些正则化的功能，但不常作为正则化的手段</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天刷完了 Andrew Ng 深度学习的第二部分课程&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Improving Deep Neural Networks: Hyperparameter tuing, Regularization and Optimization&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;收获还是非常大的，虽然 Andrew Ng 对一些 tricks 的数学细节一笔带过，但通过画图之类的手段来让我获得了一个很形象的 intuition（直观感受），后面数学的部分也要靠自己再去啃书了。这篇 Blog 主要就是对课程内容的一个梳理。&lt;/p&gt;
&lt;h2 id=&quot;Pre-processing-and-Regularization&quot;&gt;&lt;a href=&quot;#Pre-processing-and-Regularization&quot; class=&quot;headerlink&quot; title=&quot;Pre-processing and Regularization&quot;&gt;&lt;/a&gt;Pre-processing and Regularization&lt;/h2&gt;&lt;p&gt;对应第一周的内容，主要讲解了 Deep Learning 和传统 Machine Learning 一些细节的区别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Train / Dev (Cross Validation) / Test Sets 的分割。在 ML 里我们经常会用 6/2/2 这样一个比例，而在大数据量下的 Deep Learning，则一般会选用 &lt;strong&gt;98/1/1 甚至是 99.5/0.25/0.25&lt;/strong&gt; 这样一个比例，尽可能的利用数据，来训练出一个更好的模型。另外需要注意的一点就是&lt;strong&gt;保证 Dev Set和 Test Set 这两个集合来自同一数据集&lt;/strong&gt;，以确保我们能够通过在 Dev Set上的评估能正确反映在 Test Set 的性能。&lt;/li&gt;
&lt;li&gt;Deep Learning 中 Bias 和 Variance。Bias 和 Variance 需要和一个较为客观的指标进行对比，比如识别中的猫和狗，假设人类的正确率为 99.9%，那么如果我们的模型正确率只有 99%，那么这就可以说是 Bias 了。另外，在 ML 中有时需要进行 Bias 和 Variance 的权衡，但 Deep Learning 是可以训练出一个足够好的模型而不用进行这种权衡的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Andrew Ng 也给出了“Basic Recipe for Deep Learning”：&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://tobiaslee.top/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Attention-based LSTM for Text Classification</title>
    <link href="https://tobiaslee.top/2017/08/29/Attention-based-LSTM-for-Text-Classification/"/>
    <id>https://tobiaslee.top/2017/08/29/Attention-based-LSTM-for-Text-Classification/</id>
    <published>2017-08-29T12:12:54.000Z</published>
    <updated>2017-09-22T11:34:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    经过了前面一段的铺垫学习，总算走到了这次的目标：利用 TensorFlow 实现 Attention-based LSTM 来做 Text Classification，主要是在前面一篇文章讲的 TensorFlow 提供的 RNN（GRU） Model 上面添加 Attention Mechanism，注意力模型的实现上，我主要参考了 <a href="http://www.aclweb.org/anthology/P16-2034" target="_blank" rel="noopener">Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</a> 这篇论文。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>上面那篇文章和我想做的东西是非常类似的，唯二的两个区别是：</p><p>一、它使用了一个双向 LSTM 来解决长句子后半部信息丢失的问题，我只是用了单向的 LSTM</p><p>二、它做的是 Relation Classification，而我是 Text Classification。<br><a id="more"></a></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>论文中 Attention 的公式如下：</p><p><img src="/img/attention.jpeg" alt="Attention Principles"></p><p>T：文档的长度，在我们的代码里被设置为 <code>MAX_DOCUMENT_SIZE</code></p><p>H：每个 LSTM 的 output 的集合，即 [h<sub>1</sub>, h<sub>2</sub>, … ,  h<sub>T</sub>]，维度：（Embedding Size，T）</p><p>α：注意力向量，也就是一个 h 的权重分布向量，我们通过 W^T^H，希望能够模型学习得到 Weight，进一步得到 α，维度：（1，T）</p><p>r：加权后的 H，维度：（Embedding Size，1）</p><p>h*：最后的输出，添加一层 <code>tanh</code> ，把值变换到[-1, 1]区间，维度：（Embedding Size，T）</p><h3 id="Classifying"><a href="#Classifying" class="headerlink" title="Classifying"></a>Classifying</h3><p><img src="/img/attention_softmax.png" alt="Classifying"></p><p>通过 Softmax 函数我们得到一个类别的概率向量，然后再用 <code>argmax</code> 函数的结果作为我们最终的分类。</p><h3 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h3><p>其实我们可以通过几个向量的维度窥得其中几个重要的物理意义：</p><ol><li>α 的维度和文本长度相等，注意力向量的加权的思想体现在这里。</li><li>r 向量的长度和词嵌入的长度相同，可以认为是<strong>把整篇文章通过 Attention Model 变成了一个 Topic Word</strong>，随后我们通过学习 Softmax 层的 Weight 和 Bias 来找到各个 Topic Word 在词嵌入的高维空间之中对应的类别。如果能够把结果 plot 出来，应该能看到同一类别文本的 Topic Word 是相近的。</li></ol><h2 id="TensorFlow-实现"><a href="#TensorFlow-实现" class="headerlink" title="TensorFlow 实现"></a>TensorFlow 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">    batch_x = tf.placeholder(tf.int32, [<span class="keyword">None</span>, MAX_DOCUMENT_LENGTH])</span><br><span class="line">    batch_y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, MAX_LABEL])</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Embedding Var, train word embedding during the training </span></span><br><span class="line">    embeddings_var = tf.Variable(tf.random_uniform([n_words, EMBEDDING_SIZE], <span class="number">-1.0</span>, <span class="number">1.0</span>), trainable=<span class="keyword">True</span>)</span><br><span class="line">    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_x)</span><br><span class="line">    W = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="comment"># print(batch_embedded.shape)  # (?, 256, 100)</span></span><br><span class="line"></span><br><span class="line">    rnn_outputs, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE), BasicLSTMCell(HIDDEN_SIZE),</span><br><span class="line">                            inputs=batch_embedded, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># Attention</span></span><br><span class="line">    fw_outputs = rnn_outputs[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># print(fw_outputs.shape)</span></span><br><span class="line">    bw_outputs = rnn_outputs[<span class="number">1</span>]</span><br><span class="line">    H = fw_outputs + bw_outputs  <span class="comment"># (batch_size, seq_len, HIDDEN_SIZE)</span></span><br><span class="line">    M = tf.tanh(H) <span class="comment"># M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)</span></span><br><span class="line">    <span class="comment"># print(M.shape)</span></span><br><span class="line">    <span class="comment"># alpha (bs * sl, 1)</span></span><br><span class="line">    alpha = tf.nn.softmax(tf.matmul(tf.reshape(M, [<span class="number">-1</span>, HIDDEN_SIZE]), tf.reshape(W, [<span class="number">-1</span>, <span class="number">1</span>])))</span><br><span class="line">    r = tf.matmul(tf.transpose(H, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]), tf.reshape(alpha, [<span class="number">-1</span>, MAX_DOCUMENT_LENGTH, <span class="number">1</span>])) <span class="comment"># supposed to be (batch_size * HIDDEN_SIZE, 1)</span></span><br><span class="line">    print(r.shape)</span><br><span class="line">    r = tf.squeeze(r)</span><br><span class="line">    h_star = tf.tanh(r) <span class="comment"># (batch , HIDDEN_SIZE</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># dropout layer </span></span><br><span class="line">    drop = tf.nn.dropout(h_star, keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fully connected layer（dense layer)</span></span><br><span class="line">    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MAX_LABEL], stddev=<span class="number">0.1</span>))</span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.</span>, shape=[MAX_LABEL]))</span><br><span class="line">    y_hat = tf.nn.xw_plus_b(drop, W, b)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># loss function and optimizer</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y))</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Accuracy metric</span></span><br><span class="line">    prediction = tf.argmax(tf.nn.softmax(y_hat), <span class="number">1</span>)</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch_y, <span class="number">1</span>)), tf.float32))</span><br></pre></td></tr></table></figure><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>限于篇幅这里就不贴出数据准备和训练的代码，完整代码在我的 <a href="https://github.com/TobiasLee/Text-Classification" target="_blank" rel="noopener">GitHub</a> 上。</p><p>在 <code>lr=1</code> ， <code>batch_size=128</code>， <code>train_steps=1000000</code> ， <code>Embedding Size =50</code> ，<code>Max Doucment Size = 20</code>，以及没有 dropout 和 regularization 的情况下，模型最后在测试集上的准确率在 80% 左右，结果差强人意，应该还有需要大量改进的地方，有待日后补充。</p><h3 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h3><p>9.3: Change Embedding Size to 100:   Accuracy 91.7%<br>9.4: Change Max Document Size to 25:   Accuracy 93.4%</p><p>9.22：重写了代码，把 Word Embedding 的训练和 Attention-based Bidirectional LSTM 训练放在一起，5 个 epoch 之后，Accuracy: 98.3%</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    经过了前面一段的铺垫学习，总算走到了这次的目标：利用 TensorFlow 实现 Attention-based LSTM 来做 Text Classification，主要是在前面一篇文章讲的 TensorFlow 提供的 RNN（GRU） Model 上面添加 Attention Mechanism，注意力模型的实现上，我主要参考了 &lt;a href=&quot;http://www.aclweb.org/anthology/P16-2034&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification&lt;/a&gt; 这篇论文。&lt;/p&gt;
&lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;p&gt;上面那篇文章和我想做的东西是非常类似的，唯二的两个区别是：&lt;/p&gt;
&lt;p&gt;一、它使用了一个双向 LSTM 来解决长句子后半部信息丢失的问题，我只是用了单向的 LSTM&lt;/p&gt;
&lt;p&gt;二、它做的是 Relation Classification，而我是 Text Classification。&lt;br&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://tobiaslee.top/tags/Machine-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="TensorFlow" scheme="https://tobiaslee.top/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Attention Mechanism 学习笔记1</title>
    <link href="https://tobiaslee.top/2017/08/15/Attention-Mechanism-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://tobiaslee.top/2017/08/15/Attention-Mechanism-学习笔记/</id>
    <published>2017-08-15T11:28:20.000Z</published>
    <updated>2017-08-29T15:18:29.000Z</updated>
    
    <content type="html"><![CDATA[<p><code>Attention Mechanism</code> （注意力机制），是目前在 Neural Network 中应用很广的一种优化模型的方法，下一阶段的目标就是能够利用 TensorFlow 写出 Attention-based LSTM 来做 Text Classification 问题，这篇文章的目的就是记录学习过程中阅读 Paper、Blog 的理解和总结。</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>首先一个问题就是，什么是 <code>Attention</code>？我的理解就是一种 <code>focus</code>，聚焦在某一个局部。打个比方，当我们看到一照片，我们会不自觉地被某一部分所吸引，然后从这一部分向四周延伸出去，注意到画面的其他部分。当然这个吸引的部分可能因人而异，后续的向四周拓展的顺序也不一定一样。</p><p><img src="/img/landscape.jpg" alt="Landscape"></p><p>拿上面这张图片来说，我先看到的是木房子的整体，然后关注到房子的条纹结构，再到远处的海边、山，最后是天空。这就是视觉上的注意力机制，在某种程度上说，这也是我们人脑为了节省计算资源的一种方式。</p><p>那么到了深度学习领域，我们会用 CNN 去处理图片，比如给图片起标题（Image Captioning）或者说是用 RNN 处理文本，做机器翻译（Neural Machine Translation，简称 NMT），如果同样地引入注意力机制，让模型更关注图片或者是文本的某一部分，是不是也能够节省计算资源呢？答案是，Yes！<br><a id="more"></a></p><h2 id="从-NMT-说起"><a href="#从-NMT-说起" class="headerlink" title="从 NMT 说起"></a>从 NMT 说起</h2><p><code>Attention Mechanism</code> 第一次应用在 NLP 是 Bahdanau [1] 的这篇论文里，他是在之前的 <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Seq2Seq</a> 的 NMT 模型上加上了注意力机制。</p><blockquote><p>Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. </p></blockquote><p>Seq2Seq 主要就是 <code>encode-decode</code> 模型，先用一个 LSTM 把输入（一种语言）映射成一个固定长度的向量，然后再用另一个 LSTM 解码这个向量，从而得到输出（对应的另外一种语言），这样就实现了机器翻译，并且是不需要人工的标注等一系列费时费力的操作。而这个模型的缺点就在于：无论多长的句子，都会被编码成一个固定长度的向量，作为解码器的输入。那么对于长句子而言，编码过程中势必就会有信息的损失。</p><p>于是，Bahdanau 等人就提出了下图这样的模型：</p><p><img src="/img/nmt_model.png" alt="NMT Model"></p><p>首先这有一个 <code>Bidirection RNN</code>，双向 RNN 作为编码器，这样做的好处就是能够在一些语序影响翻译（比如：句末词语对冠词、代词翻译提供参考）的语言中表现得更好。</p><p>在哪里体现了 <code>Attention</code> 呢？答案是: C<sub>i</sub>，这个 <code>Context Vector</code>，上下文向量。</p><p>区别于 Seq2Seq 直接把最后一个时序 i 的输出 h<sub>i</sub> 作为上下文向量，而是将之前所有时序的输出通过加权求和得到的一个上下文向量。<strong>这个 C<sub>i</sub> 包含着各个时序输出的权重信息，也就相当于告诉我们哪一段文字对于当前的 <code>target word</code> 是重要的，哪些是不重要的</strong>。这就相当于告诉我们的注意力应该放在哪里。</p><ul><li>C<sub>i</sub> 是 Attention 矩阵的一行，表示输入 X<sub>1</sub> 到 X<sub>T</sub> 分别对 decoder 第 i 时序这个 target word 所对应的的权重（注意力大小）。</li><li>α<sub>ij</sub> 是 Attention 矩阵的一个值，表示输入表示 X<sub>j</sub>  对 decoder 第 i 时序的 target word 的权重。</li></ul><p>Attention 同时也可以看成是一种对齐模型，用来衡量输入端 j 位置和输出端 i 位置的匹配程度。</p><p>Attention 矩阵的计算方法如红框所示，由一个线性层和 softmax 层叠加得到。其输入是上一时刻 decoder 的状态 s<sub>i-1</sub> 和 encoder 对第 j 个词的输出 h<sub>j</sub>，W<sub>a</sub>、U<sub>a</sub> 和 v<sub>a</sub>则是需要我们模型去学习的参数。其对应的物理意义就是：句子中某个词对应正准备翻译的词的权重（重要性），由词语本身（h<sub>j</sub>）和前一个翻译的词（ s<sub>i-1</sub>）决定。不过这个函数（也叫Score函数）可以有别的形式，后面还会提到。</p><h3 id="Soft-Attention-amp-Hard-Attention"><a href="#Soft-Attention-amp-Hard-Attention" class="headerlink" title="Soft Attention &amp; Hard Attention"></a>Soft Attention &amp; Hard Attention</h3><p>这种形式的 Attention 被称为 Soft Attention，软对齐（注意），因为每个输入词的 h<sub>j</sub> 都参与了权重的计算，这种方法方便梯度的反向传播，便于我们训练模型。对应的有 Hard Attention，就是在输出中找到某个特定的单词，其对应权重为 100%，其余都是 0。这种模型非常的粗暴，同时也因为在文本中一一对应的难度太大，而且这样我们模型的训练会变得非常困难，需要很多优化的技巧，所以很少会 NLP 中使用 Hard Attention；但是在图像处理领域，Hard Attention 被证明是有用的。</p><h2 id="Global-Attention-amp-Local-Attention"><a href="#Global-Attention-amp-Local-Attention" class="headerlink" title="Global Attention &amp; Local Attention"></a>Global Attention &amp; Local Attention</h2><blockquote><p>Our various attention-based models are classifed into two broad categories, global and local. These classes differ in terms of whether the “attention” is placed on all source positions or on only a few source positions. </p></blockquote><p>这是 Luong [2] 论文中对于两种 Attention Model 的描述，其核心的区别在于:</p><p><strong>上下文向量的 C 的计算中，是否所有 Encoder 的 hidden states(h<sub>i</sub>) 都参与了计算：</strong></p><p><strong>全部参与计算就是 Global Attention，只有部分参与，则就是 Local Attention。</strong></p><h3 id="Global-Attention"><a href="#Global-Attention" class="headerlink" title="Global Attention"></a>Global Attention</h3><p><img src="/img/global_attention.png" alt="Global Attention"></p><p>如上图所示，蓝色部分是 Encoder，所有的 hidden states 都参与了 C<sub>t</sub> 的计算。</p><p>权重 a<sub>t</sub> 和 Bahdanau 一样，通过一个 <code>Softmax</code> 函数来计算，只不过这里的 Score 函数有所不同：</p><p><img src="/img/score_func.png" alt="Score"></p><p>Bahdanau论文中用的是第三种，而在这篇论文中发现，第一种对于 Global Attention 效果更好，而第二种应用在 Local Attention 效果更好。</p><h3 id="Local-Attention"><a href="#Local-Attention" class="headerlink" title="Local Attention"></a>Local Attention</h3><p><img src="/img/local_attention.png" alt="Local Attention"></p><p>如上图所示，这里很明显的不同是：只有部分 hidden states 参与了 C<sub>t</sub> 的计算，另外多了一个 p<sub>t</sub>，这是用于指示对齐位置（也就是哪一部分 h<sub>i</sub> 参与上下文运算的计算）的一个<strong>实数</strong>。</p><p>Local Attention 模型的流程是：</p><ol><li>计算位置对齐参数 p<sub>t</sub></li></ol><p><img src="/img/pt_func.png" alt="pt_function"></p><p>这里的 S 是源句的长度，这样通过 <code>sigmoid</code> 函数我们就能保证 p<sub>t</sub> 一定在我们的句子范围之中，v 和 W 都是需要模型去学习的参数。另外一种对齐参数是直接粗暴的一对一，认为 <code>p&lt;sub&gt;t&lt;/sub&gt; = t</code>，这个在 NMT 场景中明显是不符合逻辑的。</p><ol><li>选取在窗口范围[p<sub>t</sub> - D，p<sub>t</sub> + D]的 hidden states，计算权重向量 a<sub>t</sub></li></ol><p><img src="/img/at_func.png" alt="at_function"></p><p>这里的 D 是通过经验选取的参数（玄学），而 a<sub>t</sub> 就是一个固定长度为 2D+1 的向量，实际上就是在原来的对齐函数乘上了一个高斯分布来体现距离对权重的影响。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a>  </li><li><a href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></li><li><a href="https://blog.heuritech.com/2016/01/20/attention-mechanism/" target="_blank" rel="noopener">Attention Mechanism</a> 这篇 Blog 是让我有醍醐灌顶之感的一篇，争取找时间翻译一遍</li><li><a href="https://zhuanlan.zhihu.com/p/27766967" target="_blank" rel="noopener">Nlp中的attention mechanism</a> </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Attention Mechanism&lt;/code&gt; （注意力机制），是目前在 Neural Network 中应用很广的一种优化模型的方法，下一阶段的目标就是能够利用 TensorFlow 写出 Attention-based LSTM 来做 Text Classification 问题，这篇文章的目的就是记录学习过程中阅读 Paper、Blog 的理解和总结。&lt;/p&gt;
&lt;h2 id=&quot;Attention&quot;&gt;&lt;a href=&quot;#Attention&quot; class=&quot;headerlink&quot; title=&quot;Attention&quot;&gt;&lt;/a&gt;Attention&lt;/h2&gt;&lt;p&gt;首先一个问题就是，什么是 &lt;code&gt;Attention&lt;/code&gt;？我的理解就是一种 &lt;code&gt;focus&lt;/code&gt;，聚焦在某一个局部。打个比方，当我们看到一照片，我们会不自觉地被某一部分所吸引，然后从这一部分向四周延伸出去，注意到画面的其他部分。当然这个吸引的部分可能因人而异，后续的向四周拓展的顺序也不一定一样。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/landscape.jpg&quot; alt=&quot;Landscape&quot;&gt;&lt;/p&gt;
&lt;p&gt;拿上面这张图片来说，我先看到的是木房子的整体，然后关注到房子的条纹结构，再到远处的海边、山，最后是天空。这就是视觉上的注意力机制，在某种程度上说，这也是我们人脑为了节省计算资源的一种方式。&lt;/p&gt;
&lt;p&gt;那么到了深度学习领域，我们会用 CNN 去处理图片，比如给图片起标题（Image Captioning）或者说是用 RNN 处理文本，做机器翻译（Neural Machine Translation，简称 NMT），如果同样地引入注意力机制，让模型更关注图片或者是文本的某一部分，是不是也能够节省计算资源呢？答案是，Yes！&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://tobiaslee.top/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://tobiaslee.top/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Text Classification 代码学习笔记</title>
    <link href="https://tobiaslee.top/2017/08/07/Text-Classification%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://tobiaslee.top/2017/08/07/Text-Classification代码学习笔记/</id>
    <published>2017-08-07T12:12:44.000Z</published>
    <updated>2017-08-15T14:58:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>找到了 TensorFlow 官方的 Text Classification 的代码来学习研究，<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification.py" target="_blank" rel="noopener">GitHub地址</a>。官方有给了四个代码，用 <strong>CNN</strong> 和 <strong>RNN</strong> 分别对<strong>字符</strong>和<strong>词语</strong>做文本分类。这里的数据主要是 <code>dbpedia</code> 的数据，数据的描述是这样的:</p><blockquote><p>The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size of the training dataset is 560,000 and testing dataset 70,000.</p></blockquote><p>也就是说我们有 14 个类别的文本数据，然后我们要做的就是给每一段文本进行分类。</p><a id="more"></a><p>马士兵老师教我：看代码从 main 函数看起，于是乎：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(unused_argv)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Prepare training and testing data</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Process vocabulary</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build model</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict.</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Socre</span></span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>把细节隐去，我们能够看到 main 函数内部的主题框架，就是一个典型的 ML 的流程:</p><ol><li>准备数据</li><li>处理数据</li><li>建模</li><li>训练模型</li><li>预测结果</li><li>评价模型</li></ol><h2 id="1、数据准备"><a href="#1、数据准备" class="headerlink" title="1、数据准备"></a>1、数据准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ... <span class="comment"># 省略导包</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 官方的下载数据，以及通过pandas读取数据的代码</span></span><br><span class="line">dbpedia = tf.contrib.learn.datasets.load_dataset(</span><br><span class="line">    <span class="string">'dbpedia'</span>,</span><br><span class="line">    test_with_fake_data=FLAGS.test_with_fake_data)</span><br><span class="line"></span><br><span class="line">x_train = pd.Series(dbpedia.train.data[:, <span class="number">1</span>])</span><br><span class="line">y_train = pd.Series(dbpedia.train.target)</span><br><span class="line">x_test = pd.Series(dbpedia.test.data[:, <span class="number">1</span>])</span><br><span class="line">y_test = pd.Series(dbpedia.test.target)</span><br></pre></td></tr></table></figure><p>但是可能因为墙的缘故，我实际运行的时候等了两个小时才下好这个数据。不能每次都让他去在下载一遍吧，不然我就疯了。于是掏出《利用 Python 进行数据分析》这本书，学了一下 <code>pandas</code> 的基本用法，改成从本地读取 csv 文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">names = [<span class="string">"class"</span>, <span class="string">"title"</span>, <span class="string">"content"</span>] <span class="comment"># 描述中 数据有三项 类别 标题 和内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 csv 文件 并且利用我们制定的 names 来分组</span></span><br><span class="line">train_csv = pd.read_csv(<span class="string">"./dbpedia_data/dbpedia_csv/train.csv"</span>, names=names)</span><br><span class="line">test_csv = pd.read_csv(<span class="string">"./dbpedia_data/dbpedia_csv/test.csv"</span>, names=names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过指定的 name 拿到我们要的 x:content 和 y:class </span></span><br><span class="line">x_train = pd.Series(train_csv[<span class="string">"content"</span>])</span><br><span class="line">y_train = pd.Series(train_csv[<span class="string">"class"</span>])</span><br><span class="line">x_test = pd.Series(test_csv[<span class="string">"content"</span>])</span><br><span class="line">y_test = pd.Series(test_csv[<span class="string">"class"</span>])</span><br></pre></td></tr></table></figure><h2 id="2、数据处理"><a href="#2、数据处理" class="headerlink" title="2、数据处理"></a>2、数据处理</h2><p>这一步其实应该是相当繁琐并且麻烦的，因为我们要把文本变成机器能够读懂的东西。</p><p>而官方的代码只有寥寥几行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(</span><br><span class="line">    MAX_DOCUMENT_LENGTH)</span><br><span class="line"></span><br><span class="line">x_transform_train = vocab_processor.fit_transform(x_train)</span><br><span class="line">x_transform_test = vocab_processor.transform(x_test)</span><br><span class="line"></span><br><span class="line">x_train = np.array(list(x_transform_train))</span><br><span class="line">x_test = np.array(list(x_transform_test))</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> n_words</span><br><span class="line">n_words = len(vocab_processor.vocabulary_)</span><br><span class="line">print(<span class="string">'Total words: %d'</span> % n_words)</span><br></pre></td></tr></table></figure><p>代码少是因为调用了已经写好的代码，这里比较麻烦的一点是，有很多代码我不能通过编译器直接看到，还好，Google 把相关的代码都放在 GitHub 上。</p><p>第一行代码中的 <code>VocabularyProcessor</code> 是词汇处理器，详细的<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/preprocessing/text.py" target="_blank" rel="noopener">代码</a>。<code>tf.contrib.learn.preprocessing</code> 这个模块主要包含了一些文本预处理的工具，而词汇处理器的作用呢，就是”Maps documents to sequences of word ids”，翻过来就是“把文档映射成词汇的索引序列”。</p><p>首先，需要理解我们是把文本，一段话，变成一个向量，这样机器处理起来就会比较方便。</p><p>这里我们在构造处理器的时候传入了 <code>MAX_DOCUMENT_LENGTH</code>这个参数，来控制我们的文本向量的最大长度，太长了训练起来比较困难。</p><p>那么文本向量是怎么编码的呢？就可以看到下面两代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意这两个函数不一样哟 一个是 fit_transform 一个是 transform</span></span><br><span class="line">x_transform_train = vocab_processor.fit_transform(x_train)</span><br><span class="line">x_transform_test = vocab_processor.transform(x_test)</span><br></pre></td></tr></table></figure><blockquote><p>fit_transform(): Learn the vocabulary dictionary and return indexies of words，</p><p>transform(): Transform documents to word-id matrix</p></blockquote><p>这两个函数的返回是一个可迭代对象:</p><blockquote><p>x: iterable, [n_samples, max_document_length]. Word-id matrix.</p></blockquote><p>上面这几行代码干的就是<strong>把文本变成向量</strong>这样的工作。</p><p>首先构造词汇处理器，限制文本向量的最大长度，然后通过 <code>fit_transform()</code> 方法，这个函数内部是先 <code>fit</code> ，就是整体扫描一遍数据，给每个不同的单词一个编号，然后再 <code>transform</code>，也就是把每个对应的单词替换成数字。</p><p>看一个小例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_text = [<span class="string">'This is a cat'</span>,<span class="string">'This must be boy'</span>, <span class="string">'This is a a dog'</span>]</span><br><span class="line">vocab_pro = VocabularyProcessor(<span class="number">6</span>)</span><br><span class="line">x = np.array(list(vocab_pro.fit_transform(x_text)))</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出结果:</p><blockquote><p>[[1 2 3 4 0 0]<br> [1 5 6 7 0 0]<br> [1 2 3 3 8 0]] </p></blockquote><p>不难理解，数字 1 对应 “This”，数字 2 对应 “is”，以此类推，我们就能够得到一个词汇和数字一一对应的字典。需要特别提一下的是，我们可以给不认识的词汇统统做一个类似 <code>&lt;UNK&gt;</code> 的标记，以及没有词的地方就用 0 来表示。</p><p>在代码中有提到我们是对训练集进行 <code>fit</code> 和 <code>transform</code> ，对测试集进行 <code>transform</code>，显然，对测试集转换的时候使用的是从训练集上学得的字典。我觉得这里可能就是要求我们训练集尽可能大的一个地方，这样才能保证测试集中的词汇基本都能够出现，否则就会大大影响我们模型的准确性。</p><p>所以经过这一步，我们就把文本数据变成了机器能够比较容易处理的一组向量。</p><h2 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h2><p>代码里给出了两个模型，一个是 <strong>RNN</strong>，另一个是 <strong>bag of words</strong>，词袋模型。我个人理解，模型实质上就是一组<strong>输入到输出的映射</strong>，就相当于是函数<code>y = f(x)</code> 就是一个最简单的模型了。</p><h3 id="RNN-Model"><a href="#RNN-Model" class="headerlink" title="RNN Model"></a>RNN Model</h3><p>RNN，循环神经网络，公式和原理主要是在这篇 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Blog</a> 中，这里就不赘述了。</p><p>我觉得比较重要的一点就是，RNN 能够捕获数据在时间（多表现为序列输入）上的特征，这和我们的文本的特点是非常的符合的。</p><p>和 RNN 经常一起使用的两个技术是 GRU 以及 LSTM，这两个是在 RNN 的单元上进行了一些变化，来避免 RNN 的梯度消失或者是梯度爆炸的缺点。这里代码里使用的是 GRU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_model</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line">    <span class="string">"""用RNN模型来预测文本的类别"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 词语索引 -&gt; 词嵌入 embedding </span></span><br><span class="line">    word_vectors = tf.contrib.layers.embed_sequence(</span><br><span class="line">        features[WORDS_FEATURE], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 展平向量  word_list 变成 [batch_size, EMBEDDING_SIZE] 形状的一个 tensor</span></span><br><span class="line">    word_list = tf.unstack(word_vectors, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用嵌入层数量(也就是对应的GRU单元个数) 创建GRU单元</span></span><br><span class="line">    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个展平的 RNN 网络(其长度等于我们的MAX_DOCUMENT_LENGTH),并且把 word_list 作为输入</span></span><br><span class="line">    _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到通过 RNN 编码之后的一组向量(logits)，然后利用 softmax 来变成一个归一化概率的向量，从而预测对应的类别</span></span><br><span class="line">    logits = tf.layers.dense(encoding, MAX_LABEL, activation=<span class="keyword">None</span>)</span><br><span class="line">    <span class="keyword">return</span> estimator_spec_for_softmax_classification(</span><br><span class="line">        logits=logits, labels=labels, mode=mode)</span><br></pre></td></tr></table></figure><p>这里涉及到一个我也并不是非常清楚的概念，有待日后继续学习：</p><p><code>word embedding</code> 词嵌入，是在已经用数学向量表达的文本上再做一次变换，变换后的词向量会更能够代表词的特征（个人见解，存疑），并且比 <code>one-hot encoding</code> 编码更加节省空间，计算效率也更高。</p><h3 id="Bag-of-Words-词袋模型"><a href="#Bag-of-Words-词袋模型" class="headerlink" title="Bag of Words 词袋模型"></a>Bag of Words 词袋模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bag_of_words_model</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line">    <span class="string">"""词袋模型，不考虑文本的顺序"""</span></span><br><span class="line">    bow_column = tf.feature_column.categorical_column_with_identity(</span><br><span class="line">        WORDS_FEATURE, num_buckets=n_words)</span><br><span class="line"></span><br><span class="line">    bow_embedding_column = tf.feature_column.embedding_column(</span><br><span class="line">        bow_column, dimension=EMBEDDING_SIZE)</span><br><span class="line"></span><br><span class="line">    bow = tf.feature_column.input_layer(</span><br><span class="line">        features,</span><br><span class="line">        feature_columns=[bow_embedding_column])</span><br><span class="line">    logits = tf.layers.dense(bow, MAX_LABEL, activation=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> estimator_spec_for_softmax_classification(</span><br><span class="line">        logits=logits, labels=labels, mode=mode)</span><br></pre></td></tr></table></figure><p><code>bag of words</code> 和 RNN 最大的区别就在于<strong>它不考虑文本的顺序</strong>，只管文本中出现了什么词。这里做的和 RNN 类似，我们再一次把文本向量进行一次转换，只不过这一次不考虑文本的顺序，单纯看这句话里有什么词，然后给它对应的一个整数。（这块也不是很清楚，有待深入理解）</p><h2 id="训练模型、预测、评价"><a href="#训练模型、预测、评价" class="headerlink" title="训练模型、预测、评价"></a>训练模型、预测、评价</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimator_spec_for_softmax_classification</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        logits, labels, mode)</span>:</span></span><br><span class="line">    <span class="string">"""Returns EstimatorSpec instance for softmax classification."""</span></span><br><span class="line">    predicted_classes = tf.argmax(logits, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT <span class="comment"># Predict 时候用      </span></span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">            mode=mode,</span><br><span class="line">            predictions=&#123;</span><br><span class="line">                <span class="string">'class'</span>: predicted_classes,</span><br><span class="line">                <span class="string">'prob'</span>: tf.nn.softmax(logits)</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 生成一个 one-hot 编码的 label</span></span><br><span class="line">    onehot_labels = tf.one_hot(labels, MAX_LABEL, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">     <span class="comment"># loss 函数 交叉熵 </span></span><br><span class="line">    loss = tf.losses.softmax_cross_entropy(</span><br><span class="line">        onehot_labels=onehot_labels, logits=logits)</span><br><span class="line">    <span class="comment"># train 模式下使用</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">        <span class="comment"># 设置 学习率为0.1的 AdamOptimizer</span></span><br><span class="line">        optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">        <span class="comment"># minimize loss function</span></span><br><span class="line">        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Score 时候使用</span></span><br><span class="line">    eval_metric_ops = &#123;</span><br><span class="line">        <span class="string">'accuracy'</span>: tf.metrics.accuracy(</span><br><span class="line">            labels=labels, predictions=predicted_classes)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)</span><br></pre></td></tr></table></figure><p>这里 train 的过程和以前写的非常类似。大致就是我们通过文本的类别来 生成对应的 <code>one-hot encoding</code> 的 label，然后通过 <code>cross entropy</code> 交叉熵函数作为我们的 loss function，然后用 <code>AdamOptimizer</code> 来训练这个模型。</p><p>预测的部分就是通过我们的模型生成对应的 logits，然后再通过 softmax 函数变成一个概率向量，其中最大的那一个一般就作为我们预测的类别。</p><p>评估也很简单，就看看我们预测的是否和测试集中的 label 吻合，计算准确率就行。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>完整的数据集还是挺大的，在我 i7 的 rmbp 上跑了大概一个半小时。</p><p>其实在学习代码的时候遇到了一些有意思的情况：</p><ol><li>把 <code>GRUCell</code> 换成 <code>LSTMCell</code> 报错，不知道为什么（应该是对应参数的问题吧）</li><li>在小数据集上跑的时候，词袋模型比 RNN 的效果好很多。RNN的准确率在 31% ~ 38% 之间浮动，而词袋模型在58%左右；大数据集 RNN 在 87 % 而 词袋模型达到了 97 %！</li></ol><p>这篇文章写的还是比较仓促，很多细节部分都没有深究，也只是这个方向上的第一步探索，接下来会对细节的一些地方进行深入的理解。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;找到了 TensorFlow 官方的 Text Classification 的代码来学习研究，&lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub地址&lt;/a&gt;。官方有给了四个代码，用 &lt;strong&gt;CNN&lt;/strong&gt; 和 &lt;strong&gt;RNN&lt;/strong&gt; 分别对&lt;strong&gt;字符&lt;/strong&gt;和&lt;strong&gt;词语&lt;/strong&gt;做文本分类。这里的数据主要是 &lt;code&gt;dbpedia&lt;/code&gt; 的数据，数据的描述是这样的:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size of the training dataset is 560,000 and testing dataset 70,000.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也就是说我们有 14 个类别的文本数据，然后我们要做的就是给每一段文本进行分类。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://tobiaslee.top/tags/Machine-Learning/"/>
    
      <category term="NLP" scheme="https://tobiaslee.top/tags/NLP/"/>
    
      <category term="TensorFlow" scheme="https://tobiaslee.top/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow初步</title>
    <link href="https://tobiaslee.top/2017/08/01/TensorFlow%E5%88%9D%E6%AD%A5/"/>
    <id>https://tobiaslee.top/2017/08/01/TensorFlow初步/</id>
    <published>2017-08-01T11:13:04.000Z</published>
    <updated>2017-08-15T14:58:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>其实这是一篇 Google 官网 TensorFlow Tutorial 的学习笔记，主要侧重记录使用 TensorFlow 框架的一个流程，细节暂且按下不表。</p><p>首先是导入 TensorFlow 以及准备数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 约定的简写</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备mnist数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>提一下 <code>one hot encoding</code>，就是一个由 0/1 组成的向量来表示的 label，如果说数字是“0”，那么对应的 label 就是[1,0,0,…,0]，如果是“1”，那么就是[0,1,0,…,0]</p><p>这篇教程里使用了两个模型，一个是线性的“Wx+b”，还有一个则是CNN，因为主要侧重使用的结构以及我对CNN不是很熟悉（后者才是主要原因），所以这篇就只介绍第一种模型。<br><a id="more"></a></p><p>然后就是使用 TensorFlow 的主体框架了。</p><p>首先是规定我们的输入<code>X，</code>以及对应的 label <code>y</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># placeholder 占位符 等待外部传进来的变量</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>传入的两个参数分别是</p><p><code>dtype</code>：数据的类型，多是<code>tf.float32</code></p><p><code>shape</code>：数据的形状，因为输入是一张 <code>28*28</code>的图片，我们将它展开成一个<code>1*784</code>的向量，输出则是经过 <code>one-hot encoding</code> 的十维的向量。前面的一个参数 <code>None</code>代表可以是任何值，一般这里我们会填入数据集的大小<code>dataset_size</code>。因为这里我们并没有预先知道数据集的大小，所以就用了<code>None</code>。</p><p>那么接下来就是要引入我们的需要学习的两个变量，<code>Weight</code> 和 <code>bias</code>，这里的大写的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置变量 tf.Variable</span></span><br><span class="line"><span class="comment"># 权重 这里是一个784*10的矩阵 这样就可以在 W*x之后变成一个1*10的向量</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="comment"># 偏置 相当一个常数项</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：所有 TensorFlow 的变量在使用之前都需要<strong>初始化</strong></p><p>初始化的过程是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"><span class="comment"># 或者是 sess.run(tf.global_variables_initializer())</span></span><br></pre></td></tr></table></figure><p>这里又出现了一个新的东西：<code>Session</code>。TensorFlow 后台的计算是交给C++去实现的（不然难道用Python？），Session就是 TensorFlow 与后台计算的连接。简单来说，我们只要把计算丢到Session里去，让它run就行。</p><p>接下来，我们定义一下我们的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们的模型输出 y = Wx + b</span></span><br><span class="line">y = tf.matmul(x, W) + b</span><br></pre></td></tr></table></figure><p>然后是重要的<strong>loss function</strong>，这里我们用 cross_entropy （交叉熵）作为误差函数。这里就不赘述原理了，大致就是通过<code>softmax</code>函数将我们的输出变成一个概率，再选取0概最大的一项作为输出向量中的“1”，其他为“0”，然后我们再通过一个函数来衡量与标签的误差，最后求和取平均。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cross_entropy</span></span><br><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</span><br></pre></td></tr></table></figure><p>这里我们使用 GradientDescent（梯度下降）算法来使我们的loss function最小，并且设置<code>0.05</code>的学习率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练方法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure><p>终于到了训练模型的时候了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 我们每次训练100个数据 训练1000次</span></span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 注意后面的 feed_dict 我们传入我们的训练集数据 </span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</span><br></pre></td></tr></table></figure><p>然后就是评估模型准确度的时候了，比较输出的向量和测试集中的label。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.arg_max(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))</span><br><span class="line"><span class="comment"># 注意喂给它的数据是mnist中的测试集</span></span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><p>然后看看我们的输出</p><blockquote><p>0.9214</p></blockquote><p>其实这个accuracy是很低的，在MNIST上，没到99%都是很差的，当然这也可以理解，因为我们用的是一个最简单Softmax Regression的模型，隔壁CNN一下子就能到达99.2%。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在TF上用最简单的Model实现MNIST的识别，主要流程（其实貌似也是整个ML的流程）如下：</p><ul><li>准备工作：导包，导数据，数据处理，模型的选择</li><li>定义：变量、数据等等（<strong>初始化很重要</strong>）</li><li>训练：丢进<code>session</code>去跑，必要时还要利用可视化工具检查训练的情况</li><li>评估：模型是否 overfitting / underfitting 等等</li></ul><p>接下来就得迅速进入 RNN 以及 LSTM 去做 Text Classification了，加油了！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;其实这是一篇 Google 官网 TensorFlow Tutorial 的学习笔记，主要侧重记录使用 TensorFlow 框架的一个流程，细节暂且按下不表。&lt;/p&gt;
&lt;p&gt;首先是导入 TensorFlow 以及准备数据&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf &lt;span class=&quot;comment&quot;&gt;# 约定的简写&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tensorflow.examples.tutorials.mnist &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; input_data&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 准备mnist数据集&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mnist = input_data.read_data_sets(&lt;span class=&quot;string&quot;&gt;&#39;MNIST_data&#39;&lt;/span&gt;, one_hot=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;提一下 &lt;code&gt;one hot encoding&lt;/code&gt;，就是一个由 0/1 组成的向量来表示的 label，如果说数字是“0”，那么对应的 label 就是[1,0,0,…,0]，如果是“1”，那么就是[0,1,0,…,0]&lt;/p&gt;
&lt;p&gt;这篇教程里使用了两个模型，一个是线性的“Wx+b”，还有一个则是CNN，因为主要侧重使用的结构以及我对CNN不是很熟悉（后者才是主要原因），所以这篇就只介绍第一种模型。&lt;br&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://tobiaslee.top/tags/Machine-Learning/"/>
    
      <category term="TensorFlow" scheme="https://tobiaslee.top/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>深圳·大一·未来</title>
    <link href="https://tobiaslee.top/2017/07/23/%E6%B7%B1%E5%9C%B3%C2%B7%E5%A4%A7%E4%B8%80%C2%B7%E6%9C%AA%E6%9D%A5/"/>
    <id>https://tobiaslee.top/2017/07/23/深圳·大一·未来/</id>
    <published>2017-07-23T03:14:32.000Z</published>
    <updated>2017-07-23T08:58:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>昨天晚上回到杭州，深圳的暑实之旅告一段落，路上有花很多时间思考人生，就记下来咯。</p><h2 id="深圳"><a href="#深圳" class="headerlink" title="深圳"></a>深圳</h2><p>初到深圳，暴雨迎接了我们，在中心书城上面被浇了个透心凉。</p><p>最蠢的是我没有带伞，和 Janck 拼一把伞走在可能是人生经历过最大的一场雨中。洗的衣物鞋子也都干不了，所以前几天其实心情是蛮sad的。</p><p>参观了大大小小的公司，感触很深：<strong>大公司不把你当回事</strong>，小公司反而就相对热情很多。又一次深刻地体会到了学校的不给力吧，就想如果我是Top3的学生会被T公司这么对待么？本来对T公司好感满满，这次他们就派了一个行政像糊弄小孩一样地参观了一圈展厅，怎么说呢，好感度肯定有下降。但也收到了人生第一张和未来工作相关的名片，算是迈出了重要的一步吧?<br><a id="more"></a><br>不过回头想想其实也正常，本身我们学校可能就不是target school，不care也正常，况且我们还是一群大一大二的菜鸡，要认真对待的话这成本也太高了。</p><p>给我感觉最好的其实应该是 Oracle 的HR和大道金服的 CTO 了，提供了两个关于职业发展的不同视角。</p><p>Oracle的HR姐姐有给我们看收到了两份简历，大致明白了简历要<strong>精炼而有料</strong>，面试官会掌握你的真实情况的。外企可能在文化方面更加多元，也会有和全球工程师一起工作的烦恼和压力。</p><p>而大道金服这个创业公司的 CTO 给我们传达的就是一个不要重复造轮子的思想，他们公司使用的基本都是成熟的开源框架。但同时这应该也是属于创业公司的一个普遍现象，追求效率所以使用开源代码；但另一方面，这也就使得公司的技术壁垒降低，一旦商业模式被复制，容易被赶超。</p><p>但其实深圳之旅最高兴的还是和想见的人吃了饭啦 : )   感觉很舒服</p><h2 id="大一"><a href="#大一" class="headerlink" title="大一"></a>大一</h2><p>大一就这样结束啦，真的是唰的一下，可能也是年龄渐长，衡量时间的尺度不断变长了吧。</p><p>也懒得总结这一年都干了什么，其实这个Blog就基本记录着这一年的点点滴滴。</p><p>唯一比较遗憾的可能就是在竞赛方面，没能取得一些成绩，这也可能会影响到下学期初的奖学金。</p><p>说不在意肯定是假的，从小到大都没拿过奖学金，更别说是 8000 的国奖。但不管怎么样，都是对一年付出的回报吧，金额是次要的。</p><p>高中同学都觉得我到大学脱单会很easy，可事实上截然相反。可能这就是命吧。</p><p>呐，就这么过去了。</p><h2 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h2><p>感觉我有一直在规划未来，可一直都是变化不定。不然怎么叫飘忽不定的未来呢</p><p>当初选择软工是想毕业就工作，干个三五年出来创业，早日实现财务自由。</p><p>一年下来也觉得有些痴人说梦。本科毕业生的竞争力很弱，而且抬头看十年，本科毕业之后（多半是做Android）工作的市场可能会慢慢萎缩，而 ML 和 AI 的空间都还是比较大的，并且认可程度也相对高一点；另外就是我觉得创业公司最重要的一个是商业模式，还有一个就是技术壁垒，商业模式可能我并没有这方面的能力，但技术一定要牛逼，这就是我要做的。</p><p>所以目前的计划是保研 THU or ZJU(每个人都有Top梦的嘛)，深造同时增加项目经验，为找到钱多活少的工作努力啦。</p><p>暑假的计划应该就是学车，看书，学数据分析和机器挖掘相关的内容，翻译，以及玩。</p><p>可能会在大二尝试结束单身? 但这个真的看缘分了，不强求。</p><p>大概就这么多。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天晚上回到杭州，深圳的暑实之旅告一段落，路上有花很多时间思考人生，就记下来咯。&lt;/p&gt;
&lt;h2 id=&quot;深圳&quot;&gt;&lt;a href=&quot;#深圳&quot; class=&quot;headerlink&quot; title=&quot;深圳&quot;&gt;&lt;/a&gt;深圳&lt;/h2&gt;&lt;p&gt;初到深圳，暴雨迎接了我们，在中心书城上面被浇了个透心凉。&lt;/p&gt;
&lt;p&gt;最蠢的是我没有带伞，和 Janck 拼一把伞走在可能是人生经历过最大的一场雨中。洗的衣物鞋子也都干不了，所以前几天其实心情是蛮sad的。&lt;/p&gt;
&lt;p&gt;参观了大大小小的公司，感触很深：&lt;strong&gt;大公司不把你当回事&lt;/strong&gt;，小公司反而就相对热情很多。又一次深刻地体会到了学校的不给力吧，就想如果我是Top3的学生会被T公司这么对待么？本来对T公司好感满满，这次他们就派了一个行政像糊弄小孩一样地参观了一圈展厅，怎么说呢，好感度肯定有下降。但也收到了人生第一张和未来工作相关的名片，算是迈出了重要的一步吧?&lt;br&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>MNIST手写数字识别-Python</title>
    <link href="https://tobiaslee.top/2017/07/09/MNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB-Python/"/>
    <id>https://tobiaslee.top/2017/07/09/MNIST手写数字识别-Python/</id>
    <published>2017-07-09T08:31:19.000Z</published>
    <updated>2017-08-15T14:56:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    上篇blog讲了神经网络中BP反向传播算法的推导，并且在Andrew Ng的课程中用Matlab实现了MNIST手写数字数据集的识别。这次决定用Python的sk-learn库来实现（调包）一次。</p><h2 id="数据获取以及处理"><a href="#数据获取以及处理" class="headerlink" title="数据获取以及处理"></a>数据获取以及处理</h2><p>​    Google一下，就能找到<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a>的网站，下载四个数据集。分别是:</p><blockquote><p><code>train-images-idx3-ubyte.gz:  training set images (9912422 bytes)</code><br><code>train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)</code><br><code>t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)</code><br><code>t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)</code></p></blockquote><p>​    解压之后发现Window自作主张的把第二个<code>-</code>变成了<code>.</code>， 文件后缀也变成了<code>idx1-ubyte</code>。不过这不是什么大问题，问题是，我们怎么把这个格式的文件变成我们想要的一组特征向量以及labels。</p><blockquote><h3 id="TRAINING-SET-LABEL-FILE-train-labels-idx1-ubyte"><a href="#TRAINING-SET-LABEL-FILE-train-labels-idx1-ubyte" class="headerlink" title="TRAINING SET LABEL FILE (train-labels-idx1-ubyte):"></a>TRAINING SET LABEL FILE (train-labels-idx1-ubyte):</h3><p>offset      type                   value               description][description]</p><p>0000     32 bit integer  0x00000801(2049) magic number (MSB first)</p><p>0004     32 bit integer  60000            number of items</p><p>0008     unsigned byte   ??               label</p><p>0009     unsigned byte   ??               label</p><p>……..</p><p>xxxx     unsigned byte   ??               label</p><p><code>The labels values are 0 to 9.</code></p><h3 id="TRAINING-SET-IMAGE-FILE-train-images-idx3-ubyte"><a href="#TRAINING-SET-IMAGE-FILE-train-images-idx3-ubyte" class="headerlink" title="TRAINING SET IMAGE FILE (train-images-idx3-ubyte):"></a>TRAINING SET IMAGE FILE (train-images-idx3-ubyte):</h3><p>offset      type                   value               description</p><p>0000     32 bit integer  0x00000803(2051) magic number</p><p>0004     32 bit integer  60000            number of images</p><p>0008     32 bit integer  28               number of rows</p><p>0012     32 bit integer  28               number of columns</p><p>0016     unsigned byte   ??               pixel</p><p>0017     unsigned byte   ??               pixel</p><p>……..</p><p>xxxx     unsigned byte   ??               pixel</p><p>Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).</p></blockquote><p>​    网站也很贴心的给出了文件格式的描述，train_set和label都是有一个文件头的。然而还是拿这个ubyte文件没有办法（PS:我Python真的菜)，最后在网上找到了利用<code>struct</code>模块来处理二进制的方法:</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_idx3_ubyte</span><span class="params">(idx3_ubyte_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    解析idx3文件的通用函数</span></span><br><span class="line"><span class="string">    :param idx3_ubyte_file: idx3文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 读取二进制数据</span></span><br><span class="line">    bin_data = open(idx3_ubyte_file, <span class="string">'rb'</span>).read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析文件头信息，依次为魔数、图片数量、每张图片高、每张图片宽</span></span><br><span class="line">    offset = <span class="number">0</span></span><br><span class="line">    fmt_header = <span class="string">'&gt;iiii'</span></span><br><span class="line">    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)</span><br><span class="line">    print(<span class="string">'魔数:%d, 图片数量: %d张, 图片大小: %d*%d'</span> % (magic_number, num_images, num_rows, num_cols))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析数据集</span></span><br><span class="line">    image_size = num_rows * num_cols</span><br><span class="line">    offset += struct.calcsize(fmt_header)</span><br><span class="line">    fmt_image = <span class="string">'&gt;'</span> + str(image_size) + <span class="string">'B'</span></span><br><span class="line">    images = np.empty((num_images, num_rows, num_cols))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_images):</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'已解析 %d'</span> % (i + <span class="number">1</span>) + <span class="string">'张'</span>)</span><br><span class="line">        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols))</span><br><span class="line">        offset += struct.calcsize(fmt_image)</span><br><span class="line">    <span class="keyword">return</span> images</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_idx1_ubyte</span><span class="params">(idx1_ubyte_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    解析idx1文件的通用函数</span></span><br><span class="line"><span class="string">    :param idx1_ubyte_file: idx1文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 读取二进制数据</span></span><br><span class="line">    bin_data = open(idx1_ubyte_file, <span class="string">'rb'</span>).read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析文件头信息，依次为魔数和标签数</span></span><br><span class="line">    offset = <span class="number">0</span></span><br><span class="line">    fmt_header = <span class="string">'&gt;ii'</span></span><br><span class="line">    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)</span><br><span class="line">    print(<span class="string">'魔数:%d, 图片数量: %d张'</span> % (magic_number, num_images))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析数据集</span></span><br><span class="line">    offset += struct.calcsize(fmt_header)</span><br><span class="line">    fmt_image = <span class="string">'&gt;B'</span></span><br><span class="line">    labels = np.zeros((num_images, <span class="number">10</span>), dtype=<span class="string">'int8'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_images):</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'已解析 %d'</span> % (i + <span class="number">1</span>) + <span class="string">'张'</span>)</span><br><span class="line">        digit = (int)(struct.unpack_from(fmt_image, bin_data, offset)[<span class="number">0</span>])</span><br><span class="line">        labels[i][digit] = <span class="number">1</span></span><br><span class="line">        offset += struct.calcsize(fmt_image)</span><br><span class="line">    <span class="keyword">return</span> labels</span><br></pre></td></tr></table></figure><p>​    从二进制流中先读出文件头，确定图片数量，宽、高，然后通过<code>struct.unpacked_from(fmt, binfile, offset)</code>这个方法来读出每一个图片的相关信息，这里把整个数据集储存成了一个三维的ndarray。而label这边我对原有的代码进行了一些修改，把label的值（0~9），展开成了一个<code>1x10</code>的数组，方便后面使用神经网络来训练模型。</p><h2 id="利用sk-learn库来训练神经网络"><a href="#利用sk-learn库来训练神经网络" class="headerlink" title="利用sk-learn库来训练神经网络"></a>利用sk-learn库来训练神经网络</h2><p>​    数字识别实际上就是个多分类问题，用神经网络可以很好的解决。在Andrew Ng的课里是手搓了一部分Matlab代码来实现数字识别的，今天用Python，做一次调包侠，就可以很简单（偷懒）的完成了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnistUsingNN</span><span class="params">(train_dataSet, train_labels, test_dataSet, test_labels)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化一个分类器，传入设定的参数</span></span><br><span class="line">  clf = MLPClassifier(hidden_layer_sizes=(<span class="number">100</span>, <span class="number">50</span>, <span class="number">25</span>),</span><br><span class="line">                        activation=<span class="string">'logistic'</span>, solver=<span class="string">'adam'</span>,</span><br><span class="line">                        learning_rate_init=<span class="number">0.001</span>, max_iter=<span class="number">2000</span>)</span><br><span class="line">    print(<span class="string">'开始训练模型'</span>)</span><br><span class="line">    start = time.time()</span><br><span class="line">    clf.fit(train_dataSet, train_labels)</span><br><span class="line">    print(<span class="string">'训练完毕, 时间:'</span> + str(time.time() - start))</span><br><span class="line"></span><br><span class="line">    res = clf.predict(test_dataSet)  <span class="comment"># 对测试集进行预测</span></span><br><span class="line">    error_num = <span class="number">0</span>  <span class="comment"># 统计预测错误的数目</span></span><br><span class="line">    num = len(test_dataSet)  <span class="comment"># 测试集的数目</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):  <span class="comment"># 遍历预测结果</span></span><br><span class="line">        <span class="comment"># 比较长度为10的数组，返回包含01的数组，0为不同，1为相同</span></span><br><span class="line">        <span class="comment"># 若预测结果与真实结果相同，则10个数字全为1，否则不全为1</span></span><br><span class="line">        <span class="keyword">if</span> np.sum(res[i] == test_labels[i]) &lt; <span class="number">10</span>:</span><br><span class="line">            error_num += <span class="number">1</span></span><br><span class="line">    print(<span class="string">"Total num:"</span>, num, <span class="string">" Wrong num:"</span>,</span><br><span class="line">          error_num, <span class="string">"  CorrectRate:"</span>, (<span class="number">1</span> - error_num / float(num)) * <span class="number">100</span>, <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p>​    这里我参数设了hidden layers为3， 每层的个数分别为(100, 50, 25)， 而默认值是一层，100个<code>神经元</code>。最大迭代次数设置成了2000，基本能够保证converge了，learning_rate设置成了0.0001。</p><p>​    参数的设置比如hidden layer的层数，一般来说是层数越多效果越好，一层的时候准确率在92%， 2层就到了93%，三层达到了95%，当然训练的时间也是在不断地增加，三层用mbp跑，花了213s，用神舟跑大概是280s；learning_rate对于最后的结果也是有比较大的影响，这里的learning_rate在0.001的时候准确率只有91%，调到0.0001之后达到了95%。</p><p>​    有一点需要注意，在传参数的时候，我们读进来的train_dataSet是三维的，需要reshape一下，变成<code>60000x784</code>，也就是每一张图片对应一个列向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataSet = train_images.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">test_dataSet = test_images.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    调包的过程确实是愉快而且轻松的，但仅仅是调包、调参，可能在这种比较简单的场景之下能够达到比较高的精确度，在复杂的情况下，精确度达不到要求的时候就需要对过程进行详细分析、找出问题所在（比如过拟合或者欠拟合）。这时候就要借助一些可视化的工具（比如learing-curve)来帮助分析。这些能力是更加重要，也是仅仅调包学不到的。</p><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集文件</span></span><br><span class="line">train_images_idx3_ubyte_file = <span class="string">'./train-images-idx3-ubyte'</span></span><br><span class="line"><span class="comment"># 训练集标签文件</span></span><br><span class="line">train_labels_idx1_ubyte_file = <span class="string">'./train-labels-idx1-ubyte'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集文件</span></span><br><span class="line">test_images_idx3_ubyte_file = <span class="string">'./t10k-images-idx3-ubyte'</span></span><br><span class="line"><span class="comment"># 测试集标签文件</span></span><br><span class="line">test_labels_idx1_ubyte_file = <span class="string">'./t10k-labels-idx1-ubyte'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_idx3_ubyte</span><span class="params">(idx3_ubyte_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    解析idx3文件的通用函数</span></span><br><span class="line"><span class="string">    :param idx3_ubyte_file: idx3文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 读取二进制数据</span></span><br><span class="line">    bin_data = open(idx3_ubyte_file, <span class="string">'rb'</span>).read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析文件头信息，依次为魔数、图片数量、每张图片高、每张图片宽</span></span><br><span class="line">    offset = <span class="number">0</span></span><br><span class="line">    fmt_header = <span class="string">'&gt;iiii'</span></span><br><span class="line">    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)</span><br><span class="line">    print(<span class="string">'魔数:%d, 图片数量: %d张, 图片大小: %d*%d'</span> % (magic_number, num_images, num_rows, num_cols))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析数据集</span></span><br><span class="line">    image_size = num_rows * num_cols</span><br><span class="line">    offset += struct.calcsize(fmt_header)</span><br><span class="line">    fmt_image = <span class="string">'&gt;'</span> + str(image_size) + <span class="string">'B'</span></span><br><span class="line">    images = np.empty((num_images, num_rows, num_cols))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_images):</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'已解析 %d'</span> % (i + <span class="number">1</span>) + <span class="string">'张'</span>)</span><br><span class="line">        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols))</span><br><span class="line">        offset += struct.calcsize(fmt_image)</span><br><span class="line">    <span class="keyword">return</span> images</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_idx1_ubyte</span><span class="params">(idx1_ubyte_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    解析idx1文件的通用函数</span></span><br><span class="line"><span class="string">    :param idx1_ubyte_file: idx1文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 读取二进制数据</span></span><br><span class="line">    bin_data = open(idx1_ubyte_file, <span class="string">'rb'</span>).read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析文件头信息，依次为魔数和标签数</span></span><br><span class="line">    offset = <span class="number">0</span></span><br><span class="line">    fmt_header = <span class="string">'&gt;ii'</span></span><br><span class="line">    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)</span><br><span class="line">    print(<span class="string">'魔数:%d, 图片数量: %d张'</span> % (magic_number, num_images))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析数据集</span></span><br><span class="line">    offset += struct.calcsize(fmt_header)</span><br><span class="line">    fmt_image = <span class="string">'&gt;B'</span></span><br><span class="line">    labels = np.zeros((num_images, <span class="number">10</span>), dtype=<span class="string">'int8'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_images):</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'已解析 %d'</span> % (i + <span class="number">1</span>) + <span class="string">'张'</span>)</span><br><span class="line">        digit = (int)(struct.unpack_from(fmt_image, bin_data, offset)[<span class="number">0</span>])</span><br><span class="line">        labels[i][digit] = <span class="number">1</span></span><br><span class="line">        offset += struct.calcsize(fmt_image)</span><br><span class="line">    <span class="keyword">return</span> labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_train_images</span><span class="params">(idx_ubyte_file=train_images_idx3_ubyte_file)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> decode_idx3_ubyte(idx_ubyte_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_train_labels</span><span class="params">(idx_ubyte_file=train_labels_idx1_ubyte_file)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decode_idx1_ubyte(idx_ubyte_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_test_images</span><span class="params">(idx_ubyte_file=test_images_idx3_ubyte_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    TEST SET IMAGE FILE (t10k-images-idx3-ubyte):</span></span><br><span class="line"><span class="string">    [offset] [type]          [value]          [description]</span></span><br><span class="line"><span class="string">    0000     32 bit integer  0x00000803(2051) magic number</span></span><br><span class="line"><span class="string">    0004     32 bit integer  10000            number of images</span></span><br><span class="line"><span class="string">    0008     32 bit integer  28               number of rows</span></span><br><span class="line"><span class="string">    0012     32 bit integer  28               number of columns</span></span><br><span class="line"><span class="string">    0016     unsigned byte   ??               pixel</span></span><br><span class="line"><span class="string">    0017     unsigned byte   ??               pixel</span></span><br><span class="line"><span class="string">    ........</span></span><br><span class="line"><span class="string">    xxxx     unsigned byte   ??               pixel</span></span><br><span class="line"><span class="string">    Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param idx_ubyte_file: idx文件路径</span></span><br><span class="line"><span class="string">    :return: n*row*col维np.array对象，n为图片数量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> decode_idx3_ubyte(idx_ubyte_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_test_labels</span><span class="params">(idx_ubyte_file=test_labels_idx1_ubyte_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    TEST SET LABEL FILE (t10k-labels-idx1-ubyte):</span></span><br><span class="line"><span class="string">    [offset] [type]          [value]          [description]</span></span><br><span class="line"><span class="string">    0000     32 bit integer  0x00000801(2049) magic number (MSB first)</span></span><br><span class="line"><span class="string">    0004     32 bit integer  10000            number of items</span></span><br><span class="line"><span class="string">    0008     unsigned byte   ??               label</span></span><br><span class="line"><span class="string">    0009     unsigned byte   ??               label</span></span><br><span class="line"><span class="string">    ........</span></span><br><span class="line"><span class="string">    xxxx     unsigned byte   ??               label</span></span><br><span class="line"><span class="string">    The labels values are 0 to 9.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param idx_ubyte_file: idx文件路径</span></span><br><span class="line"><span class="string">    :return: n*1维np.array对象，n为图片数量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> decode_idx1_ubyte(idx_ubyte_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用KNN来实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnistUsingKNN</span><span class="params">(train_dataSet, train_labels, test_dataSet, test_labels)</span>:</span></span><br><span class="line"></span><br><span class="line">    knn = neighbors.KNeighborsClassifier(algorithm=<span class="string">'kd_tree'</span>, n_neighbors=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">'开始训练模型'</span>)</span><br><span class="line">    start = time.time()</span><br><span class="line">    knn.fit(train_dataSet, train_labels)</span><br><span class="line">    print(<span class="string">'训练完毕, 时间:'</span> + str(time.time() - start))</span><br><span class="line"></span><br><span class="line">    res = knn.predict(test_dataSet)  <span class="comment"># 对测试集进行预测</span></span><br><span class="line">    error_num = <span class="number">0</span>  <span class="comment"># 统计预测错误的数目</span></span><br><span class="line">    num = len(test_dataSet)  <span class="comment"># 测试集的数目</span></span><br><span class="line">    print(num)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):  <span class="comment"># 遍历预测结果</span></span><br><span class="line">        <span class="comment"># 比较长度为10的数组，返回包含01的数组，0为不同，1为相同</span></span><br><span class="line">        <span class="comment"># 若预测结果与真实结果相同，则10个数字全为1，否则不全为1</span></span><br><span class="line">        <span class="keyword">if</span> np.sum(res[i] == test_labels[i]) &lt; <span class="number">10</span>:</span><br><span class="line">            error_num += <span class="number">1</span></span><br><span class="line">    print(<span class="string">"Total num:"</span>, num, <span class="string">" Wrong num:"</span>, \</span><br><span class="line">          error_num, <span class="string">"  CorrectRate:"</span>, (<span class="number">1</span>-error_num / float(num)) * <span class="number">100</span>, <span class="string">"%"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnistUsingNN</span><span class="params">(train_dataSet, train_labels, test_dataSet, test_labels)</span>:</span></span><br><span class="line"></span><br><span class="line">    clf = MLPClassifier(hidden_layer_sizes=(<span class="number">100</span>, <span class="number">50</span>, <span class="number">25</span>),</span><br><span class="line">                        activation=<span class="string">'logistic'</span>, solver=<span class="string">'adam'</span>,</span><br><span class="line">                        learning_rate_init=<span class="number">0.0001</span>, max_iter=<span class="number">2000</span>)</span><br><span class="line">    print(<span class="string">'开始训练模型'</span>)</span><br><span class="line">    start = time.time()</span><br><span class="line">    clf.fit(train_dataSet, train_labels)</span><br><span class="line">    print(<span class="string">'训练完毕, 时间:'</span> + str(time.time() - start))</span><br><span class="line"></span><br><span class="line">    res = clf.predict(test_dataSet)  <span class="comment"># 对测试集进行预测</span></span><br><span class="line"></span><br><span class="line">    error_num = <span class="number">0</span>  <span class="comment"># 统计预测错误的数目</span></span><br><span class="line">    num = len(test_dataSet)  <span class="comment"># 测试集的数目</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):  <span class="comment"># 遍历预测结果</span></span><br><span class="line">        <span class="comment"># 比较长度为10的数组，返回包含01的数组，0为不同，1为相同</span></span><br><span class="line">        <span class="comment"># 若预测结果与真实结果相同，则10个数字全为1，否则不全为1</span></span><br><span class="line">        <span class="keyword">if</span> np.sum(res[i] == test_labels[i]) &lt; <span class="number">10</span>:</span><br><span class="line">            error_num += <span class="number">1</span></span><br><span class="line">    print(<span class="string">"Total num:"</span>, num, <span class="string">" Wrong num:"</span>,</span><br><span class="line">          error_num, <span class="string">"  CorrectRate:"</span>, (<span class="number">1</span> - error_num / float(num)) * <span class="number">100</span>, <span class="string">'%'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    train_images = load_train_images()</span><br><span class="line">    train_labels = load_train_labels()</span><br><span class="line">    test_images = load_test_images()</span><br><span class="line">    test_labels = load_test_labels()</span><br><span class="line"></span><br><span class="line">    train_dataSet = train_images.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">    test_dataSet = test_images.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    mnistUsingNN(train_dataSet, train_labels, test_dataSet, test_labels)</span><br><span class="line">    <span class="comment"># mnistUsingKNN(train_dataSet, train_labels, test_dataSet,test_labels)</span></span><br></pre></td></tr></table></figure><p>​    </p><p>​    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    上篇blog讲了神经网络中BP反向传播算法的推导，并且在Andrew Ng的课程中用Matlab实现了MNIST手写数字数据集的识别。这次决定用Python的sk-learn库来实现（调包）一次。&lt;/p&gt;
&lt;h2 id=&quot;数据获取以及处理&quot;&gt;&lt;a href=&quot;#数据获取以及处理&quot; class=&quot;headerlink&quot; title=&quot;数据获取以及处理&quot;&gt;&lt;/a&gt;数据获取以及处理&lt;/h2&gt;&lt;p&gt;​    Google一下，就能找到&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MNIST&lt;/a&gt;的网站，下载四个数据集。分别是:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;train-images-idx3-ubyte.gz:  training set images (9912422 bytes)&lt;/code&gt;&lt;br&gt;&lt;code&gt;train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)&lt;/code&gt;&lt;br&gt;&lt;code&gt;t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)&lt;/code&gt;&lt;br&gt;&lt;code&gt;t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    解压之后发现Window自作主张的把第二个&lt;code&gt;-&lt;/code&gt;变成了&lt;code&gt;.&lt;/code&gt;， 文件后缀也变成了&lt;code&gt;idx1-ubyte&lt;/code&gt;。不过这不是什么大问题，问题是，我们怎么把这个格式的文件变成我们想要的一组特征向量以及labels。&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;TRAINING-SET-LABEL-FILE-train-labels-idx1-ubyte&quot;&gt;&lt;a href=&quot;#TRAINING-SET-LABEL-FILE-train-labels-idx1-ubyte&quot; class=&quot;headerlink&quot; title=&quot;TRAINING SET LABEL FILE (train-labels-idx1-ubyte):&quot;&gt;&lt;/a&gt;TRAINING SET LABEL FILE (train-labels-idx1-ubyte):&lt;/h3&gt;&lt;p&gt;offset      type                   value               description][description]&lt;/p&gt;
&lt;p&gt;0000     32 bit integer  0x00000801(2049) magic number (MSB first)&lt;/p&gt;
&lt;p&gt;0004     32 bit integer  60000            number of items&lt;/p&gt;
&lt;p&gt;0008     unsigned byte   ??               label&lt;/p&gt;
&lt;p&gt;0009     unsigned byte   ??               label&lt;/p&gt;
&lt;p&gt;……..&lt;/p&gt;
&lt;p&gt;xxxx     unsigned byte   ??               label&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The labels values are 0 to 9.&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;TRAINING-SET-IMAGE-FILE-train-images-idx3-ubyte&quot;&gt;&lt;a href=&quot;#TRAINING-SET-IMAGE-FILE-train-images-idx3-ubyte&quot; class=&quot;headerlink&quot; title=&quot;TRAINING SET IMAGE FILE (train-images-idx3-ubyte):&quot;&gt;&lt;/a&gt;TRAINING SET IMAGE FILE (train-images-idx3-ubyte):&lt;/h3&gt;&lt;p&gt;offset      type                   value               description&lt;/p&gt;
&lt;p&gt;0000     32 bit integer  0x00000803(2051) magic number&lt;/p&gt;
&lt;p&gt;0004     32 bit integer  60000            number of images&lt;/p&gt;
&lt;p&gt;0008     32 bit integer  28               number of rows&lt;/p&gt;
&lt;p&gt;0012     32 bit integer  28               number of columns&lt;/p&gt;
&lt;p&gt;0016     unsigned byte   ??               pixel&lt;/p&gt;
&lt;p&gt;0017     unsigned byte   ??               pixel&lt;/p&gt;
&lt;p&gt;……..&lt;/p&gt;
&lt;p&gt;xxxx     unsigned byte   ??               pixel&lt;/p&gt;
&lt;p&gt;Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    网站也很贴心的给出了文件格式的描述，train_set和label都是有一个文件头的。然而还是拿这个ubyte文件没有办法（PS:我Python真的菜)，最后在网上找到了利用&lt;code&gt;struct&lt;/code&gt;模块来处理二进制的方法:&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://tobiaslee.top/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="https://tobiaslee.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>BP算法推导</title>
    <link href="https://tobiaslee.top/2017/06/19/BP%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/"/>
    <id>https://tobiaslee.top/2017/06/19/BP算法推导/</id>
    <published>2017-06-19T14:17:13.000Z</published>
    <updated>2017-08-15T14:58:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    开始ML之旅，跟Coursera上Andrew Ng的ML课到了第五周，讲了神经网络中 <code>Back Propagation</code>（逆向误差传播，简称BP）算法。但视频没有给出推导的过程，并且和周志华《机器学习》上的推导有所出入，让人觉得无比玄学。但其实思想是非常精炼的一句话（个人理解）：<strong>从后向前不断计算每一层的误差，来调整对应layer的权重, 从而实现网络权重的学习和优化</strong></p><p>​    在参考了Andrew Ng的讲义和网上的一篇<a href="https://my.oschina.net/finbill/blog/.h29001" target="_blank" rel="noopener">Blog</a>之后，我尝试着推导了一下相关的步骤，这篇文章主要是记录推导的过程和思路。<br><a id="more"></a></p><h2 id="数学记号"><a href="#数学记号" class="headerlink" title="数学记号"></a>数学记号</h2><p><img src="/img/mathNotation.png" alt="Notation"></p><h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p><img src="/img/BP1.png" alt="Process1"></p><p><img src="/img/BP2.png" alt="Process2"></p><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p><img src="/img/BPtips.png" alt="Tips"></p><h2 id="思路和感想"><a href="#思路和感想" class="headerlink" title="思路和感想"></a>思路和感想</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>​    链式法则中的偏导转换实际上存在着对应的实际意义:</p><p>​    我们要求 J 对 Theta(l) 的偏导数，首先转换成对于其下一层输出单元a(l+1)的导数（①)，再通过a(l+1)输出对z(l+1)输入的导数（②, 也就是sigmoid函数的导数)，最后再通过z(l+1)输入对Theta(l)的导数（③, 可以通过展开得到）。</p><p>​    求偏导过程其实就体现了BP的主要想法: <strong>通过Theta所影响到的下一层的输出，来调整Theta，是一种由后向前的过程。</strong>我们没有直接对Theta求导，而是转换到下一层的输出，再到输入，再到Theta这样一条路径来求导的。并且这样的转换也多次体现在步骤的其他求导过程中。</p><p>​    结果非常漂亮，使得我们可以通过不断地从后向前迭代，更新对应Theta来实现算法。</p><p>​    另外一点，Andrew Ng版本和周志华的《机器学习》中的版本不同之处在于对于Cost Function定义的不同，周的书中定义为输出与样本的方差，而Andrew Ng定义中则包含了log（详见tips）。这就是导致其推导过程不同的原因。</p><h3 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h3><p>一个BP算法，写了足足四张纸，其难度不言而喻，同时也给予了我爆棚的成就感（可把我牛逼坏了）！</p><p><strong>知其然，更要知其所以然</strong>，这应该是我认为做机器学习，或者说是科研，需要具备的基本素质。</p><p>加油！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    开始ML之旅，跟Coursera上Andrew Ng的ML课到了第五周，讲了神经网络中 &lt;code&gt;Back Propagation&lt;/code&gt;（逆向误差传播，简称BP）算法。但视频没有给出推导的过程，并且和周志华《机器学习》上的推导有所出入，让人觉得无比玄学。但其实思想是非常精炼的一句话（个人理解）：&lt;strong&gt;从后向前不断计算每一层的误差，来调整对应layer的权重, 从而实现网络权重的学习和优化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    在参考了Andrew Ng的讲义和网上的一篇&lt;a href=&quot;https://my.oschina.net/finbill/blog/.h29001&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Blog&lt;/a&gt;之后，我尝试着推导了一下相关的步骤，这篇文章主要是记录推导的过程和思路。&lt;br&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://tobiaslee.top/tags/Machine-Learning/"/>
    
      <category term="BP算法" scheme="https://tobiaslee.top/tags/BP%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>I&#39;m nothing.</title>
    <link href="https://tobiaslee.top/2017/06/12/I-m-nothing/"/>
    <id>https://tobiaslee.top/2017/06/12/I-m-nothing/</id>
    <published>2017-06-12T12:29:54.000Z</published>
    <updated>2017-06-12T14:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>空间里学弟们庆祝着解放，洋溢着的考后的喜悦和空虚提醒着我：高考，过去一年了。</p><p>当初考场上的种种，让我来到了现在所处的地方。</p><p>刚入学时候的不甘心还萦绕在心头，而现在，深深的无力感又充斥着我的身体。</p><a id="more"></a><h2 id="关于过去"><a href="#关于过去" class="headerlink" title="关于过去"></a>关于过去</h2><p>回头看大学快结束的这一个学年，大概能够用一事无成来形容吧。</p><p>英语竞赛做了分母，ACM因为开小号被取消资格（笑），数模辛苦三天也没能捞个鼓励奖。</p><p>可能是竞赛绝缘体质？</p><p>但其实自己也清楚，真要准备这些比赛必然是要前期投入大量的时间和精力的，我只是去参与了这一项又一项的比赛，但真要说参加比赛，我想我的投入还是达不到要求的。</p><p>或许这可能又是我的叙事自我在找借口了。</p><p>可能是高中的某些人让我有一种聪明的人不用很努力最后结局也能光彩熠熠的错觉？</p><p>然后我可能被上学期侥幸的GPA冲昏了脑子以为自己还算聪明？</p><p>还是应该给自己泼一盆冷水: <strong>You’re nothing.</strong></p><p>不要太过自以为是了，真牛逼的话现在又怎么会在这里？</p><p>要参加英语竞赛就把题好好刷了，用心准备的话会拿不到奖？</p><p>要打ACM就得和人家一样把课余、睡眠时间拿去训练，而不是每天准时熄灯睡觉还喜欢赖床。</p><p>要做数模就好好学Matlab，学好数理基础，用cftool拟合个函数算什么数模？看着唬人，只能骗骗自己。</p><p>别真把自己当根葱了，没有汗水，哪来的回报？</p><p>光知道眼馋人家拿奖拿牌，自己又每天过着咸鱼一样的生活，可笑么？</p><h2 id="关于未来"><a href="#关于未来" class="headerlink" title="关于未来"></a>关于未来</h2><p>上大学前信誓旦旦地说毕业就工作，谁上研谁傻逼。</p><p>现在写了大半年Android，调调API，抄书上的代码，照着教程看看源码，出了bug就Google，StackOverFlow。真的，没什么技术含量。</p><p>搓了个看着吓人的滤镜播放器，自己也清楚，都是copy的开源代码做了一些改动而已。</p><p>甚至连加几个库以外的滤镜，把视频本地化保存的能力都没有。</p><p>说没有技术含量，你又做不出来，是不是在打自己脸？</p><p>看学长Github上万把颗星，自己想要却只能让同学帮忙Star。</p><p>最近也有好久没有写代码了。</p><p>傻瓜也知道，技术这种东西，急于求成不来；没有十万行代码的功夫，哪来的star?</p><p>又不是什么万里挑一的神人，有些时候觉得过于苛求自己了。</p><p>我也想瘫在寝室快乐咸鱼每一天，但心里总是有个声音告诉我：你不可以那样。</p><p>Why？我不知道，但那并不是我想要的生活。</p><p>然后现在又有上研的想法，target大约是THU或者ZJU，方向可能是ML。</p><p>这个想法大约是听了一系列讲座而萌生的吧，以及最近很火的AlphaGo。</p><p>Google已经从Mobile First转向AI First，想要成为时代的弄潮儿必然是要向这方向发展。</p><p>也有觉得本科平台不够大，想要获取更多资源的想法吧，看了很多清华人的blog，大约就有一种top university就是不一样，校友质量碾压我等的感觉。</p><p>上研意味着不仅仅要GPA，科研竞赛都得有成绩，我又该走哪条路呢？</p><p>摇摆不定，迷茫而不知道方向。</p><p>大约只有时间能够给我答案。</p><h2 id="I’m-Nothing"><a href="#I’m-Nothing" class="headerlink" title="I’m Nothing"></a>I’m Nothing</h2><p>絮叨了很多，骂了自己也分析了原因。</p><p>大约还是要摆正姿态，<strong>No pain, no gain</strong> 不仅仅是作文里的套话，真的是事实。</p><p>你并不是什么天才，唯有勤勉，方能砥砺前行。</p><p>他人的期望也许在无形之间让你不断地苛责自己以期达到他们的要求。</p><p>但有时候，还是要听听自己内心的声音，哪怕这个叙事自我可能在骗你，不过那确确实实是你的声音。</p><p>就这样。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;空间里学弟们庆祝着解放，洋溢着的考后的喜悦和空虚提醒着我：高考，过去一年了。&lt;/p&gt;
&lt;p&gt;当初考场上的种种，让我来到了现在所处的地方。&lt;/p&gt;
&lt;p&gt;刚入学时候的不甘心还萦绕在心头，而现在，深深的无力感又充斥着我的身体。&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>HashMap in Java</title>
    <link href="https://tobiaslee.top/2017/06/07/HashMap-in-Java/"/>
    <id>https://tobiaslee.top/2017/06/07/HashMap-in-Java/</id>
    <published>2017-06-07T14:11:20.000Z</published>
    <updated>2017-06-07T14:36:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>数据结构学了哈希查找，于是乎决定学习一下Java中的HashMap。</p><p>有两个很重要的问题:</p><ol><li>HashMap的哈希算法是怎么样的</li><li>Collision是怎么解决的</li></ol><p>So，看看Java里HashMap的源码！</p><blockquote><p>Hash table based implementation of the <tt>Map</tt> interface.  This implementation provides all of the optional map operations, and <strong>permits null  values and the null key</strong>.  (The HashMapclass is roughly equivalent to Hashtable, <strong>except that it is unsynchronized and permits nulls</strong>.)  This class <strong>makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time</strong>.</p></blockquote><p>第一段告诉我们几个KeyPoints:</p><ol><li>基于Map实现</li><li>允许null的值和键</li><li>非同步（非线程安全)</li><li>不保证有序，顺序也可能随时间变化</li></ol><p>HashMap有两个重要的参数: <strong>容量(Capacity)和装载因子(Load factor)</strong>(装填的比例) </p><p>一般在我们不指定容量的情况下，Java会自动帮我们初始化为容量为16并且装载因子为.75</p><p>我们知道，装载因子对于HashMap的性能影响是比较大的，而这个.75是设计者考虑之后得出的。</p><p>另外就是当实际的装载因子超过.75这个初始值之后，HashMap会进行一次resize操作，扩容为2倍。</p><p>这里值得注意的一点是，<strong>HashMap的容量始终为2的幂次</strong>。<br><a id="more"></a></p><h2 id="哈希算法"><a href="#哈希算法" class="headerlink" title="哈希算法"></a>哈希算法</h2><p>第一个问题，key 的哈希算法:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line">    <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>); <span class="comment">// 无符号右移</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，这里是对key调用了它的<code>hashCode()</code>方法，而一般继承自Object的<code>hashCode()</code>是根据内存地址来计算的，理论上是可以保证不会发生冲突的。</p><p>然而这里还把hashCode()计算得到的h 右移16位之后和自身做了一个异或操作，这是为什么呢?</p><blockquote><p>Computes key.hashCode() and spreads (XORs) higher bits of hash to lower.  Because the table uses power-of-two masking, sets of hashes that vary only in bits above the current mask will always collide. (Among known examples are sets of Float keys holding consecutive whole numbers in small tables.)  So we apply a transform that spreads the impact of higher bits downward. There is a tradeoff between <strong>speed, utility, and quality </strong>of bit-spreading. Because many common sets of hashes are already <strong>reasonably distributed </strong>(so don’t benefit from spreading), and because <strong>we use trees to handle large sets of collisions in bins</strong>, we just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds.</p></blockquote><p>注释告诉我们: 因为实际储存节点的是一个长度为2的幂次的数组，而在计算下标时，使用的是</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i = (n - <span class="number">1</span>) &amp; hash <span class="comment">// n是表的大小</span></span><br></pre></td></tr></table></figure><p>并不是我们常用的<code>%</code>操作，而是<code>&amp;</code>操作，这可能是出于性能方面的考虑。但这样就导致在表比较小的时候，发生碰撞的概率就变大了：比如当n = 16, n -1 = 15(0x1111)，这时候生效的只有低4位，碰撞的概率非常大。</p><p>因此设计者权衡了速度，作用以及质量之后，用一个比较简便的移位异或操作，这样操作的好处有:</p><ol><li>减少因为表长较小而发生的碰撞情况</li><li>让高位参与了hash的过程，减少碰撞的概率</li><li>开销小</li></ol><p>以上就是HashMap的hash函数。</p><h2 id="Collision的处理"><a href="#Collision的处理" class="headerlink" title="Collision的处理"></a>Collision的处理</h2><p>《数据结构》书上给了3种处理冲突的方法:</p><ol><li>开放定址法：主要有线性探测、二次探测、伪随机数三种，主要思想就是就近找到位置放置冲突的key</li><li>再哈希法: 用另一个hash再算出一个新的地址</li><li>链地址法： 将所有关键字为同义词的记录储存在同一线性链表中</li></ol><p><img src="/img/linked_address.png" alt="LinkedAddress"></p><p>Java采用的是第三种，并且做了一些优化: <strong>HashMap会在链表过长(大于或等于常量<code>TREEFY_THRESHOLD</code>)时，将链表转换成红黑树</strong>。这样查找的速度就会从O(n)提升到O(lgn)</p><p>还有一点值得注意时，HashMap里的Node类是实现了<code>Map.Entry&lt;&gt;</code>的，也就是，<strong>Map会以<code>key-value</code>对的形式保存，而不仅仅只是保存key而不存value</strong>，这样就可以避免不同key算出同一个hashCode而不知道具体是哪一条的情况，而是可以通过key.equals()方法来判断是否是指定的key。</p><p>以及可以复习一下Java中equals的相关知识:</p><ol><li>相同的对象必须具有相同的hashCode</li><li>有相同的hashCode的对象不一定相等，不然就不会发生collision了</li></ol><p>所以如果有冲突，通过<code>key.equals(k)</code>去查找对应的entry；若为链表，则在链表中通过<code>key.equals(k)</code>查找，复杂度O(n)；若为树，则在树中通过<code>key.equals(k)</code>查找，复杂度O(logn)。</p><p>根据以上，推荐用String和Integer这样的不可变对象作为键，避免collision以提高效率。</p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>接下来我们就可以来看看<code>get()</code>和<code>put()</code>方法的具体实现:</p><h3 id="put"><a href="#put" class="headerlink" title="put()"></a>put()</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 用hash(key)算出hash值</span></span><br><span class="line">    <span class="keyword">return</span> putVal(hash(key), key, value, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params">                   <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">        Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line">  <span class="comment">// 空表则初始化</span></span><br><span class="line">        <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">            n = (tab = resize()).length;</span><br><span class="line">  <span class="comment">// 该位置空 命中</span></span><br><span class="line">        <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>)</span><br><span class="line">            tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">else</span>&#123;   <span class="comment">// 发生冲突</span></span><br><span class="line">            Node&lt;K,V&gt; e; K k;</span><br><span class="line">          <span class="comment">// 同一个key </span></span><br><span class="line">            <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">                ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                e = p;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode) <span class="comment">// 该链为树</span></span><br><span class="line">                e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line">            <span class="keyword">else</span> &#123; <span class="comment">// 链表</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line">                    <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        p.next = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">                        <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span></span><br><span class="line">                            treeifyBin(tab, hash);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                        ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    p = e;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="comment">// 视情况更新value</span></span><br><span class="line">            <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123; <span class="comment">// existing mapping for key</span></span><br><span class="line">                V oldValue = e.value;</span><br><span class="line">                <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line">                    e.value = value;</span><br><span class="line">                afterNodeAccess(e);</span><br><span class="line">                <span class="keyword">return</span> oldValue;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ++modCount;</span><br><span class="line">  <span class="comment">// 当大小超过load factor*current capacity 需要resize</span></span><br><span class="line">        <span class="keyword">if</span> (++size &gt; threshold)</span><br><span class="line">            resize();</span><br><span class="line">        afterNodeInsertion(evict);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="get"><a href="#get" class="headerlink" title="get()"></a>get()</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = getNode(hash(key), key)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> hash, Object key)</span> </span>&#123;</span><br><span class="line">        Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; <span class="keyword">int</span> n; K k;</span><br><span class="line">        <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">            (first = tab[(n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (first.hash == hash &amp;&amp; <span class="comment">// always check first node</span></span><br><span class="line">                ((k = first.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                <span class="keyword">return</span> first; <span class="comment">// 直接命中 这也是绝大多数发生的情况 </span></span><br><span class="line">            <span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) &#123; <span class="comment">// 未命中 则需要到树/链表中查找</span></span><br><span class="line">                <span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode) <span class="comment">// 进入树</span></span><br><span class="line">                    <span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);</span><br><span class="line">                <span class="keyword">do</span> &#123; <span class="comment">// 在链表中get</span></span><br><span class="line">                    <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                        ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                        <span class="keyword">return</span> e;</span><br><span class="line">                &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>get()的实现就比较简单，因为我们通常都会直接命中，如果没有命中，就到树或者链表中去查找。<strong>都是通过<code>key.equals()</code>方法来进行key的匹配，而不仅仅只是hashCode的相等</strong>。</p><h3 id="resize"><a href="#resize" class="headerlink" title="resize()"></a>resize()</h3><p>我们在上文之中提到: 当<code>load factor * current capacity &gt; threshold</code>会发生resize，将table扩大一倍，难道又要重新计算一遍hashCode重新摆放吗?来看看它的实现:</p><blockquote><p>Initializes or doubles table size.  If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, <strong>the elements from each bin must either stay at same index, or move with a power of two offset</strong> in the new table.</p></blockquote><p>注释说元素要么待在原来的位置，要么在原位置上移动一个2次幂。</p><p>怎么理解呢?</p><p>当我们从表长为16拓展到32到的时候，n-1从0x1111 变为 0x11111</p><p>而<code>(n-1) &amp; hash</code> 的结果前面的位数是保持不变的，增加的一位不是0(保持原位)就是1(移动一个2次幂，也就是移动一个长度oldCap)，看图:</p><p><img src="/img/hash_change.png" alt="HashChange"></p><p>所以新的index会发生如下的变化:</p><p><img src="/img/hash_change2.png" alt="IndexChange"></p><p>所以，在resize过程中我们不用重新计算一次hash，只要看看增加那一位是0还是1就可以了。以下是一个16扩充到32的resize示意图:</p><p><img src="/img/hash_example.png" alt="ResizeExample"></p><p>代码的具体实现:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</span><br><span class="line">        Node&lt;K,V&gt;[] oldTab = table;</span><br><span class="line">        <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;</span><br><span class="line">        <span class="keyword">int</span> oldThr = threshold;</span><br><span class="line">        <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="comment">// 超出了最大容量 无法扩容</span></span><br><span class="line">            <span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                threshold = Integer.MAX_VALUE;</span><br><span class="line">                <span class="keyword">return</span> oldTab;</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="comment">// 扩容一倍</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp;</span><br><span class="line">                     oldCap &gt;= DEFAULT_INITIAL_CAPACITY)</span><br><span class="line">                newThr = oldThr &lt;&lt; <span class="number">1</span>; <span class="comment">// double threshold</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>) <span class="comment">// initial capacity was placed in threshold</span></span><br><span class="line">            newCap = oldThr;</span><br><span class="line">        <span class="keyword">else</span> &#123;               <span class="comment">// zero initial threshold signifies using defaults</span></span><br><span class="line">            newCap = DEFAULT_INITIAL_CAPACITY;</span><br><span class="line">            newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);</span><br><span class="line">        &#125;</span><br><span class="line">  <span class="comment">// 计算新的threshold </span></span><br><span class="line">        <span class="keyword">if</span> (newThr == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor;</span><br><span class="line">            newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ?</span><br><span class="line">                      (<span class="keyword">int</span>)ft : Integer.MAX_VALUE);</span><br><span class="line">        &#125;</span><br><span class="line">        threshold = newThr;</span><br><span class="line">        <span class="meta">@SuppressWarnings</span>(&#123;<span class="string">"rawtypes"</span>,<span class="string">"unchecked"</span>&#125;)</span><br><span class="line">            Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];</span><br><span class="line">        table = newTab;</span><br><span class="line">        <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) &#123; <span class="comment">// 移动每个元素</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;</span><br><span class="line">                Node&lt;K,V&gt; e;</span><br><span class="line">                <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    oldTab[j] = <span class="keyword">null</span>; <span class="comment">// 清空原位</span></span><br><span class="line">                    <span class="keyword">if</span> (e.next == <span class="keyword">null</span>) <span class="comment">// 直接放置</span></span><br><span class="line">                        newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode) <span class="comment">// 树中节点的移动</span></span><br><span class="line">                        ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</span><br><span class="line">                    <span class="keyword">else</span> &#123; <span class="comment">// preserve order 保存前驱 以下是对链表中节点的移动 结合示意图理解</span></span><br><span class="line">                        Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">                        Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">                        Node&lt;K,V&gt; next;</span><br><span class="line">                        <span class="keyword">do</span> &#123; </span><br><span class="line">                            next = e.next;</span><br><span class="line">                            <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) &#123; <span class="comment">// 保持原位的节点</span></span><br><span class="line">                                <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)</span><br><span class="line">                                    loHead = e;</span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                    loTail.next = e;</span><br><span class="line">                                loTail = e;</span><br><span class="line">                            &#125;</span><br><span class="line">                            <span class="keyword">else</span> &#123; <span class="comment">// 原index + oldCap的节点 </span></span><br><span class="line">                                <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)</span><br><span class="line">                                    hiHead = e;</span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                    hiTail.next = e;</span><br><span class="line">                                hiTail = e;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">while</span> ((e = next) != <span class="keyword">null</span>);</span><br><span class="line">                      <span class="comment">// 放到新表中的原位置</span></span><br><span class="line">                        <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            loTail.next = <span class="keyword">null</span>;</span><br><span class="line">                            newTab[j] = loHead;</span><br><span class="line">                        &#125;</span><br><span class="line">                      <span class="comment">// 新表中原位置+oldCap</span></span><br><span class="line">                        <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            hiTail.next = <span class="keyword">null</span>;</span><br><span class="line">                            newTab[j + oldCap] = hiHead;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> newTab;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>HashMap的哈希算法是调用<code>object.hashCode()</code>并且与它的结果右移16位做异或得到的，通过<code>(n-1) &amp; hash</code>来计算下标。</li><li>Collision的解决用了链地址法，并且会在长度超过一定值的时候转换成一颗红黑树。</li><li>HashMap可以接受为null的键和值，是非同步的。</li><li>当<code>load factor * current capacity &gt; threshold</code>时调用resize()方法扩容一倍。</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://yikun.github.io/2015/04/01/Java-HashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/" target="_blank" rel="noopener">Java-HashMap工作原理及实现</a></p><p><a href="http://www.importnew.com/7099.html" target="_blank" rel="noopener">HashMap的工作原理</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据结构学了哈希查找，于是乎决定学习一下Java中的HashMap。&lt;/p&gt;
&lt;p&gt;有两个很重要的问题:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HashMap的哈希算法是怎么样的&lt;/li&gt;
&lt;li&gt;Collision是怎么解决的&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So，看看Java里HashMap的源码！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hash table based implementation of the &lt;tt&gt;Map&lt;/tt&gt; interface.  This implementation provides all of the optional map operations, and &lt;strong&gt;permits null  values and the null key&lt;/strong&gt;.  (The HashMapclass is roughly equivalent to Hashtable, &lt;strong&gt;except that it is unsynchronized and permits nulls&lt;/strong&gt;.)  This class &lt;strong&gt;makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第一段告诉我们几个KeyPoints:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于Map实现&lt;/li&gt;
&lt;li&gt;允许null的值和键&lt;/li&gt;
&lt;li&gt;非同步（非线程安全)&lt;/li&gt;
&lt;li&gt;不保证有序，顺序也可能随时间变化&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HashMap有两个重要的参数: &lt;strong&gt;容量(Capacity)和装载因子(Load factor)&lt;/strong&gt;(装填的比例) &lt;/p&gt;
&lt;p&gt;一般在我们不指定容量的情况下，Java会自动帮我们初始化为容量为16并且装载因子为.75&lt;/p&gt;
&lt;p&gt;我们知道，装载因子对于HashMap的性能影响是比较大的，而这个.75是设计者考虑之后得出的。&lt;/p&gt;
&lt;p&gt;另外就是当实际的装载因子超过.75这个初始值之后，HashMap会进行一次resize操作，扩容为2倍。&lt;/p&gt;
&lt;p&gt;这里值得注意的一点是，&lt;strong&gt;HashMap的容量始终为2的幂次&lt;/strong&gt;。&lt;br&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://tobiaslee.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="Java" scheme="https://tobiaslee.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Android Scroll学习笔记</title>
    <link href="https://tobiaslee.top/2017/06/05/Android-Scroll%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://tobiaslee.top/2017/06/05/Android-Scroll学习笔记/</id>
    <published>2017-06-05T15:10:58.000Z</published>
    <updated>2017-06-06T09:14:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    友球要写一个跟纸片一样的能够翻转过来跟纸片一样的自定义View，所以我就去研究了一下View的Animation和滑动、拖拽。这篇就记录一下学习《Android群英传》里的滑动部分。</p><h2 id="Key-Points"><a href="#Key-Points" class="headerlink" title="Key Points"></a>Key Points</h2><p>滑动的实现：<strong>不断地改变View的坐标</strong></p><p>Android坐标系: <strong>左上角为原点，水平向右为X轴正方向，垂直向下为Y轴正方向</strong></p><p>视图坐标系: 描述子视图在父视图中的位置关系，子视图的坐标以<strong>父视图左上角为原点</strong></p><h3 id="触控事件—MotionEvent"><a href="#触控事件—MotionEvent" class="headerlink" title="触控事件—MotionEvent"></a>触控事件—MotionEvent</h3><p>MotionEvent中封装的常用事件常量:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> fianl <span class="keyword">int</span> ACTION_DOWN = <span class="number">0</span>; <span class="comment">// 单点触摸 按下</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> fianl <span class="keyword">int</span> ACTION_UP = <span class="number">1</span>; <span class="comment">// 单点触摸 离开</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> fianl <span class="keyword">int</span> ACTION_MOVE = <span class="number">2</span>; <span class="comment">// 触摸点 移动</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> fianl <span class="keyword">int</span> ACTION_CANCEL = <span class="number">3</span>; <span class="comment">// 触摸动作 取消</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> fianl <span class="keyword">int</span> ACTION_OUTSIDE = <span class="number">4</span>; <span class="comment">// 触摸动作 超出边界</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> fianl <span class="keyword">int</span> ACTION_POINTER_DOWN = <span class="number">5</span>; <span class="comment">// 多点触摸 按下</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> fianl <span class="keyword">int</span> ACTION_POINTER_UP = <span class="number">6</span>; <span class="comment">// 多点触摸 离开</span></span><br></pre></td></tr></table></figure><p>实现触摸实现的代码套路:在<code>onTouchEvent(MotionEvent event)</code>中通过<code>event.getAction()</code>获取触控事件类型，并且用switch-case语句来筛选我们所需要的动作。</p><a id="more"></a><h4 id="触摸事件的代码套路"><a href="#触摸事件的代码套路" class="headerlink" title="触摸事件的代码套路"></a>触摸事件的代码套路</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">onTouchEvent</span><span class="params">(MotionEvent event)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 获取触摸事件坐标</span></span><br><span class="line">   <span class="keyword">int</span> x = (<span class="keyword">int</span>) event.getX();</span><br><span class="line">   <span class="keyword">int</span> y = (<span class="keyword">int</span>) event.getY();</span><br><span class="line">  <span class="keyword">switch</span> (event.getAction()) &#123;</span><br><span class="line">            <span class="keyword">case</span> MotionEvent.ACTION_DOWN:</span><br><span class="line">               <span class="comment">// 处理按下</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> MotionEvent.ACTION_MOVE:</span><br><span class="line">                <span class="comment">// 处理移动</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> MotionEvent.UP:</span><br><span class="line">                <span class="comment">// 处理离开</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Android提供的获取一系列坐标的方法示意图:</p><p><img src="/img/position_methods.png" alt="PositionMethods"></p><p>以上方法分成两类:</p><ul><li>View提供的获取坐标方法<ol><li>getTop() :  获取View自身顶边到其父布局顶边的距离</li><li>getLeft():  获取View自身左边到其父布局左边距离</li><li>getBottom(): 获取View自身的底边到父布局顶边的距离</li><li>getRight(): 获取View自身的右边到父布局左边的距离</li></ol></li><li>MotionEvent提供的方法<ol><li>getX(): 获取点击事件距离<strong>控件</strong>左边的距离，即视图坐标</li><li>getY(): 获取点击事件距离<strong>控件</strong>顶边的距离，即视图坐标</li><li>getRawX(): 获取点击事件距离<strong>整个屏幕</strong>左边的距离，即绝对坐标</li><li>getX(): 获取点击事件距离<strong>整个屏幕</strong>顶边的距离，即绝对坐标</li></ol></li></ul><p>只要牢记<strong>坐标获取都是以左/顶边为参考的</strong>即可，配合图片理解就好了。</p><h2 id="实现滑动的若干种方法"><a href="#实现滑动的若干种方法" class="headerlink" title="实现滑动的若干种方法"></a>实现滑动的若干种方法</h2><p>​    接下来就用这些系统提供的API来实现一个动态的修改一个View的坐标，也就是实现滑动效果。不管哪种方式，实现的思路是一样的：</p><p>​    <strong>触摸View时，系统记录下此时坐标；手指一动时，记录下移动后的触摸点坐标，获取到偏移量，并通过偏移量来修改View坐标；不断重复，就实现了滑动</strong></p><p>​    首先，自定义一个DragView继承自View，重写Constructor方法，并且有两个私有成员准备用来记录上一次的坐标。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DragView</span> <span class="keyword">extends</span> <span class="title">View</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> lastX = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> lastY = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DragView</span><span class="params">(Context context, @Nullable AttributeSet attrs)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(context, attrs);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    写一个简单的布局，把我们的DragView设成100*100，并且让他填上背景色方便观察。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="utf-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">LinearLayout</span></span></span><br><span class="line"><span class="tag">    <span class="attr">xmlns:android</span>=<span class="string">"http://schemas.android.com/apk/res/android"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">xmlns:tools</span>=<span class="string">"http://schemas.android.com/tools"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:layout_width</span>=<span class="string">"match_parent"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:layout_height</span>=<span class="string">"match_parent"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">tools:context</span>=<span class="string">"top.tobiaslee.viewtest.ScrollActivity"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">top.tobiaslee.viewtest.DragView</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:layout_width</span>=<span class="string">"100dp"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:layout_height</span>=<span class="string">"100dp"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:background</span>=<span class="string">"@color/colorAccent"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">LinearLayout</span>&gt;</span></span><br></pre></td></tr></table></figure><p>​    接下来，照搬套路。我们重写<code>onTouchEvent(MotionEvent event)</code>这个方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">onTouchEvent</span><span class="params">(MotionEvent event)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 获取触摸事件坐标</span></span><br><span class="line">   <span class="keyword">int</span> x = (<span class="keyword">int</span>) event.getX();</span><br><span class="line">   <span class="keyword">int</span> y = (<span class="keyword">int</span>) event.getY();</span><br><span class="line">  <span class="keyword">switch</span> (event.getAction()) &#123;</span><br><span class="line">            <span class="keyword">case</span> MotionEvent.ACTION_DOWN:</span><br><span class="line">               <span class="comment">// 处理按下</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> MotionEvent.ACTION_MOVE:</span><br><span class="line">                <span class="comment">// 处理移动</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> MotionEvent.UP:</span><br><span class="line">                <span class="comment">// 处理离开</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="法1-layout方法"><a href="#法1-layout方法" class="headerlink" title="法1. layout方法"></a>法1. layout方法</h3><p>View进行绘制的时候，会调用<code>onLayout()</code>方法来设置显示的位置，我们可以在ACTION_MOV事件中计算偏移量，然后在layout中增加偏移量就可以了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> MotionEvent.ACTION_DOWN:</span><br><span class="line"><span class="comment">// 记录触摸点坐标</span></span><br><span class="line">lastX = x;</span><br><span class="line">lastY = y; </span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> MotionEvent.ACTION_MOVE:</span><br><span class="line"><span class="comment">// 计算偏移量</span></span><br><span class="line"><span class="keyword">int</span> offsetX = x - lastX;</span><br><span class="line"><span class="keyword">int</span> offsetY = y - lastY;</span><br><span class="line"><span class="comment">// 调用layout() 在四个方向上增加偏移量</span></span><br><span class="line">    layout(getLeft() + offsetX, </span><br><span class="line">           getTop() + offsetY, </span><br><span class="line">           getRight() + offsetX, </span><br><span class="line">           getBottom() + offsetY);</span><br><span class="line"><span class="keyword">break</span>;</span><br></pre></td></tr></table></figure><p>这里是通过视图坐标来计算偏移量的，我们同样可以通过绝对坐标来计算:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">onTouchEvent</span><span class="params">(MotionEvent event)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 获取触摸事件坐标</span></span><br><span class="line">   <span class="keyword">int</span> rawX = (<span class="keyword">int</span>) event.getRawX();</span><br><span class="line">   <span class="keyword">int</span> rawY = (<span class="keyword">int</span>) event.getRawY();</span><br><span class="line">  <span class="keyword">switch</span> (event.getAction()) &#123;</span><br><span class="line">            <span class="keyword">case</span> MotionEvent.ACTION_DOWN:</span><br><span class="line">               lastX = rawX;</span><br><span class="line">      lastY = rawY;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> MotionEvent.ACTION_MOVE:</span><br><span class="line">                <span class="keyword">int</span> offsetX = rawX - lastX;</span><br><span class="line"><span class="keyword">int</span> offsetY = rawY - lastY;</span><br><span class="line">   layout(getLeft() + offsetX, </span><br><span class="line">           getTop() + offsetY, </span><br><span class="line">           getRight() + offsetX, </span><br><span class="line">           getBottom() + offsetY);</span><br><span class="line">      <span class="comment">// 重设初始坐标</span></span><br><span class="line">      lastX = rawX;</span><br><span class="line">      lastY = rawY;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    需要注意的是，因为通过绝对坐标计算偏移量，我们要及时更新上一个触摸点的坐标，所以在ACTION_MOVE方法中也要记得更新<code>lastX</code>和<code>lastY</code>，否则就会出现偏移量不准确的情况。</p><h3 id="法2-offsetLeftAndRight-与offsetTopAndBottom"><a href="#法2-offsetLeftAndRight-与offsetTopAndBottom" class="headerlink" title="法2. offsetLeftAndRight()与offsetTopAndBottom()"></a>法2. offsetLeftAndRight()与offsetTopAndBottom()</h3><p>这两个方法就是系统提供的一个对左右、上下移动的API封装，计算出偏移量后，直接调用即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 左右移动</span></span><br><span class="line">offsetLeftAndRight(offsetX);</span><br><span class="line"><span class="comment">// 上下移动</span></span><br><span class="line">offsetTopAndBottom(offsetY);</span><br></pre></td></tr></table></figure><h3 id="法3-LayoutParams"><a href="#法3-LayoutParams" class="headerlink" title="法3. LayoutParams"></a>法3. LayoutParams</h3><p>LayoutParams保存了View的布局参数，我们可以通过改变LayoutParams的参数来动态改变View的位置参数，从而实现View的移动。我们可以通过<code>getLayoutParams()</code>获取到View的LayoutParams，然后通过<code>setLayoutParams()</code>来改变其参数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过layoutParams来实现移动</span></span><br><span class="line"><span class="comment">// 通过父布局</span></span><br><span class="line">LinearLayout.LayoutParams layoutParams = (LinearLayout.LayoutParams) </span><br><span class="line">                        getLayoutParams();</span><br><span class="line"><span class="comment">// 通过MarginLayout来实现</span></span><br><span class="line">ViewGroup.MarginLayoutParams layoutParams =(ViewGroup.MarginLayoutParams)</span><br><span class="line">  getLayoutParams();</span><br><span class="line"></span><br><span class="line">layoutParams.leftMargin = getLeft() + offsetX;</span><br><span class="line">layoutParams.topMargin = getTop() + offsetY;</span><br><span class="line">setLayoutParams(layoutParams);</span><br></pre></td></tr></table></figure><p>这里有两种方法获取LayoutParams。</p><p>一种根据父布局获取对应的LayoutParams，这里我们用的是LinearLayout，所以对应的也就要使用这种类型的布局参数，如果是父布局RelativeLayout，那就是<code>RelativeLayout.LayoutParams layoutParams = (RelativeLayout.LayoutParams) getLayoutParams();</code>。但要注意，<strong>如果没有父布局，就无法获取LayoutParams</strong></p><p>还有一种是通过<code>ViewGroup.MarginLayoutParams</code>来实现，因为我们实现View的移动，通常是改变这个View的Margin属性。这种实现就不用考虑父布局，更加方便。</p><h3 id="法4-scrollTo和scrollBy"><a href="#法4-scrollTo和scrollBy" class="headerlink" title="法4. scrollTo和scrollBy"></a>法4. scrollTo和scrollBy</h3><p>在一个View中，系统提供了<code>scrollTo()</code>和<code>scrollBy()</code>来改变一个View位置。</p><p>从字面上就可以看出<strong><code>scrollBy()</code>移动一个增量(dx, dy)，<code>scrollTo()</code>移动到指定的坐标位置(x, y)</strong></p><p>但是要注意一点，这两个方法<strong>移动的是View的content，也就是让View的内容移动</strong>，比如一个TextView，content就是它的文本；ImageView，就是它的drawable对象。</p><p>所以如果我们要移动这个DrawView，就要移动它的父View</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((View)getParent()).scrollBy(offsetX, offsetY)</span><br></pre></td></tr></table></figure><p>但是这样的实现，我们会发现拖动时候，View的移动是反向的!</p><p>Why? 因为我们移动的是ViewGroup，而计算的偏移量是通过子View来计算的。</p><p>假设我们希望View移动(20,10),那么上面语句执行结果就是让父布局向右下方移动了(20, 10)，对于View来说就相当于移动了(-20, -10)，他们的移动是相反的。</p><p>所以这个和在显微镜下移动载玻片相类似，需要向相反的方向移动。</p><p>也就是:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((View)getParent()).scrollBy(-offsetX, -offsetY)</span><br></pre></td></tr></table></figure><h3 id="法5-Scroller"><a href="#法5-Scroller" class="headerlink" title="法5. Scroller"></a>法5. Scroller</h3><p>我们之前通过scrollBy()方法，通过不断微分移动的距离，切割成N个小片段，然后瞬间移动一个小片段，实现了滑动，但未免有些不够流畅；而Scroller是用来实现平滑移动效果的一个类。</p><p>Scroller的使用，我们需要在添加一个私有成员mScroller,并且在初始化的时候创建一个scroller对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">DragView</span><span class="params">(Context context, @Nullable AttributeSet attrs)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">super</span>(context, attrs);</span><br><span class="line">       mScroller = <span class="keyword">new</span> Scroller(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后重写<code>onComputeScroll()</code>方法，实现模拟滑动，这是Scroller类的核心</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">computeScroll</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>.computeScroll();</span><br><span class="line"><span class="comment">// 判断Scroller是否执行完毕</span></span><br><span class="line"><span class="keyword">if</span>(mScroller.computeScrollOffset()) &#123;</span><br><span class="line">((View)getParent()).scrollTo(mScroller.getCurrX(),</span><br><span class="line">                                    mScroller.getCurrY());</span><br><span class="line"><span class="comment">// 通过重绘 不断调用computeScroll</span></span><br><span class="line">invalidate();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Scroller类提供了<code>computeScrollOffset()</code>来判断滑动是否完成，也提供了<code>getCurrX()</code>和<code>getCurrY()</code>来获取当前的滑动坐标。</p><p><strong>需要注意的是，computeScroll()方法是不会自动的调用的，但我们只能在这里获取currX和currY，所以只能通过invalidate()-&gt;draw()-&gt;computeScroll()来间接调用，从而不断获得坐标</strong></p><p>接下来就是要启动Scroller了</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startScroll</span><span class="params">(<span class="keyword">int</span> startX, <span class="keyword">int</span> startY, <span class="keyword">int</span> dx, <span class="keyword">int</span> dy, <span class="keyword">int</span> duration）</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">public</span> <span class="keyword">void</span> startScroll(<span class="keyword">int</span> startX, <span class="keyword">int</span> startY, <span class="keyword">int</span> dx, <span class="keyword">int</span> dy)</span></span></span><br></pre></td></tr></table></figure><p>Scroller启动有两个方法，区别就在于一个指定了时间长短，一个没有。（startX, startY）起始坐标，(dx, dy)就是坐标偏移量。</p><p>然后我们在ACTION_UP中启动，让View滑动回原来的位置:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> MotionEvent.ACTION_UP:</span><br><span class="line">View viewGroup = (View)getParent();</span><br><span class="line"><span class="comment">// 滑动回原位置</span></span><br><span class="line">mScroller.startScroll(viewGroup.getScrollX(),</span><br><span class="line">                          viewGroup.getScrollY(),</span><br><span class="line">                          -viewGroup.getScrollX(),</span><br><span class="line">                          -viewGroup.getScrollY());</span><br><span class="line"><span class="comment">// 通知重绘</span></span><br><span class="line">invalidate();</span><br><span class="line"><span class="keyword">break</span>;</span><br></pre></td></tr></table></figure><h3 id="法6-7-属性动画和ViewDragHelper"><a href="#法6-7-属性动画和ViewDragHelper" class="headerlink" title="法6.7 属性动画和ViewDragHelper"></a>法6.7 属性动画和ViewDragHelper</h3><p>限于篇幅，就不放在这里讲了。</p><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>最后我们看看实现的效果:</p><p><img src="/img/scroll_view.gif" alt="scroll"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    友球要写一个跟纸片一样的能够翻转过来跟纸片一样的自定义View，所以我就去研究了一下View的Animation和滑动、拖拽。这篇就记录一下学习《Android群英传》里的滑动部分。&lt;/p&gt;
&lt;h2 id=&quot;Key-Points&quot;&gt;&lt;a href=&quot;#Key-Points&quot; class=&quot;headerlink&quot; title=&quot;Key Points&quot;&gt;&lt;/a&gt;Key Points&lt;/h2&gt;&lt;p&gt;滑动的实现：&lt;strong&gt;不断地改变View的坐标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Android坐标系: &lt;strong&gt;左上角为原点，水平向右为X轴正方向，垂直向下为Y轴正方向&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;视图坐标系: 描述子视图在父视图中的位置关系，子视图的坐标以&lt;strong&gt;父视图左上角为原点&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;触控事件—MotionEvent&quot;&gt;&lt;a href=&quot;#触控事件—MotionEvent&quot; class=&quot;headerlink&quot; title=&quot;触控事件—MotionEvent&quot;&gt;&lt;/a&gt;触控事件—MotionEvent&lt;/h3&gt;&lt;p&gt;MotionEvent中封装的常用事件常量:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; fianl &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ACTION_DOWN = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 单点触摸 按下&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; fianl &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ACTION_UP = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 单点触摸 离开&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; fianl &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ACTION_MOVE = &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 触摸点 移动&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; fianl &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ACTION_CANCEL = &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 触摸动作 取消&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; fianl &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ACTION_OUTSIDE = &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 触摸动作 超出边界&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; fianl &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ACTION_POINTER_DOWN = &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 多点触摸 按下&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; fianl &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ACTION_POINTER_UP = &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;; &lt;span class=&quot;comment&quot;&gt;// 多点触摸 离开&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;实现触摸实现的代码套路:在&lt;code&gt;onTouchEvent(MotionEvent event)&lt;/code&gt;中通过&lt;code&gt;event.getAction()&lt;/code&gt;获取触控事件类型，并且用switch-case语句来筛选我们所需要的动作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Android" scheme="https://tobiaslee.top/tags/Android/"/>
    
      <category term="自定义View" scheme="https://tobiaslee.top/tags/%E8%87%AA%E5%AE%9A%E4%B9%89View/"/>
    
  </entry>
  
  <entry>
    <title>PythonLearning-Iterator</title>
    <link href="https://tobiaslee.top/2017/05/28/PythonLearning-Iterator/"/>
    <id>https://tobiaslee.top/2017/05/28/PythonLearning-Iterator/</id>
    <published>2017-05-28T02:41:28.000Z</published>
    <updated>2017-08-15T14:57:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    迭代器和生成器应该算是Python的黑魔法，一开始学习的时候还是感觉相当的迷糊的，这篇文章就记录一下学习生成器和迭代器的过程。</p><h2 id="Iterator-迭代器"><a href="#Iterator-迭代器" class="headerlink" title="Iterator 迭代器"></a>Iterator 迭代器</h2><p>首先问一个问题：迭代器是什么？</p><blockquote><ul><li>迭代协议：实现<code>__next__()</code>  方法的对象会前进到下一个结果，而在到达一系列结果的末尾时，会引发 StopIteration 异常</li><li>任何遵循上述迭代协议的对象，都被认为是迭代器</li><li>任何可迭代对象都能通过 for 循环或其它迭代工具遍历。实际上，所有迭代工具内部都是在每次迭代中调用迭代器的 <strong>next</strong>() 方法，并通过捕捉 StopIteration 异常来确定何时离开</li></ul></blockquote><p>但是在这个定义之前，我们其实已经在无数场合用过它了: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> my_list:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><a id="more"></a><p>这里的for语句中的my_list，实际上是通过调用了Python的build-in function<code>iter()</code>获取到的一个迭代器对象。</p><p>随后通过for循环，对它不断地执行迭代协议。</p><p>所以我们可以通过实现<code>__iter__()</code>和<code>next()/__next__()</code>方法来手搓一个简单的迭代器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyListIter</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, list_data)</span>:</span></span><br><span class="line">        <span class="comment"># 其迭代的内容来自于list_data 用一个list来维护</span></span><br><span class="line">        self.list_data = list_data</span><br><span class="line">        self.index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 迭代的计算发生在__next__函数中</span></span><br><span class="line">        <span class="keyword">if</span> self.index &lt; len(self.list_data):</span><br><span class="line">            <span class="comment"># 这里是把列表中的值取出作为返回值</span></span><br><span class="line">            val = self.list_data[self.index]</span><br><span class="line">            <span class="comment"># 下标增加 指向后一个元素</span></span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> val</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration()</span><br></pre></td></tr></table></figure><p>这里我们定义了一个简单的MyListIter对象，<strong>其<code>__iter__()</code>函数返回它自身</strong>，那么后续的迭代协议就是在<strong>调用它自身的<code>__next__()</code>方法</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_list = MyListIter([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">it = iter(my_list) <span class="comment"># 获取一个iterator对象 其实现了__next__()方法 这里就是它自己</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> it <span class="comment"># 写成 for i in my_list 也完全可以</span></span><br><span class="line">print(i) <span class="comment"># 1 2 3</span></span><br><span class="line">print(next(it)) <span class="comment"># StopIteration 停止迭代 index = 3迭代器已经用完了</span></span><br></pre></td></tr></table></figure><p>需要注意的一点是，因为这里我们的<code>__next__()</code>方法的实现其实是通过其自身下标指针这一成员，不断自增，来实现取得下一个元素的。</p><p>所以当我们在for循环中使用过it进行过一次完整的从头到尾的迭代之后，index的值已经变成了3，再次调用next()方法就会出现<code>StopIteration</code>这一Exception。</p><p><strong>很多迭代器的实现都是通过其内部成员的变化来实现迭代，所以迭代器基本就只能用一次，用完就无法再进行迭代了!</strong></p><h2 id="Generator-和-yield"><a href="#Generator-和-yield" class="headerlink" title="Generator 和 yield"></a>Generator 和 yield</h2><p>问题：求无数多个素数，怎么实现？</p><p>因为不可能存的下无数个素数，自然会想到迭代协议，不断的next计算出下一个素数并且返回，就好了。</p><p>有个形象的比喻：</p><blockquote><p>有个人很喜欢吃M&amp;M豆，一天他想吃很多很多的M&amp;M豆，怎么办?</p><p>非生成器的实现，就是把所有的M&amp;M豆交给他，可能堆积如山，甚至出现放不下的情况。</p><p>而生成器就给了他一台可以生产M&amp;M豆的机器，上面有一个按钮，按一下(调用next()方法)，出一颗M&amp;M豆。</p><p>这样就可以获得无穷多的M&amp;M豆了！</p></blockquote><p>​    可是，如果按照上面的MyListIter的实现，我内部需要持有一个成员来保存当前计算的数，可能还要重写两个方法，太麻烦了不是吗？</p><p>​    这样实现很大的一个原因是因为<strong>传统的函数只有一次返回的机会，无法多次返回</strong>,而迭代就意味着我们需要保存当前的上下文，从而根据上一次的结果计算得到下一次，因而这里就用类的成员来作为这个上下文的记录者。</p><p>​    有没有简单的一点的方法呢？</p><p>​    Yes! 那就是<code>yield</code>关键字</p><h3 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h3><p>yield关键字,能够让函数变成一个生成器，那么什么是生成器呢？</p><blockquote><ul><li><p>Generator functions create generator iterators.</p><p>生成器函数产生一个生成器迭代器</p></li><li><p>Generator iterator are almost always referred to as “generators”. </p><p>生成器迭代器一般就叫做生成器</p></li><li><p>Just remember that a generator is a special type of iterator.</p><p>生活器就是一种特别的迭代器</p></li><li><p>To be considered an iterator, generators must define a few methods, one of which is <strong>next</strong>().</p><p>作为一种迭代器，生成器必须实现几个方法，包括next()</p></li></ul></blockquote><p>说白了，就是生成器和迭代器类似，是可以不断地对他调用next()函数，</p><p>也可以应用在<code>for i in generator:</code>这样的语句之中的一种对象。</p><p>yield关键字的作用和return类似，它返回一个值。</p><p>但是！！！<strong>这个值的当前状态会被保存，而不同于普通函数结束之后内存会被回收掉</strong></p><p>这就是生成器的作用了。</p><p>其最核心的特点就是<strong>惰性计算</strong>，优点就是<strong>节省空间</strong>。</p><p>来看一个例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">()</span>:</span></span><br><span class="line">    prev, curr = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">yield</span> curr</span><br><span class="line">        prev, curr = curr, prev + curr</span><br><span class="line">f = fibonacci() <span class="comment"># 得到一个生成器</span></span><br><span class="line">next(f) <span class="comment"># 1</span></span><br><span class="line">next(f) <span class="comment"># 1</span></span><br><span class="line">next(f) <span class="comment"># 2</span></span><br><span class="line">next(f) <span class="comment"># 4</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>在这里我们定义了一个<code>fibonacci()</code>函数，来求斐波那契数列，理论上，它可以求无限多个斐波那契。</p><p>为什么呢？因为我们每计算出一个curr（当前的斐波那契数) 就通过yield关键字将它返回。</p><p>我们通过<code>f = fibonacci()</code> 获取了一个生成器</p><p>而当我们调用next(f)时，程序会在初始化prev和curr后会进入循环体，并且在yield关键字处返回curr的值并保存当前上下文，再下一次调用时会根据上下文从yield关键字之后继续执行。</p><p>所以我们看到在不断执行next(f)会获得一个又一个的斐波那契数。</p><p>那么回到我们一开始的那个问题，求无限个素数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_prime</span><span class="params">(number)</span>:</span></span><br><span class="line">    <span class="string">'''判断素数函数'''</span></span><br><span class="line">    <span class="keyword">if</span> number &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> number == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> number % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> current <span class="keyword">in</span> range(<span class="number">3</span>, int(mat.sqrt(number) + <span class="number">1</span>), <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> number % current == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prime</span><span class="params">()</span>:</span></span><br><span class="line">    cnt = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">if</span>(is_prime(cnt)):</span><br><span class="line">            <span class="keyword">yield</span> cnt <span class="comment"># return cnt 并且保存其当前的值</span></span><br><span class="line">        cnt += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>​    总结一下的话，<strong>generator就是函数版的iterator，其通过yield关键字来实现迭代状态的保存，更加简洁</strong>。</p><h2 id="Java中的Iterator-和-Iterable-接口"><a href="#Java中的Iterator-和-Iterable-接口" class="headerlink" title="Java中的Iterator 和 Iterable 接口"></a>Java中的Iterator 和 Iterable 接口</h2><p>其实我看到迭代器的第一反应就Java里的Iterator和Iterable这两个接口。</p><p>接口定义:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Iterable</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function">Iterator&lt;T&gt; <span class="title">iterator</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Iterator</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function">E <span class="title">next</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>两个接口有什么区别呢？</p><blockquote><p>An <code>Iterable</code> is a simple representation of a series of elements that can be iterated over. It does not have any iteration state such as a “current element”. Instead, it has one method that produces an <code>Iterator</code>.</p><p>An <code>Iterator</code> is the object with iteration state. It lets you check if it has more elements using <code>hasNext()</code> and move to the next element (if any) using <code>next()</code>.</p><p>Typically, an <code>Iterable</code> should be able to produce any number of valid <code>Iterator</code>s.</p><p>摘自StackOverFlow</p></blockquote><p>实际上Java的两个接口的设计和Python的实现非常类似，</p><p>实现了Iterable接口返回一个迭代器，而不在乎迭代器的内部实现（也就是iteration state的保存），甚至可能根据需求返回不同iterator。</p><p>而在Iterator内部则需要实现<code>hasNext()</code>和<code>next()</code>这两个方法，用的很多的场合就是 <code>for-each loop</code>里面。</p><p>看个简单的例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterator</span> <span class="keyword">implements</span> <span class="title">Iterator</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[] myList;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> index;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MyIterator</span><span class="params">(<span class="keyword">int</span>[] myList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.myList = myList;</span><br><span class="line">        <span class="keyword">this</span>.index = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> index &lt; myList.length;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> myList[index++];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] list = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">        MyIterator myIterator = <span class="keyword">new</span> MyIterator(list);</span><br><span class="line">        <span class="keyword">while</span>(myIterator.hasNext()) &#123;</span><br><span class="line">            System.out.println(myIterator.next());</span><br><span class="line">          <span class="comment">// 1 2 3</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和Python非常的类似，只不过Java没有yield这种黑科技，不再赘述了。（Java代码真的长…)</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ol><li><strong>迭代器就是符合迭代协议的对象</strong></li><li><strong>生成器是通过yield关键字来产生迭代器的函数</strong></li><li><strong>Java中类似的实现的Iterator和Iterable接口</strong></li><li><strong>多多纵向对比语言的实现，可以帮助更好的理解</strong></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    迭代器和生成器应该算是Python的黑魔法，一开始学习的时候还是感觉相当的迷糊的，这篇文章就记录一下学习生成器和迭代器的过程。&lt;/p&gt;
&lt;h2 id=&quot;Iterator-迭代器&quot;&gt;&lt;a href=&quot;#Iterator-迭代器&quot; class=&quot;headerlink&quot; title=&quot;Iterator 迭代器&quot;&gt;&lt;/a&gt;Iterator 迭代器&lt;/h2&gt;&lt;p&gt;首先问一个问题：迭代器是什么？&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;迭代协议：实现&lt;code&gt;__next__()&lt;/code&gt;  方法的对象会前进到下一个结果，而在到达一系列结果的末尾时，会引发 StopIteration 异常&lt;/li&gt;
&lt;li&gt;任何遵循上述迭代协议的对象，都被认为是迭代器&lt;/li&gt;
&lt;li&gt;任何可迭代对象都能通过 for 循环或其它迭代工具遍历。实际上，所有迭代工具内部都是在每次迭代中调用迭代器的 &lt;strong&gt;next&lt;/strong&gt;() 方法，并通过捕捉 StopIteration 异常来确定何时离开&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;但是在这个定义之前，我们其实已经在无数场合用过它了: &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;my_list = [&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; my_list:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(i)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://tobiaslee.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫学习-1</title>
    <link href="https://tobiaslee.top/2017/05/21/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0-1/"/>
    <id>https://tobiaslee.top/2017/05/21/Python爬虫学习-1/</id>
    <published>2017-05-21T00:41:49.000Z</published>
    <updated>2017-05-21T03:11:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    前几天看了Google I/O，主题是”Mobile First to AI First”，然后又去听了一场微软工程师关于Machine Learning，主要是数据分析方面的讲座。很明显的感受到，移动互联网的浪潮正在慢慢退去，而AI、ML浪潮蓄势待发。记得吴军在《浪潮之巅》里讲，能够成为浪潮中的弄潮儿是很幸运的。但是，傻呆呆地在岸上等待，只会被巨浪裹挟，然后被拍死；而只有迎面而上，才能拥抱浪潮。</p><p>​    废话了那么多，大致意思就是：我要学习ML，避免落后。而学习ML，需要大量的数据，数据从哪来？Python爬虫。So，开始！</p><h2 id="Python爬虫"><a href="#Python爬虫" class="headerlink" title="Python爬虫"></a>Python爬虫</h2><p>​    爬虫嘛，也不是第一次听说，就是把网页的内容抓取下来，然后进行分析处理。主要会用到的库大概有urllib, requests, BeautifulSoup4等等。然后前天貌似要做教师测评，于是乎就想能不能写个爬虫模拟登陆一下教务系统，然后帮我把测评做了？在HeroHR的帮助下，用urllib实现了查成绩（虽然和评测有点区别），后来听说requests才是“HTTP for Humans”(给人用的HTTP，和Java的手搓Socket比起来，确实啊)，然后就又用requests库写了一个。</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>​    登陆的过程大致分两步:</p><pre><code>1. POST账号密码2. 服务器交给浏览器一个Cookie，以辨别身份</code></pre><p>当然有些时候很多网站会设置一些token来保证安全或者防止抓取，就需要我们从网页源代码中入手来观察。</p><a id="more"></a><p>​    首先，我们就打开教务处的网站，然后登陆一下，并且通过Chrome的检查选项来观察在登陆过程中我们给服务器POST了哪些数据</p><blockquote><p><a href="http://ids.xidian.edu.cn/authserver/login?service=http%3A%2F%2Fjwxt.xidian.edu.cn%2Fcaslogin.jsp" target="_blank" rel="noopener">http://ids.xidian.edu.cn/authserver/login?service=http%3A%2F%2Fjwxt.xidian.edu.cn%2Fcaslogin.jsp</a></p></blockquote><p><img src="/img/form_data.png" alt="Form Data"></p><p>​    我们看到POST的表单中有username, password, 以及一串很长的奇怪的字符串，目测这个就是网页来保证安全的token了，于是乎我们打开网页源代码，来查找一下这段“奇怪的字符串”。</p><p><img src="/img/lt.png" alt="Find LT"></p><p>​    哈哈哈，果不其然，它就藏在这个网页里。还有一些其他的参数例如_eventId, rmShown, execution,登陆几次之后就发现，前两个的值是固定的，而后面一个似乎是登陆的次数，第一次登陆的话他的值都是“e1s1”。那么我们只要先爬出这个网页，然后用正则匹配出lt，再作为post的表单数据POST给服务器就可以了。</p><h2 id="模拟登陆"><a href="#模拟登陆" class="headerlink" title="模拟登陆"></a>模拟登陆</h2><p>准备工作完成，接下来，就可以来写代码啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入需要的库</span></span><br><span class="line"><span class="keyword">import</span> urllib.request, urllib.parse, urllib</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"></span><br><span class="line">login_url = <span class="string">"http://ids.xidian.edu.cn/authserver/login?service=http%3A%2F%2Fjwxt.xidian.edu.cn%2Fcaslogin.jsp"</span></span><br><span class="line">header = &#123; &#125; <span class="comment"># 省略篇幅，浏览器的Header，是一个字典，用来伪装爬虫</span></span><br><span class="line"></span><br><span class="line">cj = http.cookiejar.CookieJar() <span class="comment"># cookiejar 用来保存cookie 为后面查询成绩做准备</span></span><br><span class="line">opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一次请求，获取我们需要的lt</span></span><br><span class="line">first_request = urllib.request.Request(login_url, headers=header)</span><br><span class="line">login_page = opener.open(first_request).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment"># 正则表达式匹配LT</span></span><br><span class="line">lt = re.findall(<span class="string">r'value="LT.*"'</span>, login_page)[<span class="number">0</span>]</span><br><span class="line">lt = lt[<span class="number">7</span>:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">        <span class="string">'username'</span>: get_username(),</span><br><span class="line">        <span class="string">'password'</span>: get_pwd(),</span><br><span class="line">        <span class="string">'submit'</span>: <span class="string">''</span>,</span><br><span class="line">        <span class="string">'lt'</span>: lt,</span><br><span class="line">        <span class="string">'execution'</span>: <span class="string">'e1s1'</span>,</span><br><span class="line">        <span class="string">'_eventId'</span>: <span class="string">'submit'</span>,</span><br><span class="line">        <span class="string">'rmShown'</span>: <span class="string">'1'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 第二次请求 登陆发生在这里，传入我们的url，和需要提交的表单，不过需要手动的编码一下，让他符合url的形式</span></span><br><span class="line">second_request = urllib.request.Request(</span><br><span class="line">    login_url,</span><br><span class="line">    urllib.parse.urlencode(data).encode(<span class="string">'utf-8'</span>),</span><br><span class="line">    header)</span><br><span class="line">r = opener.open(second_request)</span><br><span class="line"><span class="comment"># 查看登陆后的网页</span></span><br><span class="line">print(r.read())</span><br></pre></td></tr></table></figure><p>这里我们用到了一个opener对象，看官方文档:</p><blockquote><p>class urllib.request.build_opener([<em>handler</em>, <em>…</em>])</p><p>Return an <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.OpenerDirector" target="_blank" rel="noopener"><code>OpenerDirector</code></a> instance, which chains the handlers in the order given. </p></blockquote><p>还是不知道他在说什么，不过既然是一个OpenerDirector实例，那就再去看看它是什么：</p><blockquote><p>class urllib.request.OpenerDirector</p><p>The <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.OpenerDirector" target="_blank" rel="noopener"><code>OpenerDirector</code></a> class opens URLs via <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener"><code>BaseHandler</code></a>s chained together. It manages the chaining of handlers, and recovery from errors.</p><p>class urllib.request.BaseHandler</p><p>This is the base class for all registered handlers — and handles only the simple mechanics of registration.</p></blockquote><p>大致明白了，就是用带有一些Handler的url打开方式，而这里我们实际上穿进去的参数是HTTPCookieProcessor，用于处理服务器交给我们的Cookie。</p><p>​    还有一点需要注意下，<strong>因为网页的内容是流式的，我们请求一次，服务器发送一次，所以如果我们对一个response读取两次，第二次就读不到任何东西</strong> (掉进这个坑里，然后不知道为什么，幸好全栈的HeroHR及时提醒了我)。</p><p>​    到这里，我们的模拟登陆就完成了。</p><h2 id="查询成绩"><a href="#查询成绩" class="headerlink" title="查询成绩"></a>查询成绩</h2><p>登陆进去之后，在教务系统里找到查成绩的连接:</p><blockquote><p>[2016-2017学年第一学期(两学期) ]</p><p><a href="http://jwxt.xidian.edu.cn/gradeLnAllAction.do?type=ln&amp;oper=qbinfo&amp;lnxndm=2016-2017%D1%A7%C4%EA%B5%DA%D2%BB%D1%A7%C6%DA(%C1%BD%D1%A7%C6%DA)" target="_blank" rel="noopener">http://jwxt.xidian.edu.cn/gradeLnAllAction.do?type=ln&amp;oper=qbinfo&amp;lnxndm=2016-2017%D1%A7%C4%EA%B5%DA%D2%BB%D1%A7%C6%DA(%C1%BD%D1%A7%C6%DA)</a></p></blockquote><p>后面那一坨是什么鬼???</p><p>HeroHR和我一开始觉得可能是学号，于是对着一个一个的数，因为比较长，数了两遍，发现位数对不上，然后我想可能是随机数？机智的他发现了括号，于是猜测是不是学期，然后又数了一遍，对上了!!!转成gbk，Perfect!!</p><p>Keep Going!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scores的url</span></span><br><span class="line">scores = opener.open(urllib.request.Request(</span><br><span class="line">    <span class="string">"http://jwxt.xidian.edu.cn/gradeLnAllAction.do?type=ln&amp;oper=qbinfo&amp;lnxndm=2016-2017&#123;&#125;(&#123;&#125;)"</span>.format(urllib.parse.quote(<span class="string">"学年第一学期"</span>, encoding=<span class="string">'gbk'</span>), urllib.parse.quote(<span class="string">"两学期"</span>, encoding=<span class="string">'gbk'</span>)),</span><br><span class="line">    headers=header))</span><br><span class="line"></span><br><span class="line">scores_page = scores.read().decode(<span class="string">'gbk'</span>)</span><br><span class="line">print(scores_page)</span><br></pre></td></tr></table></figure><p>打印出来的成绩的结果是这样的:</p><p><img src="/img/score.png" alt="score"></p><p>这要是用正则来匹配，就很麻烦也费时间，于是BeautifulSoup4出场了。</p><blockquote><p><a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank" rel="noopener">Beautiful Soup</a> is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.</p></blockquote><p>它的工作原理（个人猜测）就是以标签为节点来遍历整个HTML的文档（因为HTML和XML是标签化的语言），然后建立起一颗树，那么我们就可以迅速的遍历这颗树来得到我们需要的信息。</p><p>这里我们可以看到，课程信息在<code>&lt;tr&gt;</code>这个标签里，而成绩在<code>&lt;p&gt;</code>里面。所以我们只要提取这些标签，拿到他们的内容就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建两个bs对象， 指定parser为'html5lib'</span></span><br><span class="line">bs_obj1 = BeautifulSoup(scores_page, <span class="string">'html5lib'</span>)</span><br><span class="line">bs_obj2 = BeautifulSoup(scores_page, <span class="string">'html5lib'</span>)</span><br><span class="line"><span class="comment"># 找到tr标签里属性为class = odd 的和 p</span></span><br><span class="line">tags = bs_obj1.find_all(<span class="string">'tr'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">"odd"</span>&#125;)</span><br><span class="line">score_list = bs_obj2.find_all(<span class="string">'p'</span>)</span><br></pre></td></tr></table></figure><p>tag是bs的一个类，他可能包括他的子节点(tag)和一个叶子节点(没有子tag的节点，type为NavigationString)</p><p>我们可以通过tag.contents获取内容列表，经过对内容的分析，我发现tag.content[5]就是课程的名字，所以用一个list存起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">course_list = []</span><br><span class="line">grade_list = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_courses</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> tag <span class="keyword">in</span> tags:</span><br><span class="line">        <span class="comment"># 取出课程名，然后除去空格</span></span><br><span class="line">        course_list.append(tag.contents[<span class="number">5</span>].contents[<span class="number">0</span>].strip())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_scores</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> score <span class="keyword">in</span> score_list:</span><br><span class="line">         <span class="comment"># 取出成绩，然后除去空格</span></span><br><span class="line">        grade_list.append(score.contents[<span class="number">0</span>].strip())</span><br><span class="line">gen_courses()</span><br><span class="line">gen_scores()</span><br><span class="line"><span class="comment"># 打包生成一个dic</span></span><br><span class="line">dic = dict(zip(course_list, grade_list))</span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> dic.items():</span><br><span class="line">    print(key, value)</span><br></pre></td></tr></table></figure><p>于是，大功告成！</p><p>输出的结果:</p><blockquote><p>军事理论 80.0<br>军事训练 80.0<br>大学英语(I) 免修<br>高级英语（Ⅰ） 80.0<br>思想道德修养与法律基础 80.0<br>大学生职业发展 通过<br>形势与政策（Ⅰ） 80.0<br>大学体育(I) 80.0<br>高等数学A(I) 80.0<br>大学物理（Ⅰ） 80.0<br>计算机导论（Ⅰ） 通过<br>计算机导论（Ⅱ） 80.0<br>新生研讨课 80.0<br>专业教育（Ⅰ） 通过<br>计算机系统组装实习 优秀<br>应用创造学 优秀</p></blockquote><p>(你问我为什么都是80??因为我改了不想给你看啊哈哈哈哈哈)</p><h2 id="用requests来实现"><a href="#用requests来实现" class="headerlink" title="用requests来实现"></a>用requests来实现</h2><p>​    用号称HTTP for Humans的requests库实现模拟登陆查成绩，是怎么样的呢?直接上代码！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">s = requests.Session()</span><br><span class="line">r = s.get(login_url)</span><br><span class="line">login_page = r.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理login_page 得到lt 填入data</span></span><br><span class="line">data = &#123; &#125;<span class="comment"># 节省篇幅</span></span><br><span class="line">s.post(login_url, data)</span><br><span class="line">score_url = <span class="string">"..."</span> <span class="comment"># 同上面的score_url </span></span><br><span class="line">res = s.get(score_url)</span><br><span class="line">scores_page = res.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面用bs4处理 得到数据 同urllib实现</span></span><br></pre></td></tr></table></figure><p>除去注释和空格，核心代码只要8行!!!</p><p><strong>8行，你写不了Java的一个Hello World，你可以模拟教务系统登录查成绩啊</strong></p><p>其中很重要的一个类，Session，是它帮我们做了很多幕后的工作：    </p><blockquote><h3 id="Session-Objects"><a href="#Session-Objects" class="headerlink" title="Session Objects"></a>Session Objects</h3><p>The Session object allows you to persist certain parameters across requests. It also persists cookies across all requests made from the Session instance, and will use <code>urllib3</code>‘s <a href="http://urllib3.readthedocs.io/en/latest/reference/index.html#module-urllib3.connectionpool" target="_blank" rel="noopener">connection pooling</a>. So if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase (see <a href="https://en.wikipedia.org/wiki/HTTP_persistent_connection" target="_blank" rel="noopener">HTTP persistent connection</a>).</p></blockquote><p>会话对象，持续的请求，保存Cookie。自动的TCP连接复用。</p><p>强爆了，不愧是给人用的！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    一直和HeroHR争Java和Python哪个好，其实语言只是工具，没有高低之分，怎么用才是最重要的。</p><p>​    在某些领域比如爬虫、科学计算，Python写起来简单，库多而全面。Java也同样有自己的领域。</p><p>​    爬虫之旅才刚刚开始，机器学习似乎因为数学知识储备还不够，可能得缓一缓，但学爬虫也是为了机器学习做准备啊。</p><p>​    <strong>路漫漫其修远兮，吾将上下而求索。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    前几天看了Google I/O，主题是”Mobile First to AI First”，然后又去听了一场微软工程师关于Machine Learning，主要是数据分析方面的讲座。很明显的感受到，移动互联网的浪潮正在慢慢退去，而AI、ML浪潮蓄势待发。记得吴军在《浪潮之巅》里讲，能够成为浪潮中的弄潮儿是很幸运的。但是，傻呆呆地在岸上等待，只会被巨浪裹挟，然后被拍死；而只有迎面而上，才能拥抱浪潮。&lt;/p&gt;
&lt;p&gt;​    废话了那么多，大致意思就是：我要学习ML，避免落后。而学习ML，需要大量的数据，数据从哪来？Python爬虫。So，开始！&lt;/p&gt;
&lt;h2 id=&quot;Python爬虫&quot;&gt;&lt;a href=&quot;#Python爬虫&quot; class=&quot;headerlink&quot; title=&quot;Python爬虫&quot;&gt;&lt;/a&gt;Python爬虫&lt;/h2&gt;&lt;p&gt;​    爬虫嘛，也不是第一次听说，就是把网页的内容抓取下来，然后进行分析处理。主要会用到的库大概有urllib, requests, BeautifulSoup4等等。然后前天貌似要做教师测评，于是乎就想能不能写个爬虫模拟登陆一下教务系统，然后帮我把测评做了？在HeroHR的帮助下，用urllib实现了查成绩（虽然和评测有点区别），后来听说requests才是“HTTP for Humans”(给人用的HTTP，和Java的手搓Socket比起来，确实啊)，然后就又用requests库写了一个。&lt;/p&gt;
&lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;p&gt;​    登陆的过程大致分两步:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. POST账号密码
2. 服务器交给浏览器一个Cookie，以辨别身份
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当然有些时候很多网站会设置一些token来保证安全或者防止抓取，就需要我们从网页源代码中入手来观察。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://tobiaslee.top/tags/Python/"/>
    
      <category term="爬虫" scheme="https://tobiaslee.top/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>自定义View初探</title>
    <link href="https://tobiaslee.top/2017/05/14/%E8%87%AA%E5%AE%9A%E4%B9%89View%E5%88%9D%E6%8E%A2/"/>
    <id>https://tobiaslee.top/2017/05/14/自定义View初探/</id>
    <published>2017-05-14T03:03:50.000Z</published>
    <updated>2017-05-14T08:30:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    这周敲完了《第一行代码》的最后一个项目酷欧天气，然后开始，第二本书《Android群英传》的敲代码之旅。虽然说项目驱动才是最好的学习方式，但照书敲然后思考，也不失为一种好办法。算是迈出Android进阶的第一步吧。</p><h2 id="Android控件架构"><a href="#Android控件架构" class="headerlink" title="Android控件架构"></a>Android控件架构</h2><p>​    Android里的控件，大致被分为两类，<strong>ViewGroup 和 View</strong></p><p>​    从字面上来看，ViewGroup就是一组View的集合，可以通过它来管理控制其包含的View。通过它，界面上的所有控件也就能够形成一个树形结构，也就是常说的<strong>控件树</strong>，上层控件负责下层子控件的测量与绘制，并且传递交互事件。我们在Activity里经常使用的findViewById()方法，就是在控件树中通过DFS来查找的（数据结构还是很有用的）。而对于每一颗控件树，都有一个ViewParent对象，这就是整颗树的控制核心，<strong>所有的交互管理事件都由它来进行统一调度和分配</strong>。大概就是下面这张图的样子：</p><p><img src="/img/view_tree.png" alt="ViewTree"></p><a id="more"></a><p>​    还记得写Activity的时候都要用setContentView()方法来加载布局，之后才会显示其中的内容，那这个方法究竟干了什么？首先要看Android的界面架构图：</p><p><img src="/img/activity.jpg" alt="ActivityStructure"></p><blockquote><p>每个Activity对象都包含一个Window对象，一般由PhoneWindow来实现。PhonwWindow将DecorView设置为整个应用窗口的根View。DecorView封装了一些窗口操作的通用方法，DecorView将显示的具体内容呈现在了PhoneWindow上，其中所有View的监听事件都是通过WindowManagerService来接收，并且通过Activity对象来回调相应的onClickListener。</p><p>​                                        ——    《Android群英传》</p></blockquote><p>其实这里最最简单的例子就是Button的点击监听，屏幕将点击事件传递给WindowManagerService，然后调用了我们在Button上写的setOnClickListener()方法来实现点击的逻辑。</p><p>这里书中提到了两个需要注意的点：</p><ol><li>requestWindowFeature()方法一定要在setContentView()方法之前调用，否则会无效（我就踩过这个坑），原因是 <strong>一般的Window布局由TitleView和ContentView构成，如果要设置一些特性（比如NO_TITLE），应该要在加载ContentView之前进行设置，否则，界面已经加载了TitleView，再调用方法也就会无效</strong></li></ol><ol><li>程序在onCreate()方法中调用了setContentView()后，ActivityManagerService会回调onResume()方法，此时系统才会把整个DecorView添加到PhoneWindow中，并且让它显示出来。</li></ol><h2 id="View的绘制"><a href="#View的绘制" class="headerlink" title="View的绘制"></a>View的绘制</h2><p>​    在自定义View之前，我们需要知道View是怎么绘制出来的。</p><p>​    主要操作的有三个方法<code>onMeasure()</code> 、<code>onDraw()</code>、<code>onLayout()</code></p><p>​    其实也很好理解，就和一个人要画画一样，他首先要知道画的东西的大小(onMeasure()负责测量大小）,然后是画在画布的哪里(onLayout()指导他画在那里)，再就是怎么画了(onDraw()方法来告诉它怎么画)。</p><h3 id="onMeasure-方法"><a href="#onMeasure-方法" class="headerlink" title="onMeasure()方法"></a>onMeasure()方法</h3><p>​    Android系统为我们的测量提供了一个类<code>MeasureSpec</code>，通过它来帮助我们测量View。MeasureSpec是一个32位的int，其中高2位代表测量的模式，利用位运算可以提高效率。</p><p>测量的模式分为3种:</p><ol><li>EXACTLY 精确值模式，当使用固定数值或者指定为match_parent时使用</li><li>AT_MOST 最大值模式，控件指定为warp_content时，随着控件大小变化而变化，不超过父控件允许的最大尺寸即可</li><li><p>UNSPECIFIED 未明确模式，主要用在自定View的绘制中</p><p>​View类默认的onMeasure()方法<strong>只支持EXACTLY</strong>模式，这就意味着如果要使用其他模式就必须要重写该方法。如果要让自定义View支持warp_content属性，就要在onMeasure()方法中来指定warp_content时的大小。</p><p>​另外如果仔细研究onMeasure()方法，会发现系统最终调用setMeasuredDimension()方法来吧测量后的宽和高交给view。</p><p>​总而言之，我们通过MeasureSpec这一个类能够获取View的测量模式和绘制的大小，从而控制其显示时候的大小。</p></li></ol><h3 id="onLayout-方法"><a href="#onLayout-方法" class="headerlink" title="onLayout()方法"></a>onLayout()方法</h3><p>​    控件的onLayout()方法一般都是通过实现其父控件(一般也就是一个ViewGroup)的onLayout()来实现的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">onLayout</span><span class="params">(<span class="keyword">boolean</span> changed,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">int</span> l, <span class="keyword">int</span> t, <span class="keyword">int</span> r, <span class="keyword">int</span> b)</span></span>;</span><br></pre></td></tr></table></figure><p>​    在自定义View中，onLayout()配合onMeasure()方法一起使用，可以实现自定义View的复杂布局。自定义View首先调用onMeasure进行测量，然后调用onLayout方法，动态获取子View和子View的测量大小，然后进行layout布局。</p><h3 id="onDraw-方法"><a href="#onDraw-方法" class="headerlink" title="onDraw()方法"></a>onDraw()方法</h3><p>​    画画首先需要的材料就是画布和画笔。而在Android之中正好就有两个系统2D绘图API对象Canvas和Paint。</p><p>​    Canvas就是我们的画布，不过我们在创建的时候，需要传入一个Bitmap对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Canvas canvas = <span class="keyword">new</span> Canvas(bitmap);</span><br></pre></td></tr></table></figure><p>​    传进去的Bitmap对象和Canvas画布是紧密相连的，这个过程我们称之为装载画布。Canvas.drawXXX方法都发生在这个bitmap上，bitmap储存所有绘制在Canvas上的像素信息。</p><p>​    <strong>bitmap承载Canvas的一系列绘图操作，图形的改变是通过Bitmap的改变，然后让View重新绘制来实现的</strong></p><p>​    Paint类就是一只画笔，接下来我们来结合一个自定义View的实例来看Paint和相关方法的使用。    </p><h2 id="自定义View实例-流光TextView"><a href="#自定义View实例-流光TextView" class="headerlink" title="自定义View实例 流光TextView"></a>自定义View实例 流光TextView</h2><p>实现自定义View大致有三种方法:</p><ol><li>对现有控件进行拓展</li><li>通过组合来实现新的控件</li><li>重写View来实现全新的控件</li></ol><p>这里就拿书本上的一个简单的例子，使用第一种方法拓展TextView，实现一个</p><p>流光字的TextView</p><p>既然是拓展，第一反应就是继承TextView，然后重写相关的方法。没错，就是这么简单。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTextView</span> <span class="keyword">extends</span> <span class="title">android</span>.<span class="title">support</span>.<span class="title">v7</span>.<span class="title">widget</span>.<span class="title">AppCompatTextView</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> Paint mPaint;</span><br><span class="line">    <span class="keyword">private</span> Matrix mGradientMatrix;</span><br><span class="line">    <span class="keyword">private</span> LinearGradient mLinearGradient;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mTranslate = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mViewWidth;</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// 构造方法 </span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyTextView</span><span class="params">(Context context, AttributeSet attrs)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(context, attrs);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个类里我们定义了一些成员变量，暂时先不管，然后写了一个构造方法，很简单，这就是一个简单的继承嘛。如果我们不去重写相关的方法，那么这个类和TextView其实是没有任何区别的。</p><p>要实现流光字，利用Paint对象的Shader渲染器。通过设置不断变化的LinearGradient(线性渐变)，然后用带有该属性的Paint来绘制文字（就好比在画笔上沾了一些特殊的颜料，画出来自然是带特效的）。我们先要在onSizeChanged()方法中做一些初始化:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onSizeChanged</span><span class="params">(<span class="keyword">int</span> w, <span class="keyword">int</span> h, <span class="keyword">int</span> oldw, <span class="keyword">int</span> oldh)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.onSizeChanged(w, h, oldw, oldh);</span><br><span class="line">        <span class="keyword">if</span>(mViewWidth == <span class="number">0</span>) &#123;</span><br><span class="line">            mViewWidth = getMeasuredWidth();</span><br><span class="line">            <span class="keyword">if</span>(mViewWidth &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                mPaint = getPaint(); <span class="comment">// 拿到我们的画笔</span></span><br><span class="line">                mLinearGradient = <span class="keyword">new</span> LinearGradient(<span class="number">0</span>, <span class="number">0</span>, mViewWidth,</span><br><span class="line">                        <span class="number">0</span>, <span class="keyword">new</span> <span class="keyword">int</span>[] &#123;Color.BLUE, <span class="number">0xffffffff</span>,</span><br><span class="line">                Color.BLUE&#125;, <span class="comment">// 构造一个LinearGradient 原生的TextView是没有这个属性的</span></span><br><span class="line">                        <span class="keyword">null</span>,</span><br><span class="line">                        Shader.TileMode.CLAMP);</span><br><span class="line">                mPaint.setShader(mLinearGradient);</span><br><span class="line">                mGradientMatrix = <span class="keyword">new</span> Matrix();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>然后，我们在onDraw()方法中不断用矩阵平移渐变效果，就可以实现流光字了。（和我以前在ps里面制作流光字的讨论差不多)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onDraw</span><span class="params">(Canvas canvas)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 代码如果写在这里，就是发生在文字绘制之前</span></span><br><span class="line">      <span class="keyword">super</span>.onDraw(canvas); <span class="comment">// 绘制文字</span></span><br><span class="line"><span class="comment">// 代码写在这里，发生在文字绘制之后</span></span><br><span class="line">      <span class="keyword">if</span>(mGradientMatrix != <span class="keyword">null</span>) &#123;</span><br><span class="line">          mTranslate += mViewWidth / <span class="number">5</span>;</span><br><span class="line">          <span class="keyword">if</span>(mTranslate &gt; <span class="number">2</span> * mViewWidth) &#123;</span><br><span class="line">              mTranslate = -mViewWidth; </span><br><span class="line">          &#125;</span><br><span class="line">          mGradientMatrix.setTranslate(mTranslate, <span class="number">0</span>); </span><br><span class="line">          mLinearGradient.setLocalMatrix(mGradientMatrix); <span class="comment">// 设置位移矩阵</span></span><br><span class="line">          postInvalidateDelayed(<span class="number">100</span>); <span class="comment">// 100ms之后刷新View</span></span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>效果还是很不错哒，如下所示：</p><p><img src="/img/my_textview.gif" alt="流光TextView效果图"></p><p>初探旅程就到这，还有两种方法，等待探索。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    这周敲完了《第一行代码》的最后一个项目酷欧天气，然后开始，第二本书《Android群英传》的敲代码之旅。虽然说项目驱动才是最好的学习方式，但照书敲然后思考，也不失为一种好办法。算是迈出Android进阶的第一步吧。&lt;/p&gt;
&lt;h2 id=&quot;Android控件架构&quot;&gt;&lt;a href=&quot;#Android控件架构&quot; class=&quot;headerlink&quot; title=&quot;Android控件架构&quot;&gt;&lt;/a&gt;Android控件架构&lt;/h2&gt;&lt;p&gt;​    Android里的控件，大致被分为两类，&lt;strong&gt;ViewGroup 和 View&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    从字面上来看，ViewGroup就是一组View的集合，可以通过它来管理控制其包含的View。通过它，界面上的所有控件也就能够形成一个树形结构，也就是常说的&lt;strong&gt;控件树&lt;/strong&gt;，上层控件负责下层子控件的测量与绘制，并且传递交互事件。我们在Activity里经常使用的findViewById()方法，就是在控件树中通过DFS来查找的（数据结构还是很有用的）。而对于每一颗控件树，都有一个ViewParent对象，这就是整颗树的控制核心，&lt;strong&gt;所有的交互管理事件都由它来进行统一调度和分配&lt;/strong&gt;。大概就是下面这张图的样子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/view_tree.png&quot; alt=&quot;ViewTree&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Android" scheme="https://tobiaslee.top/tags/Android/"/>
    
      <category term="View" scheme="https://tobiaslee.top/tags/View/"/>
    
  </entry>
  
  <entry>
    <title>FilterPlayer</title>
    <link href="https://tobiaslee.top/2017/04/23/FilterPlayer/"/>
    <id>https://tobiaslee.top/2017/04/23/FilterPlayer/</id>
    <published>2017-04-23T12:24:26.000Z</published>
    <updated>2017-05-14T03:22:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>​    上次讲了MediaCodec的用法，其实也是为了后面的视频渲染做准备的。一开始定的学习目标就是：能够导入视频，并且加上滤镜。这篇文章就主要讲一下导入视频之后的滤镜渲染。</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>​    其实一开始实现的思路大致是这样:</p><blockquote><p>MediaCodec解码视频流 ——&gt; 输出到GLSurfaceView ——&gt; OpenGL渲染</p></blockquote><p>​    然而因为GLSurfaceView的Surface比较特殊，无法直接作为MediaCodec的Decoder的输出容器，所以失败了。</p><p>​    后来开始研究 <a href="https://github.com/google/grafika" target="_blank" rel="noopener">grafika</a> 里的一个 Show+capture camera 的功能</p><blockquote><p><a href="https://github.com/google/grafika/blob/master/src/com/android/grafika/CameraCaptureActivity.java" target="_blank" rel="noopener">Show + capture camera</a>. Attempts to record at 720p from the front-facing camera, displaying the preview and recording it simultaneously.</p><ul><li>Use the record button to toggle recording on and off.</li><li>Recording continues until stopped. If you back out and return, recording will start again, with a real-time gap. If you try to play the movie while it’s recording, you will see an incomplete file (and probably cause the play movie activity to crash).</li><li>The recorded video is scaled to 640x480, so it will probably look squished. A real app would either set the recording size equal to the camera input size, or correct the aspect ratio by letter- or pillar-boxing the frames as they are rendered to the encoder.</li><li>You can select a filter to apply to the preview. It does not get applied to the recording. The shader used for the filters is not optimized, but seems to perform well on most devices (the original Nexus 7 (2012) being a notable exception). Demo here: <a href="http://www.youtube.com/watch?v=kH9kCP2T5Gg" target="_blank" rel="noopener">http://www.youtube.com/watch?v=kH9kCP2T5Gg</a></li><li>The output is a video-only MP4 file (“camera-test.mp4”).</li></ul></blockquote><p>​    它的Show模块和我想要做滤镜预览的是非常类似的，只是我的输入源是来自于本地文件，只要用MediaCodec来解码成视频流作为输入就可以了。当然，也和它的说明一样，他的录制功能是不带滤镜的，而我要做的进行添加滤镜之后的保存就要另辟蹊径了（还在努力中）。</p><p>​    grafika实现的思路是这样的:</p><blockquote><p>Decoder –&gt; Surface -&gt; SurfaceTexture –&gt; OpenGL Filter –&gt; GLSurfaceView</p></blockquote><p>​    关键在于，它并没有直接把GLSurfaceView的Surface交给Deocder作为输出容器，而是中间通过一个FullFrameRect的类来持有了一个OpenGL的Texture,然后我们可以用这个纹理来创建一个Surface交给Decoder。就避免了直接用GLSurfaceView的Surface作为输出的问题。</p><a id="more"></a><h2 id="关键的类-Texture2dProgram"><a href="#关键的类-Texture2dProgram" class="headerlink" title="关键的类  Texture2dProgram"></a>关键的类  Texture2dProgram</h2><p>​    上面说到，FullFrameRect 起到了一个比较重要的作用。来看看代码。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FullFrameRect</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Drawable2d mRectDrawable = <span class="keyword">new</span> Drawable2d(Drawable2d.Prefab.FULL_RECTANGLE);</span><br><span class="line">    <span class="keyword">private</span> Texture2dProgram mProgram;</span><br></pre></td></tr></table></figure><p>FullFrameRect其实就是一个精灵(sprite)，作为渲染的中间体，真正做工作的，其实是他持有的Texture2dProgram这个类，而这个类也是我们所说的Texture的真正的持有者。来看代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Texture2dProgram</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">enum</span> ProgramType &#123; <span class="comment">// 几种program的类型</span></span><br><span class="line">        TEXTURE_2D, TEXTURE_EXT, TEXTURE_EXT_BW, TEXTURE_EXT_FILT</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String VERTEX_SHADER = <span class="string">""</span>; <span class="comment">// 定点和片段着色器的代码省略了</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String FRAGMENT_SHADER_2D = <span class="string">""</span> ;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 以下几个成员变量才是关键</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mProgramHandle; <span class="comment">// openGL program 实际上就是这个mProgramHandle</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> muMVPMatrixLoc; <span class="comment">// 投影矩阵</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> muTexMatrixLoc; <span class="comment">// 纹理矩阵</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> muKernelLoc;  <span class="comment">// 内核 通过他配合片段着色器来实现滤镜功能</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> muTexOffsetLoc;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> muColorAdjustLoc; <span class="comment">// 颜色调整</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> maPositionLoc;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> maTextureCoordLoc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mTextureTarget;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">float</span>[] mKernel = <span class="keyword">new</span> <span class="keyword">float</span>[KERNEL_SIZE];</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">float</span>[] mTexOffset;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">float</span> mColorAdjust;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Texture2dProgram</span><span class="params">(ProgramType programType)</span> </span>&#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">createTextureObject</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setKernel</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看看几个关键的方法:</p><p><code>Texture2dProgram()</code> 构造方法，根据programType生成用对应的着色器生成对应的OpenGL program，如果设置有滤镜 会加载kernel数组。</p><p><code>createTextureObject()</code> 利用openGL生成TextureObject 交给外界处理。</p><p><code>setKernel()</code> 动态的来设置Kernel数组，来实现实时滤镜的切换。</p><p><code>draw()</code> 用OpenGL在Texture上绘制</p><p>所以其实渲染都是由这个Texture2dProgram完成的，我们通过改变片段着色器的参数来实现视频滤镜的添加。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>​    不过上面的思路还是有一个小问题，就是这样<strong>渲染出来的视频是没有声音</strong>的，因为在上一篇MediaCodec的文章里讲到，我们在解码的时候只选取的视频轨，而没有选取音频轨，所以如果要有声音，就要考虑音画同步，而这一点是比较困难的。既然如此，那有没有办法可以不用手动控制视频和音频呢？当然有，就是我们的MediaPlayer了。我们只要利用MediaPlayer类的<code>setSurface()</code>方法，其实就和decoder的使用一模一样了。<strong>一来免去了控制音画同步的麻烦，二还节省了MediaCodec的解码代码，真是一石二鸟!</strong> </p><p>这里我们创建一个VideoRender类，并且实现GLSurfaceView的Render接口，来作为我们渲染器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VideoRenderer</span> <span class="keyword">extends</span> <span class="title">SurfaceView</span> <span class="keyword">implements</span> <span class="title">GLSurfaceView</span>.<span class="title">Renderer</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String SAMPLE = Environment.getExternalStorageDirectory() + <span class="string">"/SAMPLE.mp4"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String TAG = <span class="string">"VideoRenderer"</span>;</span><br><span class="line">    <span class="keyword">private</span> PlayerThread mPlayer;</span><br><span class="line">    <span class="keyword">private</span> FullFrameRect mFullScreen;</span><br><span class="line">    <span class="keyword">private</span> SurfaceTexture mSurfaceTexture;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">float</span>[] mSTMatrix = <span class="keyword">new</span> <span class="keyword">float</span>[<span class="number">16</span>];</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mTextureId;</span><br><span class="line">    GLSurfaceView mGLSurfaceView ;</span><br><span class="line">    <span class="keyword">private</span> Surface mSurface;</span><br><span class="line">    <span class="keyword">private</span> MediaPlayerWithSurface playerWithSurface;</span><br><span class="line">    <span class="keyword">private</span> SeekBar seekBar;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> isStart = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">int</span> surfaceWidth, surfaceHeight;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mCurrentFilter;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mNewFilter;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> mIncomingSizeUpdated;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">VideoRenderer</span><span class="params">(Context context,GLSurfaceView glSurfaceView, SeekBar seekBar)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(context);</span><br><span class="line">        <span class="keyword">this</span>.mGLSurfaceView = glSurfaceView;</span><br><span class="line">        Log.d(TAG, <span class="string">"VideoRenderer: created"</span>);</span><br><span class="line">        mCurrentFilter = -<span class="number">1</span>;</span><br><span class="line">        mNewFilter = MainActivity.FILTER_NONE;</span><br><span class="line">        <span class="keyword">this</span>.seekBar = seekBar;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateFilter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Texture2dProgram.ProgramType programType;</span><br><span class="line">        <span class="keyword">float</span>[] kernel = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">float</span> colorAdj = <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        Log.d(TAG, <span class="string">"Updating filter to "</span> + mNewFilter);</span><br><span class="line">        <span class="keyword">switch</span> (mNewFilter) &#123;</span><br><span class="line">           <span class="comment">//...</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (programType != mFullScreen.getProgram().getProgramType()) &#123;</span><br><span class="line">            mFullScreen.changeProgram(<span class="keyword">new</span> Texture2dProgram(programType));</span><br><span class="line">            <span class="comment">// If we created a new program, we need to initialize the texture width/height.</span></span><br><span class="line">            mIncomingSizeUpdated = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Update the filter kernel (if any).</span></span><br><span class="line">        <span class="keyword">if</span> (kernel != <span class="keyword">null</span>) &#123;</span><br><span class="line">            mFullScreen.getProgram().setKernel(kernel, colorAdj);</span><br><span class="line">        &#125;</span><br><span class="line">        mCurrentFilter = mNewFilter;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSurfaceCreated</span><span class="params">(GL10 gl, EGLConfig config)</span> </span>&#123;</span><br><span class="line">        Log.d(TAG, <span class="string">"onSurfaceCreated: "</span>);</span><br><span class="line">        mFullScreen = <span class="keyword">new</span> FullFrameRect(</span><br><span class="line">                <span class="keyword">new</span> Texture2dProgram(Texture2dProgram.ProgramType.TEXTURE_EXT));</span><br><span class="line"></span><br><span class="line">        mTextureId = mFullScreen.createTextureObject();</span><br><span class="line"></span><br><span class="line">        mSurfaceTexture = <span class="keyword">new</span> SurfaceTexture(mTextureId);</span><br><span class="line"></span><br><span class="line">        mSurface = <span class="keyword">new</span> Surface(mSurfaceTexture);</span><br><span class="line"></span><br><span class="line">        mSurfaceTexture.setOnFrameAvailableListener(<span class="keyword">new</span> SurfaceTexture.OnFrameAvailableListener() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFrameAvailable</span><span class="params">(SurfaceTexture surfaceTexture)</span> </span>&#123;</span><br><span class="line">                mGLSurfaceView.requestRender();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSurfaceChanged</span><span class="params">(GL10 gl, <span class="keyword">int</span> width, <span class="keyword">int</span> height)</span> </span>&#123;</span><br><span class="line">        Log.d(TAG, <span class="string">"onSurfaceChanged:  changed"</span>);</span><br><span class="line">        <span class="comment">// 用MediaCodec来解码视频 无声音</span></span><br><span class="line"><span class="comment">//        if(mPlayer == null) &#123;</span></span><br><span class="line"><span class="comment">//            mPlayer = new PlayerThread(mSurface);</span></span><br><span class="line"><span class="comment">//            mPlayer.start();</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( playerWithSurface == <span class="keyword">null</span> ) &#123;</span><br><span class="line">            playerWithSurface = <span class="keyword">new</span> MediaPlayerWithSurface(SAMPLE,mSurface,seekBar);</span><br><span class="line">            playerWithSurface.playVideoToSurface();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        GLES20.glViewport(<span class="number">0</span>,<span class="number">0</span>,width, height);</span><br><span class="line">        surfaceWidth = width;</span><br><span class="line">        surfaceHeight = height;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onDrawFrame</span><span class="params">(GL10 gl)</span> </span>&#123;</span><br><span class="line">        mSurfaceTexture.updateTexImage();</span><br><span class="line">        <span class="keyword">if</span> (mCurrentFilter != mNewFilter) &#123;</span><br><span class="line">            updateFilter();</span><br><span class="line">        &#125;</span><br><span class="line">        mSurfaceTexture.getTransformMatrix(mSTMatrix);</span><br><span class="line">        mFullScreen.drawFrame(mTextureId, mSTMatrix);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">changeFilterMode</span><span class="params">(<span class="keyword">int</span> filter)</span> </span>&#123;</span><br><span class="line">        mNewFilter = filter;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//MediaCodec解码线程 输出到surface 具体可以参照MediaCodec文章 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">PlayerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现GLSurfaceView.Render接口需要实现三个方法，也是我们渲染的主要的三个方法:</p><p><code>onSurfaceCreated()</code> 当Surface被创建的时候，我们生成了一个FullFrameRect的实例<code>mFullScreen</code>，并且利用它来拿到TextureId，用来生成了一个<code>mSurface</code>，就和我们说的思路是一样的。然后很关键的这里要给<code>** mSurfaceTexture</code>设置onFrameAvailable的监听器，并且回调<code>GLSurfaceView.requestRender()</code>方法**，一开始我就是没写这段代码，死都搞不出来…</p><p><code>onSurfaceChanged()</code> 在这里我们开启了我们的<code>MediaPlayer</code>，并且把生成<code>mSurface</code>和文件路径作为参数交给它，<code>mSurface</code>会作为播放的输出容器。</p><p><code>onDrawFrame()</code> 我们会拿到相应的矩阵，交给<code>mSurfaceTexture</code>，并且让<code>mFullScreen</code>其实也就是它所持有的<code>Texture2dProgram</code>去做绘制的工作。如果当前的滤镜和所选择的滤镜不同就会去调用<code>updateFilter()</code>方法来实现实时的滤镜切换。</p><p><code>updateFilter()</code> 更新滤镜。会根据新的filter来生成对应的<code>mTexture2dProgram</code> 并且利用上面我们提到的<code>setKernel()</code>来设置着色器参数，达到更改滤镜的效果。</p><h2 id="项目源码"><a href="#项目源码" class="headerlink" title="项目源码"></a>项目源码</h2><p>​    由于篇幅原因，多数的类的代码都被我省略了。</p><p>​    不过这个初步的小成果已经被我放到Github上了，大致就是实现了简单的视频实时滤镜的预览，并且再利用MediaPlayer和SeekBar做一些简单的播放控制。</p><p>​    项目地址 <a href="https://github.com/lileizhenshuai/FilterPlayer" target="_blank" rel="noopener">FIlterPlayer</a> </p><p>​    接下来会努力把本地化保存做好，然后更新这个代码哒。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    上次讲了MediaCodec的用法，其实也是为了后面的视频渲染做准备的。一开始定的学习目标就是：能够导入视频，并且加上滤镜。这篇文章就主要讲一下导入视频之后的滤镜渲染。&lt;/p&gt;
&lt;h2 id=&quot;思路&quot;&gt;&lt;a href=&quot;#思路&quot; class=&quot;headerlink&quot; title=&quot;思路&quot;&gt;&lt;/a&gt;思路&lt;/h2&gt;&lt;p&gt;​    其实一开始实现的思路大致是这样:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MediaCodec解码视频流 ——&amp;gt; 输出到GLSurfaceView ——&amp;gt; OpenGL渲染&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    然而因为GLSurfaceView的Surface比较特殊，无法直接作为MediaCodec的Decoder的输出容器，所以失败了。&lt;/p&gt;
&lt;p&gt;​    后来开始研究 &lt;a href=&quot;https://github.com/google/grafika&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;grafika&lt;/a&gt; 里的一个 Show+capture camera 的功能&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/google/grafika/blob/master/src/com/android/grafika/CameraCaptureActivity.java&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Show + capture camera&lt;/a&gt;. Attempts to record at 720p from the front-facing camera, displaying the preview and recording it simultaneously.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the record button to toggle recording on and off.&lt;/li&gt;
&lt;li&gt;Recording continues until stopped. If you back out and return, recording will start again, with a real-time gap. If you try to play the movie while it’s recording, you will see an incomplete file (and probably cause the play movie activity to crash).&lt;/li&gt;
&lt;li&gt;The recorded video is scaled to 640x480, so it will probably look squished. A real app would either set the recording size equal to the camera input size, or correct the aspect ratio by letter- or pillar-boxing the frames as they are rendered to the encoder.&lt;/li&gt;
&lt;li&gt;You can select a filter to apply to the preview. It does not get applied to the recording. The shader used for the filters is not optimized, but seems to perform well on most devices (the original Nexus 7 (2012) being a notable exception). Demo here: &lt;a href=&quot;http://www.youtube.com/watch?v=kH9kCP2T5Gg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.youtube.com/watch?v=kH9kCP2T5Gg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The output is a video-only MP4 file (“camera-test.mp4”).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    它的Show模块和我想要做滤镜预览的是非常类似的，只是我的输入源是来自于本地文件，只要用MediaCodec来解码成视频流作为输入就可以了。当然，也和它的说明一样，他的录制功能是不带滤镜的，而我要做的进行添加滤镜之后的保存就要另辟蹊径了（还在努力中）。&lt;/p&gt;
&lt;p&gt;​    grafika实现的思路是这样的:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Decoder –&amp;gt; Surface -&amp;gt; SurfaceTexture –&amp;gt; OpenGL Filter –&amp;gt; GLSurfaceView&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    关键在于，它并没有直接把GLSurfaceView的Surface交给Deocder作为输出容器，而是中间通过一个FullFrameRect的类来持有了一个OpenGL的Texture,然后我们可以用这个纹理来创建一个Surface交给Decoder。就避免了直接用GLSurfaceView的Surface作为输出的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Android" scheme="https://tobiaslee.top/tags/Android/"/>
    
      <category term="MediaCodec" scheme="https://tobiaslee.top/tags/MediaCodec/"/>
    
      <category term="OpenGL" scheme="https://tobiaslee.top/tags/OpenGL/"/>
    
  </entry>
  
  <entry>
    <title>循环右移问题</title>
    <link href="https://tobiaslee.top/2017/04/12/%E5%BE%AA%E7%8E%AF%E5%8F%B3%E7%A7%BB%E9%97%AE%E9%A2%98/"/>
    <id>https://tobiaslee.top/2017/04/12/循环右移问题/</id>
    <published>2017-04-12T12:42:55.000Z</published>
    <updated>2017-04-12T12:51:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>数据结构作业里一道，非常有意思的题。</p><blockquote><p>设计一个算法，将数组An中的元素A[0]至A[n-1]循环右移k位，并要求只用一个元素大小的附加储存，元素移动或交换次数为O(n)。</p></blockquote><p>循环右移的意思就是，如果有一个数组”12345”，循环右移2位，就得到了”45123”，就好像贪吃蛇穿墙一样… 233</p><p>其实有个很暴力的解法，就是再额外申请一个数组，来储存移动之后的数组，这样空间和时间复杂度都是O(n)，但是题目要求空间是O(1)的，所以就得换个思路。</p><p>我记得TCPL上面有一道二进制数的循环右移的题目，然后翻出来看了一下，题解是直接用位操作，所以可能对于数组实现起来比较复杂，于是就换了一个想法。</p><h2 id="解一"><a href="#解一" class="headerlink" title="解一"></a>解一</h2><p>这个想法是这样的：</p><p>如图:</p><p><img src="/img/moveRight.png" alt="Solution"><br><a id="more"></a></p><p>每次交换都让一个数就位，也就是移动到一个新位置上，并且，把原来这个位置的元素用临时变量存起来，再将原来位置上的元素就位，以此下去，直到所有元素都就位。</p><p>但是这样有一个问题，对于移动位数是长度因子的情况，<strong>可能出现一趟之后回到原位，无法覆盖所有元素的情况</strong>，比如长度为9，移动3位，”1-&gt;4-&gt;7-&gt;1”，从第一位开始移动，又会回到第一位。</p><p>这里我从网上看到一个很漂亮的解决思路，用最大公约数，来解决回到原位的情况。</p><p>就是说，如果回到了原来的位置，就让指针后移一位，再重复就位的过程。到什么时候结束呢？就是移动位数k和数组长度n的最大公约数，只要走完GCD(k,n)趟，就能够保证所有的元素都就位了。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 循环右移k位移动数组a 长度为n</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">move</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> k, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  k %= n ; <span class="comment">// 避免重复移动</span></span><br><span class="line">  <span class="keyword">if</span> (!k) <span class="keyword">return</span> ; </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; gcd(n,k); i++) &#123; </span><br><span class="line">    <span class="keyword">int</span> j = i ;</span><br><span class="line">    <span class="keyword">int</span> temp = a[j];</span><br><span class="line">    <span class="keyword">do</span>&#123;</span><br><span class="line">      j = (j+k) % n ; <span class="comment">// 移动j到目标位置</span></span><br><span class="line">      <span class="keyword">int</span> t = a[j] ; <span class="comment">// 保存目标位置旧元素的值</span></span><br><span class="line">      a[j] = temp ; <span class="comment">// 前一个元素就位</span></span><br><span class="line">      temp = t; <span class="comment">// 更新temp的值</span></span><br><span class="line">    &#125;<span class="keyword">while</span>(j != i) <span class="comment">// 没有回到出发位置</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="解二"><a href="#解二" class="headerlink" title="解二"></a>解二</h2><p>这个解法是我在网上看到的，和TCPL上面那道题的解法非常类似，但是相当抽象，比较难想到。</p><p>TCPL上面的题是这样的:</p><blockquote><p>编写一个函数rightrot(x,n)，该函数返回将x循环右移n（二进制）位后得到的值。</p></blockquote><p>题解是这样的:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="title">rightrot</span><span class="params">(<span class="keyword">unsigned</span> x, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">wordlength</span><span class="params">(<span class="keyword">void</span>)</span></span>; <span class="comment">// 求字长函数</span></span><br><span class="line">  <span class="keyword">unsigned</span> rbits; </span><br><span class="line">  <span class="keyword">if</span>( (n = n % wordlength()) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    rbits = ~(~<span class="number">0</span> &lt;&lt; n) &amp; x ; <span class="comment">// 构造右边n为全部为0的屏蔽码 取出x右边的n位</span></span><br><span class="line">    rbits = rbits &lt;&lt; (wordlength() - n) ;<span class="comment">// 把右边的n位移动到左边</span></span><br><span class="line">    x = x &gt;&gt; n ; <span class="comment">// x 右移 n 位  </span></span><br><span class="line">    x = x | rbits; <span class="comment">// 把刚才右边的n位交给x的左边n位</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> x ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个解法的核心就是： <strong>一次性把右边要移动的n位取出，然后换到左边</strong></p><p>这道数组的题也可以利用这个整体处理思想。</p><p>“12345” - &gt; “45123” 其中”123”和”45”的相对位置是没有改变的，利用这一点，我们把数组分为两块，循环右移k位，就是把这两部分进行了一次交换。</p><p>然后先整体逆置 “12345”-&gt;”54321”</p><p>再把右边n-k(3)位逆置 “54321” -&gt; “54123”</p><p>再把左边的k(2)位逆置 “54123” - &gt; “45123”</p><p>大功告成</p><p>代码如下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Reverse</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> *a)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n / <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="keyword">int</span> t = a[i];</span><br><span class="line">    a[i] = a[n - i - <span class="number">1</span>];</span><br><span class="line">    a[n - i - <span class="number">1</span>] = t;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">move2</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n, <span class="keyword">int</span> *a)</span> </span>&#123;</span><br><span class="line">  m %= n;</span><br><span class="line">  <span class="keyword">if</span> (m == <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">  Reverse(n, a);</span><br><span class="line">  Reverse(m, a);</span><br><span class="line">  Reverse(n - m, a + m);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据结构作业里一道，非常有意思的题。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;设计一个算法，将数组An中的元素A[0]至A[n-1]循环右移k位，并要求只用一个元素大小的附加储存，元素移动或交换次数为O(n)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;循环右移的意思就是，如果有一个数组”12345”，循环右移2位，就得到了”45123”，就好像贪吃蛇穿墙一样… 233&lt;/p&gt;
&lt;p&gt;其实有个很暴力的解法，就是再额外申请一个数组，来储存移动之后的数组，这样空间和时间复杂度都是O(n)，但是题目要求空间是O(1)的，所以就得换个思路。&lt;/p&gt;
&lt;p&gt;我记得TCPL上面有一道二进制数的循环右移的题目，然后翻出来看了一下，题解是直接用位操作，所以可能对于数组实现起来比较复杂，于是就换了一个想法。&lt;/p&gt;
&lt;h2 id=&quot;解一&quot;&gt;&lt;a href=&quot;#解一&quot; class=&quot;headerlink&quot; title=&quot;解一&quot;&gt;&lt;/a&gt;解一&lt;/h2&gt;&lt;p&gt;这个想法是这样的：&lt;/p&gt;
&lt;p&gt;如图:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/moveRight.png&quot; alt=&quot;Solution&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://tobiaslee.top/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="数据结构" scheme="https://tobiaslee.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>AdapterPatternLearning</title>
    <link href="https://tobiaslee.top/2017/04/04/AdapterPattern/"/>
    <id>https://tobiaslee.top/2017/04/04/AdapterPattern/</id>
    <published>2017-04-04T02:52:07.000Z</published>
    <updated>2017-04-04T03:02:29.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>适配器模式：将一个类的接口转换成另一个接口，以符合需求。让接口不兼容的类可以合作无间。</code></pre><p>​    更重要的是，适配器将<strong>客户和被适配者解耦</strong>，当接口改变时，利用适配器来封装改变的部分，就可以在不改动客户调用方式的情况下实现更新。</p><p>​    打个最简单的例子，插座转换器，比如买的港版手机不能插国内的插头，中间加一个插头转换器，就可以了。这个转换器，起到的就是适配器（Adapter）的作用。</p><p><img src="/img/adapter.png" alt="Adapter"></p><h2 id="Exmaple"><a href="#Exmaple" class="headerlink" title="Exmaple"></a>Exmaple</h2><p>​    Java早期的集合（Collection）类型（e.g. Vector,Stack,Hashtable）中都实现了一个名为elements（）的方法，这个方法会返回一个Enumeration（枚举），这个接口有两个方法<code>hasMoreElemnts()</code>和<code>nextElement()</code>，从而能够在不知道集合的情况下，遍历集合中的元素。</p><p>​    Sun之后更新了集合类，开始使用Iterator（迭代器）接口，这个接口和枚举接口很类似，有三个方法，分别是<code>hasNext()</code>、<code>next()</code>、<code>remove()</code>，这个remove()就是二者主要的区别。</p><p>​    那么问题来了，如何适配新老版本的枚举和迭代器接口？适配器就是这个时候派上用场的。<br><a id="more"></a></p><h4 id="处理remove-方法"><a href="#处理remove-方法" class="headerlink" title="处理remove()方法"></a>处理remove()方法</h4><p>​    枚举不支持删除，因为枚举是一个“只读”接口。适配器无法实现一个有实际功能的remove()方法，最多只能抛出一个运行时异常。迭代器的设计者事先考虑到了这一种情况（真是机智啊），所以我们可以将remove()方法定义为会抛出UnsupportedOperationException。</p><p>​    <strong>适配器并不完美</strong>，客户必须小心潜在的异常（比如不支持），但配合文档的说明和足够小心，这个方案也算合理。</p><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><p>​    接下来就是用代码来实现简单的Enumeration适配器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EnumerationIterator</span> <span class="keyword">implements</span> <span class="title">Iterator</span></span>&#123; <span class="comment">// 适配器实现被适配者</span></span><br><span class="line">  Enumeration <span class="keyword">enum</span>; </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">EnumerationIterator</span><span class="params">(Enumeration <span class="keyword">enum</span>)</span> </span>&#123; <span class="comment">// 利用组合，将枚举结合进适配器</span></span><br><span class="line">    <span class="keyword">this</span>.<span class="keyword">enum</span> = <span class="keyword">enum</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;  <span class="comment">// 适配器的hasNext()方法实际是委托给enum的hasMoreElements()方法</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">enum</span>.hasMoreElements();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">next</span><span class="params">()</span> </span>&#123; <span class="comment">// 同样的 交给enum的nextElement()方法</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">enum</span>.nextElement();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException() ; <span class="comment">// 不支持该操作，抛出异常</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="装饰者模式和适配者模式的区别"><a href="#装饰者模式和适配者模式的区别" class="headerlink" title="装饰者模式和适配者模式的区别"></a>装饰者模式和适配者模式的区别</h2><p>​    还是插头的例子，有些比较智能的插头，不仅能转换插头的样式来适配插口，还有可能更多的功能，比如：涓流充电、指示灯、警报声等等。在需要实现这些新特性的时候，适配者模式就显得有些无力，就要用装饰者模式。</p><p>​    装饰者模式，简单来说就是给对象包装上一层有一层的外套，每添加一层就可以增加新的行为。它的主要目的是<strong>扩展对象的行为和责任</strong>，但是使用起来真的是…个人觉得很是繁琐。比如JavaIO里对Stream的一系列类都是装饰者类，使用起来就会有很恐怖的构造序列：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">File file = <span class="keyword">new</span> File(<span class="string">"Path"</span>);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">DataInputStream din = <span class="keyword">new</span> DataInputStream(</span><br><span class="line"><span class="keyword">new</span> BufferedInputStream(</span><br><span class="line"><span class="keyword">new</span> FileInputStream(file)));</span><br><span class="line">&#125; <span class="keyword">catch</span> (FileNotFoundException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    恐怖的原因其实就在于我们对于这个流的处理有很多需求，比如要利用缓冲机制，所以就套了一层BufferedInputStream，又因为是要从文件来读取输入流，内层就是FileInputStream。</p><p>​    而适配者模式，其核心就是<strong>传送需求</strong>，把所有的需求都委托给其内部持有的对象去实现。用户只要调用接口，适配者把脏活累活全做了，从而实现了用户和被适配者的解耦。</p><p>​    <strong>设计模式没有好坏之分，一切都是需求主导</strong></p><p>​    </p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;适配器模式：将一个类的接口转换成另一个接口，以符合需求。让接口不兼容的类可以合作无间。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;​    更重要的是，适配器将&lt;strong&gt;客户和被适配者解耦&lt;/strong&gt;，当接口改变时，利用适配器来封装改变的部分，就可以在不改动客户调用方式的情况下实现更新。&lt;/p&gt;
&lt;p&gt;​    打个最简单的例子，插座转换器，比如买的港版手机不能插国内的插头，中间加一个插头转换器，就可以了。这个转换器，起到的就是适配器（Adapter）的作用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/adapter.png&quot; alt=&quot;Adapter&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Exmaple&quot;&gt;&lt;a href=&quot;#Exmaple&quot; class=&quot;headerlink&quot; title=&quot;Exmaple&quot;&gt;&lt;/a&gt;Exmaple&lt;/h2&gt;&lt;p&gt;​    Java早期的集合（Collection）类型（e.g. Vector,Stack,Hashtable）中都实现了一个名为elements（）的方法，这个方法会返回一个Enumeration（枚举），这个接口有两个方法&lt;code&gt;hasMoreElemnts()&lt;/code&gt;和&lt;code&gt;nextElement()&lt;/code&gt;，从而能够在不知道集合的情况下，遍历集合中的元素。&lt;/p&gt;
&lt;p&gt;​    Sun之后更新了集合类，开始使用Iterator（迭代器）接口，这个接口和枚举接口很类似，有三个方法，分别是&lt;code&gt;hasNext()&lt;/code&gt;、&lt;code&gt;next()&lt;/code&gt;、&lt;code&gt;remove()&lt;/code&gt;，这个remove()就是二者主要的区别。&lt;/p&gt;
&lt;p&gt;​    那么问题来了，如何适配新老版本的枚举和迭代器接口？适配器就是这个时候派上用场的。&lt;br&gt;
    
    </summary>
    
    
      <category term="设计模式" scheme="https://tobiaslee.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>杂记</title>
    <link href="https://tobiaslee.top/2017/03/18/%E6%9D%82%E8%AE%B0/"/>
    <id>https://tobiaslee.top/2017/03/18/杂记/</id>
    <published>2017-03-18T09:41:58.000Z</published>
    <updated>2017-03-18T09:42:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>一开始这个标题起成“当我们谈论爱情时我们再谈论什么”，后来一想，其实我好像没什么资格写这种东西的。</p><p>所以就把名字改了，主要就写写这段时间的感受吧。</p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><p>其实这段时间感触最深的就是，爱情的无常吧。</p><p>毕竟这段时间，几个哥们都和女朋友分手了，还有一个妹子也和bf分了。</p><p>有持续不到三个月半年的，也有已经两年多三年的。</p><p>有一个哥们形容分手：感觉像是玩了很久的游戏，突然被盗号了。</p><p>不过被盗号这个比喻不恰当，因为不是被绿嘛哈哈哈，倒不如用被封号来形容，可能更加好。</p><p>他对女朋友的体贴照顾我是非常敬佩的，我个人感觉，就算够不上供着一个女菩萨，也差不了多少了。</p><p>女朋友提出分手之后他也立马飞过去，带着渺茫的她能够回心转意的希望，不过结果其实早就心知肚明。</p><p>挽回，我想，不过是给自己一个交代吧。<br><a id="more"></a></p><p>换个视角，女生提分手是怎么想的呢？</p><p>那妹子和我说:</p><blockquote><p>没有感觉了，就分手了。这样对两个人都好。</p></blockquote><p>不过显然bf还是很痴情的，一直在努力，找自己的问题。</p><p>感觉，呐，真是个很神奇的东西啊。</p><p>这也是我曾经被拒绝的理由。</p><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><p>我是有相当长一段时间都是单身状态了，上一段恋爱感觉都已经有些遥远了。</p><p>似乎手也没有牵过，最甜蜜的回忆也无非就是教她弹琴以及中午慢吞吞懒洋洋地去高吟吃饭。</p><p>情侣们相处的模式大多差不多吧，异地的聊出QQ的巨轮，天天煲电话粥，保持着联系。</p><p>在一所学校的呢，也无非一起上课吃饭压马路和周末娱乐。</p><p>这种我闭着眼睛都能猜出个八九不离十的恋爱桥段，在我看来，真的无聊透顶。</p><p>女孩子们总是感觉动物，可能在那么一刹那你闪着光的样子让她怦然心动，</p><p>一段时间相处下来，那种心动的感觉很可能就慢慢消逝了。</p><p>无非有的人快，有的人慢一点。</p><h2 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h2><p>我发觉我的恋爱模式都是一开始选中一个目标，然后在追求过程中不断地给自己心理暗示，告诉自己：非她不可。</p><p>所谓选中目标，也不过是长相、声音、性格符合口味。</p><p>然后会收集很多不起眼的片段来欺骗自己，鼓励自己坚持下去。</p><p>很蠢，却也让我收获很多。</p><p>现在就特别懒，路上看的相貌不错的小姐姐，也只是在脑海里闪过:这个是我的style。</p><p>然后，就没有然后了。</p><p>因为知道追求要付出很多精力啊，而女孩子们对你的感觉在见到你第一眼就决定了。</p><p>再多的努力也只能感动自己。</p><p>不如刷刷GPA、敲敲代码，或者看会书弹会琴。</p><p><strong>我觉得我自己一个人过的很舒服，所以，为什么要两个人呢？</strong></p><h2 id="-3"><a href="#-3" class="headerlink" title=" "></a> </h2><p>这段时间过的其实挺糟心。</p><p>课比上学期多，宿舍的网还断了，靠着慢吞吞的CMCC-EDU残喘苟延。</p><p>入了渲染的坑，也是经常踩坑踩到怀疑人生。</p><p>还要走很长的路，留给我的时间也不是很多了。</p><p>加油吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一开始这个标题起成“当我们谈论爱情时我们再谈论什么”，后来一想，其实我好像没什么资格写这种东西的。&lt;/p&gt;
&lt;p&gt;所以就把名字改了，主要就写写这段时间的感受吧。&lt;/p&gt;
&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h2&gt;&lt;p&gt;其实这段时间感触最深的就是，爱情的无常吧。&lt;/p&gt;
&lt;p&gt;毕竟这段时间，几个哥们都和女朋友分手了，还有一个妹子也和bf分了。&lt;/p&gt;
&lt;p&gt;有持续不到三个月半年的，也有已经两年多三年的。&lt;/p&gt;
&lt;p&gt;有一个哥们形容分手：感觉像是玩了很久的游戏，突然被盗号了。&lt;/p&gt;
&lt;p&gt;不过被盗号这个比喻不恰当，因为不是被绿嘛哈哈哈，倒不如用被封号来形容，可能更加好。&lt;/p&gt;
&lt;p&gt;他对女朋友的体贴照顾我是非常敬佩的，我个人感觉，就算够不上供着一个女菩萨，也差不了多少了。&lt;/p&gt;
&lt;p&gt;女朋友提出分手之后他也立马飞过去，带着渺茫的她能够回心转意的希望，不过结果其实早就心知肚明。&lt;/p&gt;
&lt;p&gt;挽回，我想，不过是给自己一个交代吧。&lt;br&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>MediaCodecLearning</title>
    <link href="https://tobiaslee.top/2017/03/17/MediaCodecLearning/"/>
    <id>https://tobiaslee.top/2017/03/17/MediaCodecLearning/</id>
    <published>2017-03-17T13:14:05.000Z</published>
    <updated>2017-03-17T13:24:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MediaCodec编码与解码学习笔记"><a href="#MediaCodec编码与解码学习笔记" class="headerlink" title="MediaCodec编码与解码学习笔记"></a>MediaCodec编码与解码学习笔记</h1><p>上了秦爷的车，进了视频渲染的坑。</p><p>这两天跟着几个Demo敲了一下MediaCodec的编码和解码，记录一下。</p><p><a href="https://developer.android.com/reference/android/media/MediaCodec.html" target="_blank" rel="noopener">AndroidDeveloper MediaCodec</a></p><p>官方的文档写的很详细，应该要仔细去翻阅一下</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>​    MediacCodec是Android平台的一个音视频的编解码类，是比较底层的多媒体工具类，经常配合使用的有MediaFormat（媒体格式类）、MediaExtractor（抓取多媒体轨道）、MediaMuxer（混合轨道）、Surface（容器）这几个类。</p><p>​    MediaCodec主要的工作过程就是它持有一个InputBuffer和OutputBuffer，使用的时候，根据需要提供输入流，交给codec处理，然后拿到输出流作为处理后的结果。这是一个典型的“生产者-消费者”模型。</p><p><img src="/img/codecState.png" alt="MediaCodecStates"></p><p> 上面是Codec的状态机</p><p>使用的主要流程就是通过MediaFormat来配置MediaCodec 进入Configured状态 然后start 不断地取出输入流 直到EOS标识，使用完后释放占用的资源。</p><a id="more"></a><h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><p>这里以编码本地的一个mp4视频输入，用一个Surface作为输出的容器。</p><p>看一下主要的代码:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">PlayerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"><span class="comment">//主要用到的类</span></span><br><span class="line">        <span class="keyword">private</span> MediaExtractor extractor; <span class="comment">//轨道抓取</span></span><br><span class="line">        <span class="keyword">private</span> MediaCodec decoder; <span class="comment">//解码器</span></span><br><span class="line">        <span class="keyword">private</span> Surface surface; <span class="comment">// 内容容器</span></span><br><span class="line">  </span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">PlayerThread</span><span class="params">(Surface surface)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.surface = surface;</span><br><span class="line">        &#125;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                extractor = <span class="keyword">new</span> MediaExtractor();</span><br><span class="line">                extractor.setDataSource(SAMPLE); <span class="comment">//设置元数据 这里的SAMPLE是视频文件的路径</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; extractor.getTrackCount(); i++) &#123;</span><br><span class="line">                   MediaFormat format = extractor.getTrackFormat(i);</span><br><span class="line">                    String mime = format.getString(MediaFormat.KEY_MIME);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span>(mime.startsWith(<span class="string">"video/"</span>)) &#123;<span class="comment">//如果是我们想要的视频轨的格式 </span></span><br><span class="line">                      <span class="comment">// 音频轨的开头一般是 "audio/"</span></span><br><span class="line">                        extractor.selectTrack(i);</span><br><span class="line">                      <span class="comment">//生成对应的解码器</span></span><br><span class="line">                        decoder = MediaCodec.createDecoderByType(mime);</span><br><span class="line"></span><br><span class="line"><span class="comment">//MediaFormat 有几个必须设置的属性 不设置decoder会报初始化异常</span></span><br><span class="line">                        format.setInteger(MediaFormat.KEY_BIT_RATE, <span class="number">870000</span>);</span><br><span class="line">                        format.setInteger(MediaFormat.KEY_SAMPLE_RATE,<span class="number">44100</span> );</span><br><span class="line">                        format.setInteger(MediaFormat.KEY_CHANNEL_COUNT, <span class="number">1</span>);</span><br><span class="line">                        decoder.configure(format,surface,<span class="keyword">null</span>,<span class="number">0</span>);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(decoder == <span class="keyword">null</span>)&#123;</span><br><span class="line">                    Log.d(TAG, <span class="string">"can't find video info"</span>);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                decoder.start();</span><br><span class="line"></span><br><span class="line">            ByteBuffer[] inputBuffers = decoder.getInputBuffers();</span><br><span class="line">            ByteBuffer[] outputBuffers = decoder.getOutputBuffers();</span><br><span class="line"></span><br><span class="line">            MediaCodec.BufferInfo info = <span class="keyword">new</span> MediaCodec.BufferInfo();</span><br><span class="line">            <span class="keyword">boolean</span> isEOS = <span class="keyword">false</span> ;</span><br><span class="line">            <span class="keyword">long</span> startMs = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(!Thread.interrupted()) &#123;</span><br><span class="line">                <span class="keyword">if</span>(!isEOS) &#123;</span><br><span class="line">                    <span class="keyword">int</span> inIndex = decoder.dequeueInputBuffer(<span class="number">10000</span>);</span><br><span class="line">                    <span class="keyword">if</span>(inIndex &gt;= <span class="number">0</span> ) &#123;</span><br><span class="line">                        ByteBuffer buffer = inputBuffers[inIndex];</span><br><span class="line">                        <span class="keyword">int</span> sampleSize = extractor.readSampleData(buffer,<span class="number">0</span>);</span><br><span class="line">                        <span class="keyword">if</span>(sampleSize &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                         <span class="comment">//不终止 把eos标志传给 encoder 我们会再次 从 outputBuffer 中得到这个标志</span></span><br><span class="line">                         Log.d(TAG, <span class="string">"InputBuffer BUFFER_FALG_END_OF_STREAM"</span>);              decoder.                         queueInputBuffer(inIndex,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,MediaCodec.BUFFER_FLAG_END_OF_STREAM);</span><br><span class="line">                              isEOS = <span class="keyword">true</span>;</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                decoder.queueInputBuffer(inIndex,<span class="number">0</span>,sampleSize,extractor.getSampleTime(),<span class="number">0</span>);</span><br><span class="line">                          <span class="comment">//读取下一条数据</span></span><br><span class="line">                                extractor.advance();</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"><span class="comment">//后一个参数是设置等待时间 -1为永久等待</span></span><br><span class="line">                    <span class="keyword">int</span> outIndex = decoder.dequeueOutputBuffer(info,<span class="number">10000</span>);</span><br><span class="line">                    <span class="keyword">switch</span> (outIndex) &#123;</span><br><span class="line">                        <span class="comment">//.. 对一些异常情况进行处理</span></span><br><span class="line">                        <span class="keyword">default</span>:</span><br><span class="line">                            ByteBuffer buffer = outputBuffers[outIndex];</span><br><span class="line">                            <span class="comment">// keep the fps using a simple clock</span></span><br><span class="line">                            <span class="keyword">while</span>(info.presentationTimeUs / <span class="number">1000</span> &gt; System.currentTimeMillis() - startMs) &#123;</span><br><span class="line">                                    sleep(<span class="number">10</span>);</span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                </span><br><span class="line">                            &#125;</span><br><span class="line">                            decoder.releaseOutputBuffer(outIndex,<span class="keyword">true</span>);</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span>( (info.flags &amp; MediaCodec.BUFFER_FLAG_END_OF_STREAM) != <span class="number">0</span>) &#123;</span><br><span class="line">                        Log.d(TAG, <span class="string">"BUFFER_FLAG_END_OFSTREAM"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                decoder.stop();</span><br><span class="line">          </span><br><span class="line">                decoder.release();</span><br><span class="line">                extractor.release();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于视频的解码主要是以下几个步骤:</p><p>1、用MediaExtractor类抓取我们想要的音频轨的格式，生成对应的MediaFormat类format和MediaCodec类decoder</p><p>2、<strong>对生成的format类进行一些必要参数的设置，不然会报错</strong>，然后将decoder进行配置，启动decoder</p><p>3、利用decoder的inputBuffers，通过extractor将数据以ByteBuffer的形式送到inputBuffer里，注意一下eos的处理</p><p>4、然后再处理输出流，直到同样出现EOS标记</p><p>5、停止decoder，释放相关资源</p><p>需要注意的是，因为这里只对视频轨进行了解码，所以效果是只有画面没有声音的，如果希望能够有声音，需要再开一个解码器同时对音频轨进行解码，并且涉及到复杂的同步。</p><blockquote><p>根据时间戳和一个计时器来控制同步，必要的时候手动掉帧，EXOPlayer源码里有体现。</p><p>——秦爷</p></blockquote><p>暂时就不研究了…</p><h2 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h2><p>解码的学习主要参考了grafic的soft_input_movie demo</p><p>代码主要的目的是通过Canvas绘图 然后作为输入，编码生成在对应路径生成mp4格式的视频</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">generateMovie</span><span class="params">(File outputFile)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            prepareEncoder(outputFile);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; NUM_FRAMES; i++) &#123;</span><br><span class="line">                drainEncoder(<span class="keyword">false</span>);</span><br><span class="line">                generateFrame(i);</span><br><span class="line">            &#125;</span><br><span class="line">            drainEncoder(<span class="keyword">true</span>);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (IOException e ) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            releaseEncoder();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里把解码主要的几个步骤封装成了独立的函数</p><p>主要就是 准备编码器( prepareEncoder() ) -&gt; 绘图产生输入 并榨干编码器(generateFrame() 和drainEncoder()) -&gt; 释放资源( releaseEncoder() )</p><p>先来看看编码器的准备工作，主要就是format的配置，这一块和解码类似，设置几个必须的参数，以及设置MediaMuxer，muxer释义是多路器/合成器，它的作用和MediaExtractor恰好相反，是把音视频轨合成成文件，这里我们给Muxer指定了输出的文件路径和格式，但是并没有启动它，因为Encoder的数据还没有准备好。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">prepareEncoder</span><span class="params">(File outputFile)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      mBufferInfo = <span class="keyword">new</span> MediaCodec.BufferInfo();</span><br><span class="line">      MediaFormat format = MediaFormat.createVideoFormat(MIME_TYPE,WIDTH,HEIGHT);</span><br><span class="line">  </span><br><span class="line">      <span class="comment">// set necessary properties for the format</span></span><br><span class="line">      format.setInteger(MediaFormat.KEY_COLOR_FORMAT,MediaCodecInfo.CodecCapabilities</span><br><span class="line">          .COLOR_FormatSurface);</span><br><span class="line">      format.setInteger(MediaFormat.KEY_BIT_RATE,BIT_RATE);</span><br><span class="line">      format.setInteger(MediaFormat.KEY_FRAME_RATE,FRAMES_PER_SECOND);</span><br><span class="line">      format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL,IFRAME_INTERVAL);</span><br><span class="line">      <span class="keyword">if</span>(VERBOSE) Log.d(TAG, <span class="string">"format:"</span> + format);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// create a media encoder and configure it with our format. get a surface</span></span><br><span class="line">      mEncoder = MediaCodec.createEncoderByType(MIME_TYPE);</span><br><span class="line">      mEncoder.configure(format,<span class="keyword">null</span>,<span class="keyword">null</span>,MediaCodec.CONFIGURE_FLAG_ENCODE);</span><br><span class="line">      mInputSurface = mEncoder.createInputSurface();</span><br><span class="line">      mEncoder.start();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Create a MediaMuxer.  We can't add the video track and start() the muxer here,</span></span><br><span class="line">      <span class="comment">// because our MediaFormat doesn't have the Magic Goodies.  These can only be</span></span><br><span class="line">      <span class="comment">// obtained from the encoder after it has started processing data.</span></span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">      <span class="comment">// We're not actually interested in multiplexing audio.  We just want to convert</span></span><br><span class="line">      <span class="comment">// the raw H.264 elementary stream we get from MediaCodec into a .mp4 file.</span></span><br><span class="line">      <span class="keyword">if</span>(VERBOSE) Log.d(TAG, <span class="string">"output will go to "</span> + outputFile);</span><br><span class="line">      mMuxer = <span class="keyword">new</span> MediaMuxer(outputFile.getPath(),</span><br><span class="line">              MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);</span><br><span class="line">      mTrackIndex = -<span class="number">1</span>;</span><br><span class="line">      mMuxerStarted = <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>在上大块头的drainEncoder代码之前我们先看最后一个函数 realeaseEncoder() 释放相关的资源。<br><strong>这里最后给mEncoder 和mMuxer赋值为null  是比较优雅的写法，相当于通知GC来回收，更彻底的节省了资源</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">releaseEncoder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(VERBOSE) Log.d(TAG, <span class="string">"release encode objects"</span>);</span><br><span class="line">       <span class="keyword">if</span>(mEncoder != <span class="keyword">null</span>) &#123;</span><br><span class="line">           mEncoder.stop();</span><br><span class="line">           mEncoder.release();</span><br><span class="line">           mEncoder = <span class="keyword">null</span> ;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(mMuxer != <span class="keyword">null</span>) &#123;</span><br><span class="line">           mMuxer.stop();</span><br><span class="line">           mMuxer.release();</span><br><span class="line">           mMuxer = <span class="keyword">null</span> ;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(mInputSurface != <span class="keyword">null</span>) &#123;</span><br><span class="line">           mInputSurface.release();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>接下来就是重头戏drainEncoder了，这里给他传入的参数是一个EOS标志，我们在Canvas上的绘图结束之后再给他传EOS，标志着输入流的结束。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">drainEncoder</span><span class="params">(<span class="keyword">boolean</span> endOfStream)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//设置超时时间</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> TIMEOUT_USEC = <span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(endOfStream) &#123;</span><br><span class="line">        <span class="keyword">if</span>(VERBOSE) Log.d(TAG, <span class="string">"sending eos to encoder"</span>);</span><br><span class="line">        mEncoder.signalEndOfInputStream();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ByteBuffer[] encoderOutputBuffers = mEncoder.getOutputBuffers();</span><br><span class="line">    <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">     <span class="comment">//获取编码的状态，让输出流缓冲队列出列，dequeueOutputBuffer会返回一个int类型的值作为编码状态</span></span><br><span class="line">        <span class="keyword">int</span> encodeStatus = mEncoder.dequeueOutputBuffer(mBufferInfo,TIMEOUT_USEC);</span><br><span class="line">      <span class="comment">//处理各种可能出现的编码状态</span></span><br><span class="line">        <span class="keyword">if</span>(encodeStatus == MediaCodec.INFO_TRY_AGAIN_LATER) &#123;</span><br><span class="line">            <span class="comment">// no output available yet</span></span><br><span class="line">            <span class="keyword">if</span>(!endOfStream) &#123;</span><br><span class="line">                <span class="keyword">break</span>; <span class="comment">// out of while</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span>(VERBOSE) Log.d(TAG, <span class="string">"no output available ,spinning to await EOS"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (encodeStatus == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) &#123;</span><br><span class="line">            <span class="comment">//not expected for an encoder</span></span><br><span class="line">            encoderOutputBuffers = mEncoder.getOutputBuffers();</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (encodeStatus == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) &#123;</span><br><span class="line">            <span class="comment">// should happen before receiving buffers ,and should only happen once</span></span><br><span class="line">            <span class="keyword">if</span>(mMuxerStarted) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"format changed twice"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// the first time format change </span></span><br><span class="line">          <span class="comment">//format改变 意味着要开始将得到的数据编码成mp4了，启动mMuxer</span></span><br><span class="line">            MediaFormat newFormat = mEncoder.getOutputFormat();</span><br><span class="line">            Log.d(TAG, <span class="string">"encoder output format changed : "</span> + newFormat);</span><br><span class="line">            <span class="comment">// now we have the magic goodies ,start the muxer</span></span><br><span class="line">            mTrackIndex = mMuxer.addTrack(newFormat);</span><br><span class="line">            mMuxer.start();</span><br><span class="line">            mMuxerStarted = <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (encodeStatus &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            Log.w(TAG, <span class="string">"unexpected result from encoder dequeue output buffers"</span> +</span><br><span class="line">                encodeStatus);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 启动了mMuxer之后的逻辑 以Byte为单位来读取数据</span></span><br><span class="line">            ByteBuffer encodeData = encoderOutputBuffers[encodeStatus];</span><br><span class="line">            <span class="keyword">if</span>(encodeData == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"encoder data "</span> + encodeStatus + <span class="string">"is null"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>((mBufferInfo.flags &amp; MediaCodec.BUFFER_FLAG_CODEC_CONFIG) != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// The codec config data was pulled out and fed to the muxer when we got the INFO_OUTPUT_FORMAT_CHANGED status.  Ignore it.</span></span><br><span class="line">                <span class="keyword">if</span> (VERBOSE) Log.d(TAG, <span class="string">"ignoring BUFFER_FLAG_CODEC_CONFIG"</span>);</span><br><span class="line">                mBufferInfo.size = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(mBufferInfo.size != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span>(!mMuxerStarted) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"muxer hasn't started"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">              <span class="comment">//这段代码是定位我们要写入的数据，通过在dequeueOutputBuffer中传入的mBufferInfo来设置</span></span><br><span class="line">                encodeData.position(mBufferInfo.offset);</span><br><span class="line">                encodeData.limit(mBufferInfo.offset+mBufferInfo.size);</span><br><span class="line">              </span><br><span class="line">                mBufferInfo.presentationTimeUs = mFakePts;</span><br><span class="line">                mFakePts += <span class="number">1000000L</span> /FRAMES_PER_SECOND;</span><br><span class="line">              <span class="comment">//通过mMuer写入编码后的数据</span></span><br><span class="line">                mMuxer.writeSampleData(mTrackIndex,encodeData,mBufferInfo);</span><br><span class="line">                <span class="keyword">if</span>(VERBOSE) Log.d(TAG, <span class="string">"send "</span> + mBufferInfo.size + <span class="string">"bytes to muxer "</span>);</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="comment">//释放持有的OutputBuffer</span></span><br><span class="line">            mEncoder.releaseOutputBuffer(encodeStatus,<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>((mBufferInfo.flags &amp; MediaCodec.BUFFER_FLAG_END_OF_STREAM) != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span>(!endOfStream) &#123;</span><br><span class="line">                    Log.d(TAG, <span class="string">"reach end of stream unexpretly"</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span>(VERBOSE) Log.d(TAG, <span class="string">"reach end of stream"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>; <span class="comment">// break of a while</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    其实编码的样例代码的函数封装基本上就代表了使用MediaCodec的几个主要的步骤了，编码和解码，其实都大同小异。无非一个是利用MediaExtractor从已有文件中抓取轨道读取数据，而另一个则是将较为底层的数据利用MediaMuxer来合成。使用的时候还是要多加小心，里面有很多不小心就会踩的坑。</p><p>​    PS：图书馆的插座居然都没有电，我已经要报警了!!!</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="[https://github.com/google/grafika]">grafic</a></p><p><a href="[https://github.com/vecio/MediaCodecDemo]">MediaCodecDemo</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;MediaCodec编码与解码学习笔记&quot;&gt;&lt;a href=&quot;#MediaCodec编码与解码学习笔记&quot; class=&quot;headerlink&quot; title=&quot;MediaCodec编码与解码学习笔记&quot;&gt;&lt;/a&gt;MediaCodec编码与解码学习笔记&lt;/h1&gt;&lt;p&gt;上了秦爷的车，进了视频渲染的坑。&lt;/p&gt;
&lt;p&gt;这两天跟着几个Demo敲了一下MediaCodec的编码和解码，记录一下。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://developer.android.com/reference/android/media/MediaCodec.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AndroidDeveloper MediaCodec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方的文档写的很详细，应该要仔细去翻阅一下&lt;/p&gt;
&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;​    MediacCodec是Android平台的一个音视频的编解码类，是比较底层的多媒体工具类，经常配合使用的有MediaFormat（媒体格式类）、MediaExtractor（抓取多媒体轨道）、MediaMuxer（混合轨道）、Surface（容器）这几个类。&lt;/p&gt;
&lt;p&gt;​    MediaCodec主要的工作过程就是它持有一个InputBuffer和OutputBuffer，使用的时候，根据需要提供输入流，交给codec处理，然后拿到输出流作为处理后的结果。这是一个典型的“生产者-消费者”模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/codecState.png&quot; alt=&quot;MediaCodecStates&quot;&gt;&lt;/p&gt;
&lt;p&gt; 上面是Codec的状态机&lt;/p&gt;
&lt;p&gt;使用的主要流程就是通过MediaFormat来配置MediaCodec 进入Configured状态 然后start 不断地取出输入流 直到EOS标识，使用完后释放占用的资源。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Android" scheme="https://tobiaslee.top/tags/Android/"/>
    
      <category term="MediaCodec" scheme="https://tobiaslee.top/tags/MediaCodec/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Java虚拟机-重写和重载的实现</title>
    <link href="https://tobiaslee.top/2017/02/14/Override-and-Overload/"/>
    <id>https://tobiaslee.top/2017/02/14/Override-and-Overload/</id>
    <published>2017-02-14T08:11:00.000Z</published>
    <updated>2017-02-14T08:12:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>在《深入理解Java虚拟机》看到8.3 方法调用这一节的时候，突然就想到上次看到的三道清华Java测试题，里面就有一题考察的是关于Java 方法的Overload。<br>涉及到的一个核心问题就是： Java是怎么知道要调用哪个方法的?<br>首先，方法调用不等同于方法执行。方法调用唯一的任务就是确定调用方法的版本。<br>而Class文件中不包含方法调用的入口地址，而只是一个符号引用。这给Java带来了动态扩展的能力，但也使得方法调用的过程变的更为复杂。</p><a id="more"></a><p>Java中的方法调用分为两大类：</p><p>1、解析调用(Resolution)： 在类加载的解析阶段，会把其中的一部分符号引用转化为直接引用。前提是：方法在程序运行之前，就有一个可确定的调用版本，且该版本在运行期不可变。即“编译期可知，运行期不变”，符合这个要求的主要包括静态方法和私有方法两大类，前者与类型直接关联，后者外部无法调用，因此无法通过继承重写。</p><p>2、分派调用(Dispatch)：又分为 “静态分派”  “动态分派” “多分派” “单分派”。在运行期间才能确定调用方法的版本。</p><h2 id="解析调用："><a href="#解析调用：" class="headerlink" title="解析调用："></a>解析调用：</h2><p>Java虚拟机中提供了5条方法调用的字节码指令</p><blockquote></blockquote><ol><li>invokestatic: 调用静态方法</li><li>invokespecial: 调用实例构造器<init>方法、私有方法和父类方法</init></li><li>invokevirtual:调用所有的虚方法</li><li>invokeinterface:调用接口方法，会在运行时确定一个实现此接口的对象</li><li><p>invokedynamic: 先在运行时动态解析出调用点限定符所引用的方法，然后再执行</p><p>上面前4条调用指令、分派逻辑是固化在Java虚拟机内部的，而invokedynamic则由用户指定的引导方法决定。<br>只要能被 invokestatic 和 invokespecial 调用的方法，都可以在解析阶段确定唯一的调用版本。符合这个条件的有 <strong>静态方法、私有方法、实例构造器、父类方法</strong> 四种，他们在类加载阶段就会把<strong>方法的符号引用解析为直接引用（内存地址入口）</strong>。这类方法也称为非虚方法。<br>以下是《深入理解Java虚拟机》的示例代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StaticResolution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span>  <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"hello world"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        StaticResolution.sayHello();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>在命令行通过javac 编译后得到.class文件 再利用javap来查看字节码发现</p><blockquote><p> $ javap -verbose StaticResolution<br>  public static void main(java.lang.String[]);<br>    descriptor: ([Ljava/lang/String;)V<br>    flags: ACC_PUBLIC, ACC_STATIC<br>    Code:<br>        stack=0, locals=1, args_size=1<br>         0: invokestatic  #5                  // Method sayHello:()V<br>         3: return<br>      LineNumberTable:<br>        line 6: 0<br>        line 7: 3<br>}</p></blockquote><p>确实是通过invokestatic来调用了方法。<br>Java中非虚方法还有一种，final方法,虽然是用invokevirtual来调用的，但是因为它无法被覆盖，没有其他版本，多态的选择也一定是唯一的。<strong>在Java语言规范中规定了final方法是一种非虚方法</strong></p><h2 id="分派调用："><a href="#分派调用：" class="headerlink" title="分派调用："></a>分派调用：</h2><p>分派调用揭示了OOP多态性的一些最基本的体现。“重载”和“重写”，就是其中之一。</p><h3 id="1-静态分派"><a href="#1-静态分派" class="headerlink" title="1.静态分派"></a>1.静态分派</h3><p>先来看一段代码<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StaticDispatch</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Human</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Man</span> <span class="keyword">extends</span>  <span class="title">Human</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Woman</span> <span class="keyword">extends</span>  <span class="title">Human</span></span>&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(Human guy)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello human"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(Man guy)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello man"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(Woman guy)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello woman"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Human man = <span class="keyword">new</span> Man();</span><br><span class="line">        Human woman = <span class="keyword">new</span> Woman();</span><br><span class="line">        StaticDispatch sr = <span class="keyword">new</span> StaticDispatch();</span><br><span class="line">        sr.sayHello(man);</span><br><span class="line">        sr.sayHello(woman);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出是</p><blockquote><p>Hello human<br>Hello human</p></blockquote><p>为了弄清楚这是为什么，需要先明白一个定义</p><blockquote><p>Human man = new Man();</p></blockquote><p>这里 “Human”是 man变量的 静态类型 (Static Type) 或者叫 外观类型(Apparent Type)<br>而后面的 “Man” 则是 man 变量的 实际类型(Actual Type)<br>静态类型都实际类型在程序中都可以发生变化，区别在于<strong>静态类型的变化仅仅是在使用时发生，而其本身的静态类型并不发生改变</strong><br>看一个例子<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//实际类型改变</span></span><br><span class="line">Human man = <span class="keyword">new</span> Man();</span><br><span class="line">man = <span class="keyword">new</span> Woman();</span><br><span class="line"><span class="comment">//静态类型改变</span></span><br><span class="line">sr.sayHello((Man) man);</span><br><span class="line">sr.sayHello((Woman) man);</span><br></pre></td></tr></table></figure></p><p>上述代码中的实际类型改变之后,Man的实际类型就由”Man”变成了”Woman”<br>而通过强制转换的静态类型转换，只是通知调用者，以”Man”或者”Woman”的方式来看待man变量，<strong>其静态类型“Human”并没有发生改变</strong></p><p>方法调用确定版本的要素有两个:<br>1.方法的调用者 这里都是sr<br>2.方法的参数的数量和数据类型<br>静态类型在编译期可知，而动态类型只有实际运行时能够获知。<br><strong>虚拟机（准确说是编译器）是通过参数静态类型作为重载的判定依据</strong><br>因此在编译阶段，Javac编译器会根据参数的静态类型决定使用哪个重载版本。<br>两个变量实际类型不同，然并卵，静态类型相同，就决定了他们会使用同一个重载函数。</p><p>所以，所有依赖静态类型来定位方法执行版本的分派动作就是静态分派。典型应用就是方法重载(Overload)<br><strong>静态分派发生在编译阶段</strong> 但有些时候，这个版本并不唯一，只能确定一个“更加合适的版本”，就是上次清华Java题里面null的情况。<br>这种情况主要发生在 字面量作为变量的情况下，因为字面量不需要定义，所以也没有显式的静态类型。<br>来看一个极其恶心人的代码。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Overload</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(Object obj)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello object"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(<span class="keyword">int</span> arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello int"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(<span class="keyword">long</span> arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello long"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(Character arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello character"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(<span class="keyword">char</span> arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello char"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(<span class="keyword">char</span> ...arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello char ..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(Serializable arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello Serializable "</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        sayHello(<span class="string">'a'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>运行之后输出</p><blockquote><p>Hello char</p></blockquote><p>没毛病。如果我们把sayHello(char arg)给注释掉 输出就变成了</p><blockquote><p>Hello int</p></blockquote><p>也能理解， ‘a’ 发生一次自动类型转换，变成一个整形数97，再把 sayHello(int arg)注了</p><blockquote><p>Hello long</p></blockquote><p>这里又发生了一次自动类型转换,’a’-&gt;int 97 -&gt; long 97 匹配了long的重载<br>虽然这里没有 double float类型的重载 但事实上如果有 就还会发生<br>按照 char -&gt; int -&gt; long -&gt;float -&gt; double 的顺序发生自动类型转换 但不会匹配byte 和short类型的重载<br>，因为类型转换不安全。再把long也注释掉。</p><blockquote><p>Hello character</p></blockquote><p>嘿呀，这里就发生了一次自动装箱(AutoBoxing) 原始类型char ‘a’被包装它对应的封装类型 java.lang.Character 所以匹配了Character的重载，继续注掉Character，输出</p><blockquote><p>Hello Serializable</p></blockquote><p>有点莫名其妙，为什么会输出这个呢？java.lang.Serializable 是 java.lang.Character实现的一个接口<br>当’a’被自动装箱成Character还是找不到装箱类匹配的函数,他就找到了装箱类实现的接口类型。所以又发生了一次自动转型，Character转型为Serializable 安全地转型为它的接口或者父类 但是肯定不能变成Integer。<br>Character还实现了Comparable这个接口，如果有一个Comparable&lt; Character &gt;的重载方法，那么这两个接口的优先级是相同的，会提示Ambiguous method call 模糊的方法调用而拒绝编译<br>除非在调用时显示的指明字面量的类型<br>比如:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sayHello(Comparable&lt;Character&gt; <span class="string">'a'</span>);</span><br></pre></td></tr></table></figure></p><p>继续我们的注释，输出是</p><blockquote><p>Hello object</p></blockquote><p>Amazing!为什么不选貌似更为接近的变长参数char … 呢，因为变长参数的优先级是重载方法中最低的。<br>‘a’装箱成Character以后开始向父类转型，辈分越高，优先级越低，即使是null，也是这样一个顺序。<br>再注释掉，自然只剩下一个结果</p><blockquote><p>Hello char …</p></blockquote><p>注意不要混淆 <strong>解析和静态分配</strong>：<br>两种确定方式，是在不同层次上去筛选的过程。<br>静态方法会在类加载期进行解析，但是静态方法也可能有多个重载版本，这就是静态分派了。<br>eg.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResolutionAndDispatch</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(<span class="keyword">int</span> arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello int"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(<span class="keyword">char</span> arg)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello char"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        ResolutionAndDispatch.sayHello(<span class="string">'a’);</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure></p><p>在类加载阶段，Class中的指向sayHello的符号引用全部被换成直接引用，<br>而在编译时,’a’  实际上是 一个char 静态类型 就作为编译器静态分派的判断依据。 </p><h3 id="2、动态分派"><a href="#2、动态分派" class="headerlink" title="2、动态分派"></a>2、动态分派</h3><p>动态分派和多态性另一个重要体现 重写 (Override) 有密切的关系。<br>看代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicDispatch</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Human</span></span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Man</span> <span class="keyword">extends</span> <span class="title">Human</span></span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"man say hello"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Woman</span> <span class="keyword">extends</span> <span class="title">Human</span></span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"woman say hello"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Human man = <span class="keyword">new</span> Man();</span><br><span class="line">        Human woman = <span class="keyword">new</span> Woman();</span><br><span class="line">        man.sayHello();</span><br><span class="line">        woman.sayHello();</span><br><span class="line">        man = <span class="keyword">new</span> Woman();</span><br><span class="line">        man.sayHello();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出:</p><blockquote><p>man say hello<br>woman say hello<br>woman say hello</p></blockquote><p>非常顺眼的输出，感觉没有什么奇怪，但是为什么这里静态类型都是Human 没有去调用父类的方法呢??<br>用javap 输出字节码看一下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public static void main(java.lang.String[]);</span><br><span class="line">  descriptor: ([Ljava/lang/String;)V</span><br><span class="line">  flags: ACC_PUBLIC, ACC_STATIC</span><br><span class="line">  Code:</span><br><span class="line">    stack=2, locals=3, args_size=1</span><br><span class="line">       0: new           #2                  // class DynamicDispatch$Man</span><br><span class="line">       3: dup</span><br><span class="line">       4: invokespecial #3                  // Method DynamicDispatch$Man.&quot;&lt;in                                  it&gt;&quot;:()V</span><br><span class="line">       7: astore_1</span><br><span class="line">       8: new           #4                  // class DynamicDispatch$Woman</span><br><span class="line">      11: dup</span><br><span class="line">      12: invokespecial #5                  // Method DynamicDispatch$Woman.&quot;&lt;                                  init&gt;&quot;:()V</span><br><span class="line">      15: astore_2</span><br><span class="line">      16: aload_1</span><br><span class="line">      17: invokevirtual #6                  // Method DynamicDispatch$Human.sa                                  yHello:()V</span><br><span class="line">      20: aload_2</span><br><span class="line">      21: invokevirtual #6                  // Method DynamicDispatch$Human.sa                                  yHello:()V</span><br><span class="line">      24: new           #4                  // class DynamicDispatch$Woman</span><br><span class="line">      27: dup</span><br><span class="line">      28: invokespecial #5                  // Method DynamicDispatch$Woman.&quot;&lt;                                  init&gt;&quot;:()V</span><br><span class="line">      31: astore_1</span><br><span class="line">      32: aload_1</span><br><span class="line">      33: invokevirtual #6                  // Method DynamicDispatch$Human.sa                                  yHello:()V</span><br><span class="line">      36: return</span><br></pre></td></tr></table></figure></p><p>0~15 在做准备动作 我们看到调用了两次 invokespecial 是调用了实例构造器 构造了man 和woman两个实例，并且把他们的引用放在1、2个局部变量表Slot中<br>接下来的16~21，16和20两句aload_1和aload_2 把创建的对象的引用压到栈顶，这两个对象是将要执行的方法sayHello()的执行者，称作接受者(Receiver) 17和21两句的方法调用指令 和参数 都是一样的，但是最终执行的目标方法不同，原因就是invokevirtual指令的多态查找:</p><blockquote></blockquote><p>1、找到操作数栈顶的第一个元素所指向的对象的实际类型，记作C (这里就是Man或者是Woman)<br>2、如果在类型C中找到和常量中描述符合简单名称都相符合的方法，则进行访问权限校验，如果通过就返回这个方法的直接引用，查找结束；不通过，返回 IllegalAccessError异常<br>3、否则（没找到符合的方法），按照继承关系从下往上对C的各个父类进行第2步的搜索和验证过程<br>4、如果始终没有找到合适的方法，则跑出java.lang.AbstractMethodError异常</p><p><strong>invokevirtual指令 第一步就是确定接受者的实际类型</strong><br>所以两次调用把<br><strong>常量池相同的类方法符号引用解析到了不同的直接引用上，这就是Java方法重写的本质</strong><br>这种在运行期间根据实际类型确定方法执行版本的分派过程就是 动态分派</p><h3 id="3-单分派与多分派"><a href="#3-单分派与多分派" class="headerlink" title="3.单分派与多分派"></a>3.单分派与多分派</h3><p>方法的接受者与方法的参数统称为方法的 <strong>宗量</strong><br>单分派和多分派，顾名思义，根据一个宗量来对目标方法选择就是单分派，多于一个，则是多分派。<br>看代码<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dispatch</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">QQ</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">_360</span></span>&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Father</span></span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hardChoice</span><span class="params">(QQ arg)</span></span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"father choose QQ"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hardChoice</span><span class="params">(_360 arg)</span></span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"father choose 360"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Son</span> <span class="keyword">extends</span> <span class="title">Father</span></span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hardChoice</span><span class="params">(QQ arg)</span></span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"son choose QQ"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hardChoice</span><span class="params">(_360 arg)</span></span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"son choose 360"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Father father = <span class="keyword">new</span> Father();</span><br><span class="line">        Father son = <span class="keyword">new</span> Son();</span><br><span class="line">        father.hardChoice(<span class="keyword">new</span> _360());</span><br><span class="line">        son.hardChoice(<span class="keyword">new</span> QQ());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>father choose 360<br>son choose QQ</p></blockquote><p>编译期间编译器的选择，也就是静态分派，<br>依据有两条：1.方法的接受者是father还是son 2.参数的静态类型是360还是QQ<br>这次选择最终产生        </p><blockquote><p>24: invokevirtual #8                  // Method Dispatch$Father.hardChoice:(LDispatch$_360;)V<br>        35: invokevirtual #11                 // Method Dispatch$Father.hardChoice:(LDispatch$QQ;)V</p></blockquote><p>这两条分别指向Father.hardChoice(QQ)和Father.hardChoice(360)的invokevirtual指令<br>根据两个宗量进行选择，所以Java的静态分派是多宗量分派。</p><p>运行期间，JVM的选择，也就是动态分派<br>编译期间已经确定了son.sayHello(new QQ()) 这句 目标方法的签名必须是 hardChoice(QQ) 所以虚拟机不管是“腾讯QQ”还是“奇瑞QQ” 参数的静态类型和实际类型不会对目标方法的选择产生影响<br>唯一能够影响JVM选择的，只由接受者的实际类型是Son还是Father<br>仅仅根据接受者这一宗量，所以，动态分派是 单宗量分派</p><h2 id="虚拟机动态分派的实现"><a href="#虚拟机动态分派的实现" class="headerlink" title="虚拟机动态分派的实现"></a>虚拟机动态分派的实现</h2><p>动态分派是很频繁的过程，虚拟机对此用了一种“稳定优化”的手段来加快这一过程。<br>为类在方法区中建立一个虚方法表，用虚方法表索引来代替元数据查找以提高性能。<br>虚方法表中存放着各个方法的实际入口地址，如果子类没有重写父类方法，那么子类的虚方法表中该函数的入口地址和父类相同方法的地址是一致的。<br>为了程序实现的方便，具有相同签名的方法，父类和子类虚方法表中都应当具有相同的索引，这样类型改变，只要换一张虚方法表就可以了，迅速通过相同的索引找到对应方法的入口地址。<br>方法表一般在类加载的连接阶段进行初始化。</p><p>实际上跟着样例敲一遍代码走一遍javap 看看字节码收获还是很大的。<br>总结一下：<br>Java是一门静态多宗量，动态单宗量的语言。<br>静态分派，编译期间选择，多和重载有关。（类内部的方法多态性）<br>动态分派，运行期才执行，与重写有关。（类继承的多态性）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在《深入理解Java虚拟机》看到8.3 方法调用这一节的时候，突然就想到上次看到的三道清华Java测试题，里面就有一题考察的是关于Java 方法的Overload。&lt;br&gt;涉及到的一个核心问题就是： Java是怎么知道要调用哪个方法的?&lt;br&gt;首先，方法调用不等同于方法执行。方法调用唯一的任务就是确定调用方法的版本。&lt;br&gt;而Class文件中不包含方法调用的入口地址，而只是一个符号引用。这给Java带来了动态扩展的能力，但也使得方法调用的过程变的更为复杂。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Java" scheme="https://tobiaslee.top/tags/Java/"/>
    
      <category term="JVM" scheme="https://tobiaslee.top/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>数据结构学习笔记-栈的应用</title>
    <link href="https://tobiaslee.top/2017/02/13/DataStructureLearning-Stack2/"/>
    <id>https://tobiaslee.top/2017/02/13/DataStructureLearning-Stack2/</id>
    <published>2017-02-13T07:43:42.000Z</published>
    <updated>2017-03-17T13:35:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>栈的特点 LIFO 后进先出 使得它在处理一些相关问题的时候非常强大</p><blockquote><p>1、逆序输出(conversion)<br>    特点：1.输出与处理过程相反 2.递归深度、输出长度不易预知<br>    典型问题: 进制转换<br>2、递归嵌套<br>    特点:1.具有自相似的问题可递归描述 2.分支和位置不确定<br>    典型问题: 括号匹配<br>3、延迟缓冲<br>    特点:线性扫描算法中要预读足够长才能够处理前缀<br>    典型问题: 中缀表达式<br>4、RPN (逆波兰表达式) 基于栈结构的特定计算模式<br>—— 清华大学 邓俊辉《数据结构》 </p></blockquote><a id="more"></a><p>这里就举一道 括号匹配 和 布尔表达式求值 的题目作为例子，尝试利用栈结构来解决一些问题。</p><blockquote><p>括号匹配<br>    对于给定的字符串 只包含( { [ ] } ) 括号可以嵌套使用<br>    合法如 “((){}[{}])” 则输出Yes 非法如 “((){}[)” 则输出 No</p></blockquote><p>对题目进行初步分析后，我们想，怎么才能缩小问题的规模呢?<br>对于给定的一串括号 我们发现<br><strong>遇到右括号时如果存在对应的左括号 则可以消去这对括号 原问题的合法性和其消去一对括号的合法性是相同的</strong><br>比如说 如果”((){})”合法 那么 “({})”也必然是合法 那么”()”合法 就回归到一个最基本的情形了<br>我们通过消去匹配的括号，来缩小问题规模<br>也就是 <strong>减而治之(Decrease and Conquer)</strong> 的策略<br>利用栈结构，我们对输入的字符串进行扫描<br>如果我们遇到一个左括号，则令其入栈，遇到右括号，则令栈顶弹出。<br>如果不匹配，则终止扫描，输出No;如果匹配，弹出栈顶的左括号，就达到了消去括号的目的。<br>如果最后整个栈为空栈，则证明所有的括号都成功消去，即匹配。<br>以下是C语言实现的代码</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//括号匹配 通过栈来实现</span></span><br><span class="line"><span class="comment">//将所有的左括号 压入栈 然如果碰到右括号 则令左括号弹出 </span></span><br><span class="line">...</span><br><span class="line"><span class="comment">//为节省篇幅 省略定义栈结构相关代码</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">char</span> <span class="title">pop</span><span class="params">(Stack* ps)</span></span>&#123; <span class="comment">//出栈 并且返回出栈的节点的数据 这里的数据域是char类型</span></span><br><span class="line"><span class="keyword">if</span>(ps-&gt;size == <span class="number">0</span>)&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Empty stack!"</span>);</span><br><span class="line"><span class="keyword">return</span> ;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">ps-&gt;size -- ;</span><br><span class="line"><span class="keyword">char</span> ret = ps-&gt;top-&gt;data;</span><br><span class="line">Node* temp = ps-&gt;top ;</span><br><span class="line">ps-&gt;top = ps-&gt;top-&gt;next;</span><br><span class="line"><span class="built_in">free</span>(temp);</span><br><span class="line">temp = <span class="literal">NULL</span> ;</span><br><span class="line"><span class="keyword">return</span> ret ;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">isEmpty</span><span class="params">(Stack* ps)</span></span>&#123; <span class="comment">// 判断栈是否为空栈</span></span><br><span class="line"><span class="keyword">return</span> ps-&gt;size &gt; <span class="number">0</span> ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">checkPro</span><span class="params">(Stack* <span class="built_in">stack</span>,<span class="keyword">char</span> * s )</span></span>&#123; <span class="comment">//检测括号匹配</span></span><br><span class="line"><span class="keyword">int</span> i ;</span><br><span class="line"><span class="keyword">char</span> c ;</span><br><span class="line"><span class="keyword">for</span>( i = <span class="number">0</span> ; i &lt;<span class="built_in">strlen</span>(s); i++)&#123;</span><br><span class="line"><span class="keyword">if</span>(s[i] == <span class="string">'('</span>)&#123;</span><br><span class="line">push(<span class="built_in">stack</span>,<span class="string">'s'</span>); <span class="comment">// 小括号 对应的数据为 's' 入栈</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (s[i] == <span class="string">'['</span>)&#123;</span><br><span class="line">push(<span class="built_in">stack</span>,<span class="string">'m'</span>);<span class="comment">// 中括号 对应的数据为 'm' 入栈</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(s[i] == <span class="string">'&#123;'</span>)&#123;</span><br><span class="line">push(<span class="built_in">stack</span>,<span class="string">'l'</span>);<span class="comment">// 大括号</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(s[i] == <span class="string">')'</span> &amp;&amp; !isEmpty(<span class="built_in">stack</span>))&#123; <span class="comment">//碰到右小括号 且 栈不为空</span></span><br><span class="line">c = pop(<span class="built_in">stack</span>); <span class="comment">// 栈顶出栈</span></span><br><span class="line"><span class="keyword">if</span>( c != <span class="string">'s'</span>)&#123; <span class="comment">// 弹出的栈 不匹配</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(s[i] == <span class="string">']'</span> &amp;&amp; !isEmpty(<span class="built_in">stack</span>))&#123;<span class="comment">//同小括号</span></span><br><span class="line">c = pop(<span class="built_in">stack</span>);</span><br><span class="line"><span class="keyword">if</span>( c != <span class="string">'m'</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(s[i] == <span class="string">'&#125;'</span> &amp;&amp; !isEmpty(<span class="built_in">stack</span>))&#123;<span class="comment">//同小括号</span></span><br><span class="line">c = pop(<span class="built_in">stack</span>);</span><br><span class="line"><span class="keyword">if</span>( c != <span class="string">'l'</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> isEmpty(<span class="built_in">stack</span>); <span class="comment">// 如果扫描进行结束没有退出 栈为空则匹配 非空则说明有多余的左括号</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">Stack <span class="built_in">stack</span> ;</span><br><span class="line">init(&amp;<span class="built_in">stack</span>);</span><br><span class="line"><span class="keyword">char</span> s[<span class="number">100</span>];</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%s"</span>,&amp;s);</span><br><span class="line"><span class="keyword">if</span>(checkPro(&amp;<span class="built_in">stack</span>,s))&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"yes"</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"No"</span>);</span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Boolean Expressions<br>[POJ Boolean Expressions][1]<br>  [1]: <a href="http://poj.org/problem?id=2106" target="_blank" rel="noopener">http://poj.org/problem?id=2106</a><br>  利用栈结构进行表达式求值，主要实现的思路是：<br>  1、操作数与值分离，用独立的两个栈来存放操作数和值<br>  2、判断操作数的优先级，一般来说，如果当前操作数优先级低于栈顶操作数的优先级，则说明栈顶的操作数可以执行了，就弹出操作数栈顶和相应的值栈，进行运算，并且将运算结果重新压入值栈中<br> 这一题布尔表达式求值 实际上操作数只有 () ! | &amp; 五个<br> (优先级最高 !次之 &amp;再次 然后是| )可以定义为最低<br> 然后因为这题值栈事实上就是V/F 操作数栈也不过5个操作数 所以可以用两个数组来进行模拟简单的栈<br> 又由于!的优先级是很高的 所以我们可以把 !V !F 看做是一个值 来进行压入值栈的操作<br> 计算只要负责 | 和 &amp; 就可以了</p><p> 以下是借鉴自某CSDN博客的代码</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX 150</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">bool</span> val[MAX]; <span class="comment">// 值栈 </span></span><br><span class="line"><span class="keyword">int</span> op[MAX];  <span class="comment">//操作数栈 </span></span><br><span class="line"><span class="keyword">int</span> vp, pp; <span class="comment">// 栈顶下标</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 定义优先级  </span></span><br><span class="line"><span class="comment">    (:  0  </span></span><br><span class="line"><span class="comment">| : 1 </span></span><br><span class="line"><span class="comment"> &amp; : 2</span></span><br><span class="line"><span class="comment">! : 3</span></span><br><span class="line"><span class="comment"> ) : 4</span></span><br><span class="line"><span class="comment">*/</span>   </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">bool</span> b)</span></span>&#123;  <span class="comment">//入栈操作 </span></span><br><span class="line"><span class="keyword">while</span>( pp &amp;&amp;  (op[pp<span class="number">-1</span>] == <span class="number">3</span>) )&#123; <span class="comment">// 处理!操作 如果操作数栈优先级为3 则说明为!操作 </span></span><br><span class="line">b = ! b; <span class="comment">// 取反</span></span><br><span class="line">--pp; <span class="comment">// 栈顶下标-1 相当于 弹出栈顶操作数</span></span><br><span class="line">&#125;</span><br><span class="line">val[vp++] = b; <span class="comment">// 将值压入栈中 并且vp++ 栈顶下标+1 </span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">calc</span><span class="params">()</span></span>&#123; <span class="comment">// 计算操作</span></span><br><span class="line"><span class="keyword">bool</span> b = val[--vp]; <span class="comment">// 取出一个值 </span></span><br><span class="line"><span class="keyword">bool</span> a = val[--vp];</span><br><span class="line"><span class="keyword">int</span> opr = op[--pp]; <span class="comment">//取出一个操作数  这里的pp 和 vp 原来都是比实际栈顶下标大 1的</span></span><br><span class="line">insert(opr == <span class="number">1</span> ? a | b : a &amp; b ); <span class="comment">// 压入结果  如果操作数为1即为 | 不为1 则为 &amp;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>; <span class="comment">// 计算第几个表达式的值</span></span><br><span class="line"><span class="keyword">char</span> c ;</span><br><span class="line"><span class="keyword">while</span>( ~(c = getchar()) )&#123; <span class="comment">// 这是个小技巧 EOF = -1 按位取反 等价于 (c = getchar()) != EOF</span></span><br><span class="line">vp = pp = <span class="number">0</span> ; <span class="comment">// 初始化下标为 0 pp也代表着操作数的数量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">do</span>&#123;</span><br><span class="line"><span class="keyword">if</span>( c == <span class="string">'('</span>)&#123; <span class="comment">//左括号 压入栈中 </span></span><br><span class="line">op[pp++] = <span class="number">0</span> ;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>( c == <span class="string">')'</span>)&#123;</span><br><span class="line"><span class="keyword">while</span>(pp &amp;&amp; op[pp<span class="number">-1</span>])&#123; <span class="comment">//处理左括号以前所有的操作 处理完之后相当于消去了()</span></span><br><span class="line">calc();</span><br><span class="line">&#125;</span><br><span class="line">--pp; <span class="comment">// pp-- 相当于弹出 ) </span></span><br><span class="line">insert(val[--vp]); <span class="comment">// 如果表达式为 !() 形式 还要计算一次! </span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>( c == <span class="string">'!'</span>)&#123;</span><br><span class="line">op[pp++] = <span class="number">3</span>; <span class="comment">//操作数 ! 入栈  </span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>( c == <span class="string">'|'</span>)&#123; </span><br><span class="line"><span class="keyword">while</span>( pp &amp;&amp; op[pp<span class="number">-1</span>] &gt;= <span class="number">1</span>)&#123;  <span class="comment">// 计算|之前 优先级比|高的操作</span></span><br><span class="line">calc();</span><br><span class="line">&#125;</span><br><span class="line">op[pp++] = <span class="number">1</span>; <span class="comment">// 压入 | 操作</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>( c == <span class="string">'&amp;'</span>)&#123;</span><br><span class="line"><span class="keyword">while</span>( pp &amp;&amp; op[pp<span class="number">-1</span>] &gt;= <span class="number">2</span>)&#123; <span class="comment">// 计算&amp;之前 优先级比&amp;高的操作</span></span><br><span class="line">calc();</span><br><span class="line">&#125;</span><br><span class="line">op[pp++] = <span class="number">2</span> ;<span class="comment">// 压入 &amp; 操作</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>( c == <span class="string">'V'</span> || c == <span class="string">'F'</span>)&#123;</span><br><span class="line">insert(c == <span class="string">'V'</span> ? <span class="literal">true</span> : <span class="literal">false</span>); <span class="comment">//压入 true or false</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 空格被忽略</span></span><br><span class="line">&#125;<span class="keyword">while</span>((c = getchar())!= <span class="string">'\n'</span> &amp;&amp;  ~c); <span class="comment">// ~c 等价于 c != EOF</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(pp)&#123; <span class="comment">// 如果还存在操作数 继续计算</span></span><br><span class="line">calc();</span><br><span class="line">&#125;</span><br><span class="line">        <span class="comment">//输出结果</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Expression %d: %c\n"</span>,++cnt,(val[<span class="number">0</span>] ? <span class="string">'V'</span> : <span class="string">'F'</span>));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个代码写的我个人觉得非常的优雅，<br>首先是通过将运算符分优先级后编号，然后就利用两个数组来实现值栈和操作数栈，简洁而有效。<br>然后是两个pp vp 操作数下标 和 值的下标 既作为栈顶 又作为操作数的数量 实际写的过程中 ++ – 其实是很容易搞错的<br>并且将 !的计算放在了 calc()的函数中 这样就可以同等对待剩余的 | 和 &amp;<br>还有特殊的 !()模式的处理 以及 剩余操作数的 处理<br>非常值得学习的一段代码，努力把它消化了</p><p>总结一下：<br>栈结构很强大，LIFO的特点使得它在表达式求值、相关匹配的算法题中起到很大作用。<br>但也不一定要拘泥于栈结构的形式，数组也可以作为栈。而且一般向量(数组)作为栈的<br>会以<strong>向量尾部</strong>作为栈顶，因为这样出栈的操作时间复杂度就是O(1) 而如果在头部， 则是<strong>O(n) 和向量长度成正比</strong>，值得注意。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;栈的特点 LIFO 后进先出 使得它在处理一些相关问题的时候非常强大&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1、逆序输出(conversion)&lt;br&gt;    特点：1.输出与处理过程相反 2.递归深度、输出长度不易预知&lt;br&gt;    典型问题: 进制转换&lt;br&gt;2、递归嵌套&lt;br&gt;    特点:1.具有自相似的问题可递归描述 2.分支和位置不确定&lt;br&gt;    典型问题: 括号匹配&lt;br&gt;3、延迟缓冲&lt;br&gt;    特点:线性扫描算法中要预读足够长才能够处理前缀&lt;br&gt;    典型问题: 中缀表达式&lt;br&gt;4、RPN (逆波兰表达式) 基于栈结构的特定计算模式&lt;br&gt;—— 清华大学 邓俊辉《数据结构》 &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://tobiaslee.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="栈" scheme="https://tobiaslee.top/tags/%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>数据结构学习笔记-栈的实现</title>
    <link href="https://tobiaslee.top/2017/02/13/DataStructureLearning-Stack1/"/>
    <id>https://tobiaslee.top/2017/02/13/DataStructureLearning-Stack1/</id>
    <published>2017-02-13T02:38:26.000Z</published>
    <updated>2017-02-13T02:41:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>栈和向量、列表一样，是<strong>线性序列</strong>的一种<br>但它的核心特点在于：<strong>只能访问和操作序列的一端（栈顶）</strong> 并且是 <strong>LIFO (Last In First Out)</strong><br>可以用一叠盘子来进行比喻，对于一叠盘子，我们对它进行操作的时候，要么是拿走顶上的一块盘子，要么就是在顶上放一块盘子。<br>所以对应的，栈有最为基本的两个操作，push(入栈)和pop(出栈)</p><p>以下就用C语言来实现一个数据类型为int的简单的栈结构</p><a id="more"></a><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Node</span>&#123;</span> <span class="comment">// 栈节点 其实实现的是链表的一个节点</span></span><br><span class="line">    <span class="keyword">int</span> data ; <span class="comment">//数据域</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">Node</span>* <span class="title">next</span>;</span> <span class="comment">// 下一个节点的指针</span></span><br><span class="line">&#125;Node;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Stack</span>&#123;</span> <span class="comment">//栈结构 </span></span><br><span class="line">    <span class="keyword">int</span> size ; <span class="comment">// 栈的大小</span></span><br><span class="line">    Node* top ; </span><br><span class="line">    Node* bottom ; </span><br><span class="line">&#125;Stack;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(Stack* ps)</span></span>&#123; <span class="comment">//初始化栈函数</span></span><br><span class="line">    ps-&gt;size = <span class="number">0</span> ; <span class="comment">// 这里 ps是一个栈指针 结构体指针-&gt;成员 是 结构体-&gt;成员 的一种简便写法 </span></span><br><span class="line">    Node* node = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">    ps-&gt;bottom = ps-&gt;top = node ; <span class="comment">//初始化一个空指针作为栈底</span></span><br><span class="line">    ps-&gt;top-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(Stack* ps,<span class="keyword">int</span> data)</span></span>&#123; <span class="comment">// 入栈函数</span></span><br><span class="line">    ps-&gt;size ++ ; <span class="comment">//  栈长度加 1</span></span><br><span class="line">    Node* newNode = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node)); <span class="comment">//为新节点申请内存空间</span></span><br><span class="line">    newNode-&gt;data = data ; <span class="comment">// 为新节点赋值</span></span><br><span class="line">    newNode-&gt;next = ps-&gt;top ; <span class="comment">// 注意接下来两步的顺序  先将new-&gt;next设为ps-&gt;top 再将newNode作为栈顶</span></span><br><span class="line">    ps-&gt;top = newNode;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">(Stack* ps)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(ps-&gt;size &lt; <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Empty Stack!"</span>);<span class="comment">//空栈无法出栈</span></span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    ps-&gt;size -- ; <span class="comment">// 栈长度减一</span></span><br><span class="line">    Node* temp = ps-&gt;top ; <span class="comment">//temp指针用来进行内存释放操作 避免内存泄露</span></span><br><span class="line">    ps-&gt;top = ps-&gt;top-&gt;next ; <span class="comment">// 将栈顶的下一个节点作为栈顶</span></span><br><span class="line">    <span class="built_in">free</span>(temp); <span class="comment">//释放栈顶内存空间</span></span><br><span class="line">    temp = <span class="literal">NULL</span> ; <span class="comment">// 安全操作</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">tranverse</span><span class="params">(Stack* ps)</span></span>&#123; <span class="comment">//遍历栈结构 并输出数据</span></span><br><span class="line">    <span class="keyword">int</span> i ;</span><br><span class="line">    <span class="keyword">if</span>(ps-&gt;size &lt; <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Empty Stack!"</span>);<span class="comment">//空栈无法遍历</span></span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">while</span>( ps-&gt;size &gt; <span class="number">0</span>)&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d\n"</span>,ps-&gt;top-&gt;data);</span><br><span class="line">pop(ps);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Stack <span class="built_in">stack</span> ;</span><br><span class="line">    init(&amp;<span class="built_in">stack</span>);</span><br><span class="line">    push(&amp;<span class="built_in">stack</span>,<span class="number">1</span>);</span><br><span class="line">    push(&amp;<span class="built_in">stack</span>,<span class="number">5</span>);</span><br><span class="line">    push(&amp;<span class="built_in">stack</span>,<span class="number">2</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"size :%d\n"</span>,ps-&gt;size);</span><br><span class="line">    pop(&amp;<span class="built_in">stack</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"size :%d\n"</span>,ps-&gt;size);</span><br><span class="line">    tranverse(&amp;<span class="built_in">stack</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果</p><blockquote><p>   size :3<br>    size :2<br>    5<br>    1</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;栈和向量、列表一样，是&lt;strong&gt;线性序列&lt;/strong&gt;的一种&lt;br&gt;但它的核心特点在于：&lt;strong&gt;只能访问和操作序列的一端（栈顶）&lt;/strong&gt; 并且是 &lt;strong&gt;LIFO (Last In First Out)&lt;/strong&gt;&lt;br&gt;可以用一叠盘子来进行比喻，对于一叠盘子，我们对它进行操作的时候，要么是拿走顶上的一块盘子，要么就是在顶上放一块盘子。&lt;br&gt;所以对应的，栈有最为基本的两个操作，push(入栈)和pop(出栈)&lt;/p&gt;
&lt;p&gt;以下就用C语言来实现一个数据类型为int的简单的栈结构&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://tobiaslee.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="栈" scheme="https://tobiaslee.top/tags/%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>Retrofit学习和源码解读</title>
    <link href="https://tobiaslee.top/2017/02/05/Retrofit-Learning/"/>
    <id>https://tobiaslee.top/2017/02/05/Retrofit-Learning/</id>
    <published>2017-02-05T08:34:19.000Z</published>
    <updated>2017-02-05T08:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>在小蓝项目里用Retrofit + OkHttp作为网络的框架</p><p>来尝试着解读一下源码并且看能不能自己实现一个 低配版Retrofit</p><p>使用方面 主要步骤就是</p><p>1、添加依赖 Retrofit和OkHttp</p><p>2、定义接口和对应的Response类</p><p>3、在需要调用的时候生成对应的service 并且调用call方法</p><p>4、在Callback方法里处理OnResponse的相关逻辑</p><p>使用方法见RetrofitTest里面利用和风天气Api做的一个天气查询demo</p><a id="more"></a><p>接下来主要探讨Retrofit实现的机制</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Retrofit retrofit = <span class="keyword">new</span> Retrofit.Builder()</span><br><span class="line">                .baseUrl(API_URL)</span><br><span class="line">             .addConverterFactory(GsonConverterFactory.create())</span><br><span class="line">                .build();</span><br></pre></td></tr></table></figure><p>这里Retrofit实例retrofit的生成是通过 Builder Pattern 来实现的</p><blockquote><p>The <strong>builder pattern</strong> is an <a href="https://en.wikipedia.org/wiki/Creational_pattern" target="_blank" rel="noopener">object creation</a> software <a href="https://en.wikipedia.org/wiki/Design_pattern_(computer_science" target="_blank" rel="noopener">design pattern</a>). Unlike the <a href="https://en.wikipedia.org/wiki/Abstract_factory_pattern" target="_blank" rel="noopener">abstract factory pattern</a> and the <a href="https://en.wikipedia.org/wiki/Factory_method_pattern" target="_blank" rel="noopener">factory method pattern</a> whose intention is to enable <a href="https://en.wikipedia.org/wiki/Polymorphism_(computer_science" target="_blank" rel="noopener">polymorphism</a>), the intention of the builder pattern is to find a solution to the telescoping constructor <a href="https://en.wikipedia.org/wiki/Anti-pattern" target="_blank" rel="noopener">anti-pattern</a>[<a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" target="_blank" rel="noopener"><em>citation needed</em></a>]. The telescoping constructor anti-pattern occurs when the increase of object constructor parameter combination leads to an exponential list of constructors. Instead of using numerous constructors, the builder pattern uses another object, a builder, that receives each initialization parameter step by step and then returns the resulting constructed object at once.</p></blockquote><p>这是Wikipedia上对于建造者模式的描述</p><p>使用建造者的主要原因就是 :  </p><p>构造函数的参数太多，如果重载很多个构造函数，很不优雅。所以就有了 builder 这样的媒介出现，来接收各个参数，然后再通过retrofit里面的各个函数来设定参数值。使用起来能够更加灵活。</p><p>然后通过builder.build()方法返回一个Retrofit的实例 就实现了对应参数下的retrofit实例的生成。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">HeWeatherApi</span> </span>&#123;</span><br><span class="line">    <span class="meta">@GET</span>(<span class="string">"now"</span>)</span><br><span class="line">    <span class="function">Call&lt;WeatherSearchResponse&gt; <span class="title">getResponse</span><span class="params">(@Query(<span class="string">"city"</span>)</span> String city,@<span class="title">Query</span><span class="params">(<span class="string">"key"</span>)</span> </span></span><br><span class="line"><span class="function">                                            String key)</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">HeWeatherApi weather = retrofit.create(HeWeatherApi.class);</span><br><span class="line">Call&lt;WeatherSearchResponse&gt; call =    </span><br><span class="line">        weather.getResponse(city_name.getText().toString(),API_KEY);</span><br><span class="line">call.enqueue(<span class="keyword">new</span> Callback&lt;WeatherSearchResponse&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onResponse</span><span class="params">(Call&lt;WeatherSearchResponse&gt; call,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   Response&lt;WeatherSearchResponse&gt; response)</span> </span>&#123;</span><br><span class="line"> <span class="comment">//业务逻辑</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Call&lt;WeatherSearchResponse&gt; call, Throwable t)</span> </span>&#123;</span><br><span class="line"><span class="comment">//失败处理</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>retrofit.create()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;T&gt; <span class="function">T <span class="title">create</span><span class="params">(<span class="keyword">final</span> Class&lt;T&gt; service)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//判断是否是一个service接口</span></span><br><span class="line">    Utils.validateServiceInterface(service);</span><br><span class="line">  <span class="comment">// validateEagerly为True 我们就直接调用这个service的方法</span></span><br><span class="line">    <span class="keyword">if</span> (validateEagerly) &#123;</span><br><span class="line">      eagerlyValidateMethods(service);</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">//否则 用Proxy生成一个新的代理类 并且必须实现 InvocationHandler这个接口 </span></span><br><span class="line">    <span class="keyword">return</span> (T) Proxy.newProxyInstance(service.getClassLoader(), <span class="keyword">new</span> Class&lt;?&gt;[] &#123; service &#125;,</span><br><span class="line">        <span class="comment">//实现InvocationHandler接口 可以用lambda 更优雅一点 2333</span></span><br><span class="line">        <span class="keyword">new</span> InvocationHandler() &#123;</span><br><span class="line">          <span class="comment">// 拿到平台 Java Android 或者是 iOS</span></span><br><span class="line">          <span class="keyword">private</span> <span class="keyword">final</span> Platform platform = Platform.get();</span><br><span class="line"><span class="comment">/* invoke方法 三个参数 代理类实例proxy 被调用的方法对象 method 以及 方法的参数 args 因为数量是不确定的 所以是 Object... 也可以是 Object[]</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">          <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Object <span class="title">invoke</span><span class="params">(Object proxy, Method method, Object... args)</span></span></span><br><span class="line"><span class="function">              <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">            <span class="comment">// If the method is a method from Object then defer to normal invocation.</span></span><br><span class="line">            <span class="keyword">if</span> (method.getDeclaringClass() == Object.class) &#123;</span><br><span class="line">              <span class="comment">//如果这个方法所属的类是Object  这里是我们自己定义的接口 结果为false</span></span><br><span class="line">              <span class="keyword">return</span> method.invoke(<span class="keyword">this</span>, args);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// Android.isDefaultMethod 结果为false</span></span><br><span class="line">            <span class="keyword">if</span> (platform.isDefaultMethod(method)) &#123;</span><br><span class="line">              <span class="keyword">return</span> platform.invokeDefaultMethod(method, service, proxy, args);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 最后通过loadServiceMethod来调用方法</span></span><br><span class="line">            ServiceMethod serviceMethod = loadServiceMethod(method);</span><br><span class="line">            OkHttpCall okHttpCall = <span class="keyword">new</span> OkHttpCall&lt;&gt;(serviceMethod, args);</span><br><span class="line">            <span class="keyword">return</span> serviceMethod.callAdapter.adapt(okHttpCall);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">eagerlyValidateMethods</span><span class="params">(Class&lt;?&gt; service)</span> </span>&#123;</span><br><span class="line">    Platform platform = Platform.get();</span><br><span class="line">    <span class="keyword">for</span> (Method method : service.getDeclaredMethods()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!platform.isDefaultMethod(method)) &#123;</span><br><span class="line">        loadServiceMethod(method);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 通过loadServiceMethod来调用接口的方法</span></span><br><span class="line">  <span class="function">ServiceMethod <span class="title">loadServiceMethod</span><span class="params">(Method method)</span> </span>&#123;</span><br><span class="line">    ServiceMethod result;</span><br><span class="line">    <span class="comment">// synchronized 锁住方法 避免因为片段打断造成的错误</span></span><br><span class="line">    <span class="keyword">synchronized</span> (serviceMethodCache) &#123;</span><br><span class="line">      result = serviceMethodCache.get(method);</span><br><span class="line">      <span class="keyword">if</span> (result == <span class="keyword">null</span>) &#123;</span><br><span class="line">        </span><br><span class="line">        result = <span class="keyword">new</span> ServiceMethod.Builder(<span class="keyword">this</span>, method).build();</span><br><span class="line">        <span class="comment">//用一个LinkedHashMap serviceMethodCache作为method的缓存  </span></span><br><span class="line">        serviceMethodCache.put(method, result);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>这段代码精华所在就是 动态代理 又是一种设计模式 代理模式</p><p>Retrofit在写的时候 是不知道关于所需要使用的对应的类的任何信息的，我们不可能根据每个人的要求写一个Retrofit出来，所以<strong>动态代理</strong>就出现了。</p><p>动态代理，我个人的理解就是，在运行时才知道对应是哪个类，然后通过Java的反射机制，拿到这个类的一些信息（ClassLoader、Methods等)，然后能够生成对应的代理类，然后我们再通过代理类来调用接口或者类的一系列方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> ServiceMethod <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="comment">//在ServiceMethod.build()方法里生成了我们需要的这个CallAdapter</span></span><br><span class="line">      callAdapter = createCallAdapter();</span><br><span class="line">      responseType = callAdapter.responseType();</span><br><span class="line">      <span class="keyword">if</span> (responseType == Response.class || responseType == okhttp3.Response.class) &#123;</span><br><span class="line">        <span class="keyword">throw</span> methodError(<span class="string">"'"</span></span><br><span class="line">            + Utils.getRawType(responseType).getName()</span><br><span class="line">            + <span class="string">"' is not a valid response body type. Did you mean ResponseBody?"</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      responseConverter = createResponseConverter();</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//后面代码是对于接口中的注释(Annotation)做一些解析和处理</span></span><br><span class="line">...</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> CallAdapter&lt;?&gt; createCallAdapter() &#123;</span><br><span class="line">      Type returnType = method.getGenericReturnType();</span><br><span class="line">      <span class="keyword">if</span> (Utils.hasUnresolvableType(returnType)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> methodError(</span><br><span class="line">            <span class="string">"Method return type must not include a type variable or wildcard: %s"</span>, returnType);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (returnType == <span class="keyword">void</span>.class) &#123;</span><br><span class="line">        <span class="keyword">throw</span> methodError(<span class="string">"Service methods cannot return void."</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      Annotation[] annotations = method.getAnnotations();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//实际上调用retrofit的callAdapter方法</span></span><br><span class="line">        <span class="keyword">return</span> retrofit.callAdapter(returnType, annotations);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (RuntimeException e) &#123; <span class="comment">// Wide exception range because factories are user code.</span></span><br><span class="line">        <span class="keyword">throw</span> methodError(e, <span class="string">"Unable to create call adapter for %s"</span>, returnType);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再来看看retrofit的callAdapter()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> CallAdapter&lt;?&gt; callAdapter(Type returnType, Annotation[] annotations) &#123;</span><br><span class="line">    <span class="keyword">return</span> nextCallAdapter(<span class="keyword">null</span>, returnType, annotations);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> CallAdapter&lt;?&gt; nextCallAdapter(CallAdapter.Factory skipPast, Type returnType,</span><br><span class="line">      Annotation[] annotations) &#123;</span><br><span class="line">    checkNotNull(returnType, <span class="string">"returnType == null"</span>);</span><br><span class="line">    checkNotNull(annotations, <span class="string">"annotations == null"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> start = adapterFactories.indexOf(skipPast) + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = start, count = adapterFactories.size(); i &lt; count; i++) &#123;</span><br><span class="line">      <span class="comment">//通过遍历adapterFacories工厂来找到匹配我们的返回类型的adapter</span></span><br><span class="line">      CallAdapter&lt;?&gt; adapter = adapterFactories.get(i).get(returnType, annotations, <span class="keyword">this</span>);</span><br><span class="line">      <span class="keyword">if</span> (adapter != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> adapter;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>据说在adapter这里可以结合RxJava用 我不是很清楚 所以这里就是Retrofit默认的callAdapter 是一个call</p><p>是一个异步的方法</p><blockquote><p>   /*<em> </em> Asynchronously send the request and notify {@code callback} of its response or if an error <em> occurred talking to the server, creating the request, or processing the response. </em>/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; void enqueue(Callback&lt;T&gt; callback);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>call.enqueue() 这个操作实际上是调用了ExecutorCallbackCall类里面的enqueue方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//默认的adapterFactory就是这个</span></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ExecutorCallAdapterFactory</span> <span class="keyword">extends</span> <span class="title">CallAdapter</span>.<span class="title">Factory</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> Executor callbackExecutor;</span><br><span class="line"></span><br><span class="line">  ExecutorCallAdapterFactory(Executor callbackExecutor) &#123;</span><br><span class="line">    <span class="keyword">this</span>.callbackExecutor = callbackExecutor;</span><br><span class="line">  &#125;</span><br><span class="line"> <span class="comment">// 我们在上面通过这个类的get方法 拿到了一个 ExecutorCallbackCall 作为我们的call</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> CallAdapter&lt;Call&lt;?&gt;&gt; get(Type returnType, Annotation[] annotations, Retrofit retrofit) &#123;</span><br><span class="line">    <span class="keyword">if</span> (getRawType(returnType) != Call.class) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">final</span> Type responseType = Utils.getCallResponseType(returnType);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> CallAdapter&lt;Call&lt;?&gt;&gt;() &#123;</span><br><span class="line">      <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Type <span class="title">responseType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> responseType;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="meta">@Override</span> <span class="keyword">public</span> &lt;R&gt; <span class="function">Call&lt;R&gt; <span class="title">adapt</span><span class="params">(Call&lt;R&gt; call)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ExecutorCallbackCall&lt;&gt;(callbackExecutor, call);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ExecutorCallbackCall</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">Call</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Executor callbackExecutor;</span><br><span class="line">    <span class="keyword">final</span> Call&lt;T&gt; delegate;</span><br><span class="line"></span><br><span class="line">    ExecutorCallbackCall(Executor callbackExecutor, Call&lt;T&gt; delegate) &#123;</span><br><span class="line">      <span class="keyword">this</span>.callbackExecutor = callbackExecutor;</span><br><span class="line">      <span class="keyword">this</span>.delegate = delegate;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 调用call.enqueue 是调用了这个方法</span></span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">enqueue</span><span class="params">(<span class="keyword">final</span> Callback&lt;T&gt; callback)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (callback == <span class="keyword">null</span>) <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(<span class="string">"callback == null"</span>);</span><br><span class="line"></span><br><span class="line">      delegate.enqueue(<span class="keyword">new</span> Callback&lt;T&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onResponse</span><span class="params">(Call&lt;T&gt; call, <span class="keyword">final</span> Response&lt;T&gt; response)</span> </span>&#123;</span><br><span class="line">          callbackExecutor.execute(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">              <span class="keyword">if</span> (delegate.isCanceled()) &#123;</span><br><span class="line">                <span class="comment">// Emulate OkHttp's behavior of throwing/delivering an IOException on cancellation.</span></span><br><span class="line">                callback.onFailure(ExecutorCallbackCall.<span class="keyword">this</span>, <span class="keyword">new</span> IOException(<span class="string">"Canceled"</span>));</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                callback.onResponse(ExecutorCallbackCall.<span class="keyword">this</span>, response);</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Call&lt;T&gt; call, <span class="keyword">final</span> Throwable t)</span> </span>&#123;</span><br><span class="line">          callbackExecutor.execute(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">              callback.onFailure(ExecutorCallbackCall.<span class="keyword">this</span>, t);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ExecutorCallbackCall又把网络请求委托给了OkHttpCall去实现，然后拿到Response，再根据我们重写的OnResponse来处理请求的数据。</p><p>本来还想继续挖一下OkHttpCall enqueue的实现的，不过我觉得Retrofit到这里就差不多了。</p><p>顺着这条线，有两个很重要的设计模式(事实上是三个)：Builder Pattern,Proxy Pattern,Factory Pattern (工厂模式出现了 但是没有提)</p><p>这也仅仅只是Retrofit的冰山一角，还要继续努力啊，加油！</p><p>参考资料：掘金-Retrofit源码剖析</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在小蓝项目里用Retrofit + OkHttp作为网络的框架&lt;/p&gt;
&lt;p&gt;来尝试着解读一下源码并且看能不能自己实现一个 低配版Retrofit&lt;/p&gt;
&lt;p&gt;使用方面 主要步骤就是&lt;/p&gt;
&lt;p&gt;1、添加依赖 Retrofit和OkHttp&lt;/p&gt;
&lt;p&gt;2、定义接口和对应的Response类&lt;/p&gt;
&lt;p&gt;3、在需要调用的时候生成对应的service 并且调用call方法&lt;/p&gt;
&lt;p&gt;4、在Callback方法里处理OnResponse的相关逻辑&lt;/p&gt;
&lt;p&gt;使用方法见RetrofitTest里面利用和风天气Api做的一个天气查询demo&lt;/p&gt;
    
    </summary>
    
    
      <category term="Android" scheme="https://tobiaslee.top/tags/Android/"/>
    
      <category term="Retrofit" scheme="https://tobiaslee.top/tags/Retrofit/"/>
    
  </entry>
  
  <entry>
    <title>BitOperators</title>
    <link href="https://tobiaslee.top/2017/01/31/BitOperators/"/>
    <id>https://tobiaslee.top/2017/01/31/BitOperators/</id>
    <published>2017-01-31T08:09:41.000Z</published>
    <updated>2017-02-05T08:37:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>昨天看的几个很有意思的问题 都是用 二进制方法解决的<br>今天学习的PKU的算法课里面有很有意思的熄灯问题 也是用二进制方法来简便的解决思路<br>二进制方法 最为重要的一点是<br><strong>二进制的思想 和 二进制位的操作</strong></p><p>这篇主要讲二进制的位操作，以后再记录一下关于思想的一些思考。</p><p>按位运算符主要有：<br>&amp; 按位与  （AND）<br>| 按位或  （OR）<br>^ 按位异或 （XOR）<br>左移&lt;&lt;<br>右移&gt;&gt;<br>~ 按位取反</p><p>&amp; 的使用很简单 1 &amp; 0  = 0, 1 &amp; 1  = 1 , 0 &amp; 0 = 0 ; 只有两边都为 1 时为 1<br>常用于 将二进制位 置0<br>| 同样         1 | 0  = 1, 1 | 1  = 1 ，0 | 0 = 0 ；只有两边都为 0 时为 0<br>常用于 将二进制位 置1<br>^当两个操作数对应位不同时 为 1 相同时 为 0       1 ^ 0 = 1, 1 ^ 1 = 0 , 0 ^ 0 = 0;<br>移位操作 &lt;&lt; &gt;&gt;分别用于将运算的操作数进行左移 与右移<br>00111 &lt;&lt; 2 得到的结果就是 11100 等价于对表达式 乘 4<br>左移补0，右移不同机器有不同的移位方式 有用符号位填补（算数移位） 也有用0填补 （逻辑移位)</p><a id="more"></a><p>看一个TCPL上的例子<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="title">getBits</span><span class="params">(<span class="keyword">unsigned</span> x ,<span class="keyword">int</span> p, <span class="keyword">int</span> n )</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x &gt;&gt; (p + <span class="number">1</span> - n ) &amp; ~ (~ <span class="number">0</span> &lt;&lt; n ) ) ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>二进制位 习惯上 <strong>从右往左数</strong><br>x &gt;&gt; ( p +１－ｎ） 将期望得到的字段移到最右端<br>要保留这n个位 制作出 00000…111 进行 &amp;<br>所以 利用 ~0 全是1的屏蔽码 左移n位 得到 1111..000<br>然后取反 得到 ~(~0 &lt;&lt; n )<br>再与 x 的右端进行 And 操作<br>即 (x &gt;&gt; (p + 1 - n ) &amp; ~ (~ 0 &lt;&lt; n ) ) </p><p>再来看一道练习题 </p><blockquote><p>编写一个函数 setBits(x,p,n,y) 该函数返回对x执行下列操作后的结果值:<br>将x中从第p位开始的n个二进制位设置成y中最右边的n位的值 x的其余各位保持不变。</p></blockquote><p>我们就是要把<br>x: xxxx…nnn…xxx 和<br>y: yyyy…yyy…nnn 进行一个对应位的交换<br>怎么做呢<br>首先是我们要把 x 的 n个位 置零 得到<br>xxxx…000…xxx 然后和 经过 置 1 操作的 y’<br>0000…nnn…000 进行 OR 操作 就可以完成赋值了</p><p>怎么得到 xxxx…000…xxx 呢<br>也就是要把原来的<br>xxxx…nnn…xxx 和<br>1111…000…111 进行一个 AND 操作<br>得到 1111…000…111的方法 就很简单<br>利用 <strong>~0</strong> 这个全部都是1的屏蔽码 左移n位  ~0 &lt;&lt; n得到  1111…111…000(n个)<br>然后 取反 即 ~(~0 &lt;&lt; n ) 得到 0000…000…111(n个)<br>再将其移动到 p位 处  即  (~(~0 &lt;&lt; n) &lt;&lt; (p -n + 1） 得到 0000…111…000<br>取反再移动到对应位上 利用自动补0这一特点 就变得更为轻松了 先移动的话就会得到 1111…000…000 这种尴尬地屏蔽码了<br>然后再取一次反 就得到我们最终需要的屏蔽码了 即  ~((~(~0 &lt;&lt; n) &lt;&lt; (p -n + 1）)<br>x &amp;  ~((~(~0 &lt;&lt; n) &lt;&lt; (p -n + 1）) 就是我们要的 p以后n位置0 以后的 x</p><p>怎么得到 1111…nnn…111呢<br>其实就是保留y右端的n位<br>利用 ~0 &lt;&lt; n 得到 1111…111…000<br>取反 ~(~0 &lt;&lt; n ) 得到 0000…000…111<br>再和y进行 AND 即 y &amp; ~(~0 &lt;&lt; n )得到 0000…000…nnn<br>左移 (y &amp; ~(~0 &lt;&lt; n )) &lt;&lt; (p -n + 1 ) 得到 0000…nnn…000</p><p>所以仿照着例子 我们可以写出这样的函数<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="title">setBits</span><span class="params">(<span class="keyword">unsigned</span> x ,<span class="keyword">int</span> p, <span class="keyword">int</span> n ,<span class="keyword">int</span> y )</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x &amp;  ~((~(~<span class="number">0</span> &lt;&lt; n) &lt;&lt; (p -n + <span class="number">1</span>）) |</span><br><span class="line">        (y &amp; ~(~<span class="number">0</span> &lt;&lt; n )) &lt;&lt; (p -n + <span class="number">1</span> ) ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>大功告成</p><p>总结一下：主要用到的技巧就是<br>OR 和 0 来进行保留原有位<br>AND 和 0 来 进行置 0<br>利用取反 和 移位 来构造恰当的 屏蔽码 实现位操作</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天看的几个很有意思的问题 都是用 二进制方法解决的&lt;br&gt;今天学习的PKU的算法课里面有很有意思的熄灯问题 也是用二进制方法来简便的解决思路&lt;br&gt;二进制方法 最为重要的一点是&lt;br&gt;&lt;strong&gt;二进制的思想 和 二进制位的操作&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这篇主要讲二进制的位操作，以后再记录一下关于思想的一些思考。&lt;/p&gt;
&lt;p&gt;按位运算符主要有：&lt;br&gt;&amp;amp; 按位与  （AND）&lt;br&gt;| 按位或  （OR）&lt;br&gt;^ 按位异或 （XOR）&lt;br&gt;左移&amp;lt;&amp;lt;&lt;br&gt;右移&amp;gt;&amp;gt;&lt;br&gt;~ 按位取反&lt;/p&gt;
&lt;p&gt;&amp;amp; 的使用很简单 1 &amp;amp; 0  = 0, 1 &amp;amp; 1  = 1 , 0 &amp;amp; 0 = 0 ; 只有两边都为 1 时为 1&lt;br&gt;常用于 将二进制位 置0&lt;br&gt;| 同样         1 | 0  = 1, 1 | 1  = 1 ，0 | 0 = 0 ；只有两边都为 0 时为 0&lt;br&gt;常用于 将二进制位 置1&lt;br&gt;^当两个操作数对应位不同时 为 1 相同时 为 0       1 ^ 0 = 1, 1 ^ 1 = 0 , 0 ^ 0 = 0;&lt;br&gt;移位操作 &amp;lt;&amp;lt; &amp;gt;&amp;gt;分别用于将运算的操作数进行左移 与右移&lt;br&gt;00111 &amp;lt;&amp;lt; 2 得到的结果就是 11100 等价于对表达式 乘 4&lt;br&gt;左移补0，右移不同机器有不同的移位方式 有用符号位填补（算数移位） 也有用0填补 （逻辑移位)&lt;/p&gt;
    
    </summary>
    
    
      <category term="C" scheme="https://tobiaslee.top/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Algorithms Notebook</title>
    <link href="https://tobiaslee.top/2017/01/29/Algorithms-Notebook/"/>
    <id>https://tobiaslee.top/2017/01/29/Algorithms-Notebook/</id>
    <published>2017-01-29T06:45:11.000Z</published>
    <updated>2017-02-13T13:05:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>突然萌生了一个想法 就是收集一些经典、优雅的算法<br>没事多翻翻看看 能够沉淀下来一些就好了</p><p>1、最大子序列的和<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">maxSubsequenceSum</span><span class="params">(<span class="keyword">int</span>[] a ,<span class="keyword">int</span> N)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> thisSum,MaxSum;</span><br><span class="line">    thisSum = MaxSum = <span class="number">0</span> ;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        thisSum += a[i];</span><br><span class="line">        <span class="keyword">if</span>(thisSum &gt; MaxSum)&#123;</span><br><span class="line">            MaxSum = thisSum;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(thisSum &lt; <span class="number">0</span>)&#123;</span><br><span class="line">            thisSum = <span class="number">0</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> MaxSum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在数列不是全部为负数的前提下<br>算法很巧妙的用两个变量来储存 当前子列和 以及 最大子列和<br>并且及时地交换二者的值或者是 在当前子列和 小于0时 重置为 0<br>实现 <strong>一遍遍历就能得到最大子列和的目的</strong>  即 复杂度为O(n)</p><a id="more"></a><p>2、二分查找<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="keyword">int</span> key,<span class="keyword">int</span>[] a)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> lo = <span class="number">0</span> ;</span><br><span class="line">    <span class="keyword">int</span> hi = a.length -<span class="number">1</span> ;</span><br><span class="line">    <span class="keyword">while</span>( lo &lt;= hi)&#123;</span><br><span class="line">         <span class="keyword">int</span> mid = (lo + hi) / <span class="number">2</span> ;</span><br><span class="line">         <span class="keyword">if</span> ( key &lt; a[mid]) hi = mid - <span class="number">1</span> ;</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">if</span>( a[mid] &lt; key) lo = mid + <span class="number">1</span> ;</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">return</span> mid ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span> ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>是一个很经典的算法 前提是数列有序 复杂度为 O(lg n)</p><p>3、进制转换<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> digits[] = &#123;<span class="string">'0'</span>,<span class="string">'1'</span>,<span class="string">'2'</span>,<span class="string">'3'</span>,<span class="string">'4'</span>,<span class="string">'5'</span>,<span class="string">'6'</span>,<span class="string">'7'</span>,<span class="string">'8'</span>,<span class="string">'9'</span>,<span class="string">'A'</span>,</span><br><span class="line"><span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>,<span class="string">'E'</span>,<span class="string">'F'</span>&#125;; <span class="comment">//全局变量</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">convert</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line"><span class="keyword">if</span>( x != <span class="number">0</span>) &#123;</span><br><span class="line">convert(x /y , y);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%c"</span>,digits[x % y]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>一个精巧的进制转换代码，将十进制整数 x 转换为 y 进制 然后输出。<br>进制转换无非就是 除 然后 取余 ，但是有一个输出和处理过程是颠倒的问题。<br>精妙之处在于利用递归解决了输出需要逆置的问题，也可以用栈结构来解决。</p><p>待续…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;突然萌生了一个想法 就是收集一些经典、优雅的算法&lt;br&gt;没事多翻翻看看 能够沉淀下来一些就好了&lt;/p&gt;
&lt;p&gt;1、最大子序列的和&lt;br&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;maxSubsequenceSum&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;[] a ,&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; N)&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; thisSum,MaxSum;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    thisSum = MaxSum = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt; ;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; i &amp;lt; N; i++) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        thisSum += a[i];&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(thisSum &amp;gt; MaxSum)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            MaxSum = thisSum;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(thisSum &amp;lt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            thisSum = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt; ;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; MaxSum;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在数列不是全部为负数的前提下&lt;br&gt;算法很巧妙的用两个变量来储存 当前子列和 以及 最大子列和&lt;br&gt;并且及时地交换二者的值或者是 在当前子列和 小于0时 重置为 0&lt;br&gt;实现 &lt;strong&gt;一遍遍历就能得到最大子列和的目的&lt;/strong&gt;  即 复杂度为O(n)&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://tobiaslee.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>WorkLoad</title>
    <link href="https://tobiaslee.top/2017/01/09/WorkLoad/"/>
    <id>https://tobiaslee.top/2017/01/09/WorkLoad/</id>
    <published>2017-01-09T07:13:20.000Z</published>
    <updated>2017-01-09T07:39:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>做了道TOJ的题，不是很难。<br>怎么说呢，就是脑袋里有想法，但是代码就是写不出来….<br>功力还是不行啊</p><p><a href="http://dsa.cs.tsinghua.edu.cn/oj/problem.shtml?id=53" target="_blank" rel="noopener">TOJ 69.Workload</a></p><blockquote><p>有n份工作要分配给n个人来完成，每个人完成一份。第 i 个人完成第 k份工作所用的时间为一个正整数tik，其中1 ≤ i, k ≤ n。试确定一个分配方案，使得完成这n份工作的时间总和最小。</p></blockquote><blockquote><p>输入<br>输入包含n+1行。</p></blockquote><blockquote><p>第一行为一个正整数n。</p></blockquote><blockquote><p>第2行到第n+1行中每行都包含n个正整数，形成了一个n×n的矩阵。在该矩阵中，第 i 行第k列元素tik表示第 i 个人完成第 k件工作所要用的时间。</p></blockquote><blockquote><p>输出<br>一行，包含1个正整数，表示所有分配方案中最小的时间总和。<br><a id="more"></a><br>思考的时候脑袋里大概有这么些想法:<br>1.用一个二维数组记录时间，totalTime就是第一维下标各不相同和第二维下标各不相同的相加之和<br>比如： n = 3 的时候 可能就是<br>time<a href="http://dsa.cs.tsinghua.edu.cn/oj/problem.shtml?id=53" target="_blank" rel="noopener">1</a>  + time[2][2] + time[3][3] //这里的第一个方括号打不出来 尴尬…<br>2.应该是一个人先挑一种工作，然后让下一个人，在剩余工作中再挑一个，这就需要一个isWorked的数组，<br>来表示各个工作是否已经被挑选掉，比如，0为未做，1为做了。<br>3.初始化时间花费cost为对角线之和 各种情况和cost比较 更新cost的值</p></blockquote><p>想法是有了，然而想写二重循环来解决…可能是个二维数组的思维定势<br>于是就卡壳了…<br>后来找到一篇blog 看了之后感觉就是实现了我的想法…<br>[CSDN ACM典例之工作分配问题][2]<br>  [2]: <a href="http://blog.csdn.net/f309587969/article/details/6338683" target="_blank" rel="noopener">http://blog.csdn.net/f309587969/article/details/6338683</a></p><p>  然后照着敲了一遍<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> isWorked[<span class="number">16</span>] = &#123;<span class="number">0</span>,&#125;;</span><br><span class="line"><span class="keyword">int</span> time[<span class="number">16</span>][<span class="number">16</span>] ;</span><br><span class="line"><span class="keyword">int</span> cost ;</span><br><span class="line"><span class="comment">// i表示工作的人 </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">work</span><span class="params">(<span class="keyword">int</span> i , <span class="keyword">int</span> count,<span class="keyword">int</span> n )</span></span>&#123;</span><br><span class="line"><span class="comment">//如果i 超出了工作的人数上限 表示分配完成 并且count比原来cost花费少 则更新cost的值</span></span><br><span class="line"><span class="keyword">if</span>( i &gt; n &amp;&amp; count &lt; cost )&#123;</span><br><span class="line">cost = count ;</span><br><span class="line"><span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> j ;</span><br><span class="line"><span class="comment">//回溯思想 </span></span><br><span class="line"><span class="keyword">if</span>(count &lt; cost)</span><br><span class="line"><span class="comment">// j表示第几件工作 </span></span><br><span class="line"><span class="keyword">for</span>( j = <span class="number">1</span> ; j&lt;= n;j++)&#123;</span><br><span class="line"><span class="comment">//如果工作未被做 isWorked = 0  </span></span><br><span class="line"><span class="keyword">if</span>(isWorked[j] == <span class="number">0</span>)&#123;</span><br><span class="line">isWorked[j] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//工作交给i+1 并且把开关设为1 表示已经做掉 </span></span><br><span class="line">work( i+<span class="number">1</span>,count+time[i][j],n);</span><br><span class="line"><span class="comment">//出来之后 要进行重新分配 将isWorked[j]重设为 0</span></span><br><span class="line">isWorked[j] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> n ; </span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line"><span class="keyword">int</span> i,j ;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">1</span> ; i &lt;= n ; i ++)&#123;</span><br><span class="line"><span class="keyword">for</span>(j = <span class="number">1</span> ; j &lt;= n ; j++)</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;time[i][j]);</span><br><span class="line">cost += time[i][i];</span><br><span class="line">&#125;</span><br><span class="line">work(<span class="number">1</span>,<span class="number">0</span>,n); </span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d\n"</span>,cost);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span> ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>关键就是这个work函数 递归的来实现了工作的分配<br>这应该就是我冥思苦想没想到的解决办法。</p><p>一道简单的题，暴露出水平还差得很。<br>不过不着急，慢慢来。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;做了道TOJ的题，不是很难。&lt;br&gt;怎么说呢，就是脑袋里有想法，但是代码就是写不出来….&lt;br&gt;功力还是不行啊&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://dsa.cs.tsinghua.edu.cn/oj/problem.shtml?id=53&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TOJ 69.Workload&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;有n份工作要分配给n个人来完成，每个人完成一份。第 i 个人完成第 k份工作所用的时间为一个正整数tik，其中1 ≤ i, k ≤ n。试确定一个分配方案，使得完成这n份工作的时间总和最小。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;输入&lt;br&gt;输入包含n+1行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;第一行为一个正整数n。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;第2行到第n+1行中每行都包含n个正整数，形成了一个n×n的矩阵。在该矩阵中，第 i 行第k列元素tik表示第 i 个人完成第 k件工作所要用的时间。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;输出&lt;br&gt;一行，包含1个正整数，表示所有分配方案中最小的时间总和。&lt;br&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://tobiaslee.top/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="C" scheme="https://tobiaslee.top/tags/C/"/>
    
      <category term="ACM" scheme="https://tobiaslee.top/tags/ACM/"/>
    
  </entry>
  
  <entry>
    <title>Lambda学习笔记</title>
    <link href="https://tobiaslee.top/2017/01/02/Lambda-Expressions/"/>
    <id>https://tobiaslee.top/2017/01/02/Lambda-Expressions/</id>
    <published>2017-01-02T01:13:35.000Z</published>
    <updated>2017-01-02T02:24:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>昨天学习了一下Java 8 中非常隆重推出的Lambda表达式<br>不过说实话好像非常的鸡肋???<br>个人感觉仅仅是表达方式上的更为简洁一点 似乎和我想象的函数式编程不那么一样?<br>不过也不是不能接受，毕竟作为一门以健壮性称道的语言，<br><strong>如果什么东西好就往里加，那Java就不是Java啦。</strong></p><p>Oracle 文档中，中对Lambda表达式的描述时这样的是这样的</p><blockquote><p>Lambda expressions are a new and important feature included in Java SE 8. They provide a clear and concise way to represent one method interface using an expression. </p></blockquote><a id="more"></a><p>我个人认为的重点就是三个词 “clear” “concise” “method interface”<br>来看看使用Lambda的三个例子，来看看Lambda到底怎么用以及优雅在哪里。</p><p>第一个例子:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//lambda examples</span></span><br><span class="line">Thread t1 = <span class="keyword">new</span> Thread( () -&gt;&#123;</span><br><span class="line">    System.out.println(<span class="string">"Hello world!,I'm Lambda!"</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//non lambda</span></span><br><span class="line">Thread t2 = <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello world,I'm not lambda"</span>);     </span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>这个例子在日常Codeing中使用频率还是比较高的，需要new 一个线程来多线程工作的时候,在没有Lambda以前的实现方式就是上面的t2的实现。<br>在引进Lambda表达式以后呢，就变成了t1。<br>非常直观的一点，就是Lambda确实更为简洁和清晰，将原来的6行代码缩减到3行，更加优雅了。<br>Lambda在这里究竟是怎么样一个作用呢？<br>当我们新建一个线程的时候，我们都会需要传入一个实现了Runnable接口的匿名内部类，然后通过重写这个匿名内部类的run()方法来执行我们想要执行的代码。<br>那么对于这种只有一个抽象方法的接口（比如Runnable）需要实现这种接口的对象时，我们就可以提供一个Lambda表达式，这种接口称为函数式接口。<br>所以在Lambda表达式中 我们利用()-&gt; {…} 替代了匿名内部类，产生的效果还是一样的。</p><p>第二个例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JButton jButton = <span class="keyword">new</span> JButton(<span class="string">"Test Button"</span>);</span><br><span class="line"><span class="comment">//non-lambda expression</span></span><br><span class="line">jButton.addActionListener(<span class="keyword">new</span> ActionListener() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">actionPerformed</span><span class="params">(ActionEvent e)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Button Clicked"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">//lambda expression</span></span><br><span class="line">jButton.addActionListener(e -&gt;</span><br><span class="line">        System.out.println(<span class="string">"Clicked detected by lambda"</span>));</span><br></pre></td></tr></table></figure><p>这个例子描述的是给JButton添加事件监听，事实上也就是实现了ActionListener中actionPerformed的一段代码。这里把6行代码缩减到2行，看起来更舒服，更为一目了然。<br>所以，<strong>Lambda表达式把很多臃肿的匿名内部类的创建过程给省略了，而直接通过实现函数式接口来完成代码块传递的这样一个过程</strong></p><p>第三个例子：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">repeatMsg</span><span class="params">(String text,<span class="keyword">int</span> delay)</span></span>&#123;</span><br><span class="line">    ActionListener listener = e -&gt; &#123;</span><br><span class="line">        System.out.println(text);</span><br><span class="line">        Toolkit.getDefaultToolkit().beep();</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">new</span> Timer(delay,listener).start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    repeatMsg(<span class="string">"Hello"</span>,<span class="number">1000</span>);  <span class="comment">//print "Hello" every second</span></span><br></pre></td></tr></table></figure></p><p>这里有一个问题，如果text这个参数变量已经不存在了，那怎么lambda表达式不就无法知道text是什么了吗？<br>巩固一下理解，Lambda表达式有三个部分:</p><ol><li>一个代码块；</li><li>参数；</li><li>自由变量的值，这是指非参数而且不在代码中定义的变量。<br>在这个例子里面，lambda表达式有一个自由变量text。<br><strong>表示lambda表达式的数据结构必须储存自由变量的值</strong><br>这里就是”Hello”这个字符串，我们说它被lambda给捕获了（captured）<br>关于代码块和自由变量值有一个术语叫做“闭包”（closure），所以，Java也是有闭包的。<br><strong>但是，Java所捕获的外部变量的值，必须是明确定义的，而且，其引用值不会改变</strong><br>这一点主要是考虑到了并发时线程安全的问题，也就是说如果该自由变量可能被外部函数所改变的话，那么就无法保证数据的准确性，也就不合法了。</li></ol><p>什么时候用Lambda表达式呢，Java 核心技术卷 I 中给出了以下几个原因：</p><ol><li>在一个单独的线程中运行代码（Thread的例子）；</li><li>多次运行代码（下面的repeat)；</li><li>在算法的适当位置运行代码（比如排序中的比较操作）；</li><li>发生某种情况时执行代码（按钮点击监听，数据到达等等）；</li><li><p>只在必要时才运行代码。</p><p>在使用Lambda的时候，只要定义自己需要的函数接口，然后实现它就可以了。<br>下面这个repeat的例子，就是重复操作的一个例子。</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IntConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">accept</span><span class="params">(<span class="keyword">int</span> value)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">repeat</span><span class="params">(<span class="keyword">int</span> n ,IntConsumer action)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; i ++) </span><br><span class="line">        action.accept(i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">repeat(<span class="number">10</span>,value -&gt; System.out.println(<span class="string">"Countdown:"</span> + ( <span class="number">9</span>  - value)));</span><br></pre></td></tr></table></figure><p>比较复杂的是在repeat这段代码中需要一个int类型的参数，而IntConsumer是处理int值的标准接口<br>通过value -&gt; {…} 将这个int传给了accept(int value)，实现相应的代码。</p><p>总结一下：<br>Lambda表达式主要用在函数式接口（只有一个抽象方法的接口）上来取代臃肿的内部类创建过程。<br>注意传入的参数要和函数的参数匹配。<br>Lambda有闭包，但是只能捕获最终变量（effective final) 即改变量初始化后不会再被更新，比如字符串常量。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天学习了一下Java 8 中非常隆重推出的Lambda表达式&lt;br&gt;不过说实话好像非常的鸡肋???&lt;br&gt;个人感觉仅仅是表达方式上的更为简洁一点 似乎和我想象的函数式编程不那么一样?&lt;br&gt;不过也不是不能接受，毕竟作为一门以健壮性称道的语言，&lt;br&gt;&lt;strong&gt;如果什么东西好就往里加，那Java就不是Java啦。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Oracle 文档中，中对Lambda表达式的描述时这样的是这样的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lambda expressions are a new and important feature included in Java SE 8. They provide a clear and concise way to represent one method interface using an expression. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Java" scheme="https://tobiaslee.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>2016 GoodBye , Hello 2017</title>
    <link href="https://tobiaslee.top/2016/12/31/2016-GoodBye-Hello-2017/"/>
    <id>https://tobiaslee.top/2016/12/31/2016-GoodBye-Hello-2017/</id>
    <published>2016-12-31T14:38:44.000Z</published>
    <updated>2016-12-31T15:13:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>2016年，过去啦。<br>刚看完《血战钢锯岭》回来，心情其实蛮沉重的。<br>但也想着在这个最后一天，写点什么吧。<br>人这个东西很奇怪，总是有一些奇怪的仪式感的东西，比如年度总结。<br>想到哪写到哪吧那就。</p><p>印象最深的自然是高考啦，<br>六月以前，一直都是在为高考而奋斗，而努力。<br>结果出来的时候，怎么说呢，波澜不惊。<br>没有实现自己的目标，也辜负了一些人的期望。<br>不过 <strong>一切都已经尘埃落定</strong> 能做的 也只有 <strong>往前看</strong></p><p>然后就到了西电，学校一般吧，<br>哪都有NB的人，也少不了一群庸人。<br>我要做的，就是努力成为 被人仰望的那一小撮人吧 。<br><a id="more"></a><br>16年的下半年算是真正开始了大学生活，<br>主要干了几件事情<br>1.学习Java 只能说堪堪入门<br>2.开始接触 Android 也只是皮毛<br>3.完成了若干本书的阅读<br>也确实没做出什么事情来，不过我觉得17年，是真正需要开始发力的时候了。</p><p>大学四年的规划其实大致有数了。<br>毕业之后就业 如果薪资待遇不能满足我的意愿 就保研<br>所以把GPA保持在前列是很重要的，目前看来也并不是很难做到。<br>就业，不出意外的话会是移动端也就是Android开发，再远一点的技术转型也暂时遇见不到了。<br>大三想去腾讯实习 0.0 毕业了也是 T &gt; A &gt; H<br>不想去帝都…感觉西安的霾已经快杀死我了 所以深圳 &gt; 杭州 </p><p>貌似T算是西电天花板?所以如果故步自封肯定会死的很惨。<br>17 年的规划 暂时如下， 明年年底来检查自己完成了几项。</p><ol><li>参加Coolpad的安卓开发大赛，无论结果，一定要努力去做。</li><li>好好学习数据结构和算法，虽说没有ACM的想法，但是OJ的题似乎和招聘很有关，完成清华数据结构的所有题目</li><li>继续深入学习Java 掌握底层实现 看完《深入理解Java虚拟机》 《Effective Java 》 《Java编程思想》</li><li>学习Android 了解底层的机制 并且对于使用的开源库的源码和实现要有所了解 不能做一个面条程序员</li><li>至少学会三首指弹曲子</li><li>学会一个人生活 拓宽知识面 至少看10本 与专业无关的书籍 (这是注定不找女朋友了吗?</li><li>拿到奖学金</li></ol><p>时间总是越过越快的，每一秒的过去，都是无法挽回的。<br>并不是说最后一天就会变得特别漫长，其实6月的一天和12月的一天，是一样的。<br>能做的，就是充实地过好每一天。<br>你要成为一个很厉害的人啊，然后才能去守护你想要守护的人。</p><p>2016,GoodBye ~<br>Hello,2017,<br>让美好发生!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2016年，过去啦。&lt;br&gt;刚看完《血战钢锯岭》回来，心情其实蛮沉重的。&lt;br&gt;但也想着在这个最后一天，写点什么吧。&lt;br&gt;人这个东西很奇怪，总是有一些奇怪的仪式感的东西，比如年度总结。&lt;br&gt;想到哪写到哪吧那就。&lt;/p&gt;
&lt;p&gt;印象最深的自然是高考啦，&lt;br&gt;六月以前，一直都是在为高考而奋斗，而努力。&lt;br&gt;结果出来的时候，怎么说呢，波澜不惊。&lt;br&gt;没有实现自己的目标，也辜负了一些人的期望。&lt;br&gt;不过 &lt;strong&gt;一切都已经尘埃落定&lt;/strong&gt; 能做的 也只有 &lt;strong&gt;往前看&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;然后就到了西电，学校一般吧，&lt;br&gt;哪都有NB的人，也少不了一群庸人。&lt;br&gt;我要做的，就是努力成为 被人仰望的那一小撮人吧 。&lt;br&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>三道清华Java测试题</title>
    <link href="https://tobiaslee.top/2016/12/15/java-1/"/>
    <id>https://tobiaslee.top/2016/12/15/java-1/</id>
    <published>2016-12-15T03:56:42.000Z</published>
    <updated>2017-08-10T11:54:54.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span>  </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">(Object object)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Object"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">     <span class="comment">// class X()&#123;&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">(String s)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"String"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// public void test(X x)&#123;</span></span><br><span class="line">    <span class="comment">//      System.out.println("X");</span></span><br><span class="line">    <span class="comment">// &#125;</span></span><br><span class="line">    <span class="keyword">static</span> String  s1 ,s0;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     <span class="comment">// 第一题</span></span><br><span class="line">        Main main = <span class="keyword">new</span> Main();</span><br><span class="line">        main.test(<span class="keyword">null</span>);</span><br><span class="line">     <span class="comment">// 第二题</span></span><br><span class="line">        String a = <span class="string">"abc"</span>;</span><br><span class="line">        String b = <span class="string">"ab"</span> + <span class="string">"c"</span> ;</span><br><span class="line">        System.out.println(a == b);</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 第三题</span></span><br><span class="line">        s0 = s1 + s0;</span><br><span class="line">        System.out.println(s0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一题 输出的是 String<br>因为在调用方法时 null可以匹配不同的重载版本  但是String s 的方法更为具体 所以选择他<br>如果是同样的具体的类 譬如说X  这样test.(null)方法时 因为X 和 String 都具体 所以会报<br>Ambiguous method 错误</p><p>第二题 输出true<br>这里考察的是编译时常量 和常量的折叠 以及String interning的知识<br>“abc” “a” “bc” 在Java里都是String类型的编译时常量  + 左右两侧都是常量时  这个+表达式也会被认为是编译时常量表达式</p><p>第三题 输出 nullnull</p><p>考察了静态变量的默认初始化 以及String的连接 +<br>静态变量会在类加载过程的linking阶段得到默认初始化 引用类型的静态变量会被默认初始化为null<br>然后是String 对象的连接 s0 = s1 + s0; 是一个语法糖<br>会被解糖为 s0 = new StringBuilder.append(s0).append(s1).toString();<br>StringBulider 根据规范 会把null的字符串 当做”null” 所以最后输出nullnull </p><p>TAT 我真是菜哭了….<br>还要好好学Java</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=
      
    
    </summary>
    
    
      <category term="Java" scheme="https://tobiaslee.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>多线程学习笔记</title>
    <link href="https://tobiaslee.top/2016/11/06/SynchronizeLearning/"/>
    <id>https://tobiaslee.top/2016/11/06/SynchronizeLearning/</id>
    <published>2016-11-06T12:31:28.000Z</published>
    <updated>2016-11-08T09:14:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>看着马士兵的视频学习了一些简单的多线程知识，<br>就着《Java核心技术1》 把相关知识梳理一下。</p><ul><li>1 线程和进程的区别:<br>  机器上的每一个exe文件，打开之后就会形成一个“进程”，进程是一个运行中的程序，是系统分配资源的单位，而“线程”，则是进程中多条指令集执行的路径，线程之间共享数据。与进程相比，线程更为轻量级。如果把进程比作一条马路，那么线程就是各个车道。</li><li><p>2 实现多线程的方法：</p><ul><li><p>1 定义Class t 实现Runnable接口 并且重写run方法 在主线程中New Thread 并且将t传入Thread（）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">R</span> <span class="keyword">implements</span> <span class="title">Runnable</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[]args)</span></span>&#123;</span><br><span class="line">    R r = <span class="keyword">new</span> R();</span><br><span class="line">    Thread t = <span class="keyword">new</span> Thread(r);</span><br><span class="line">    t.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>2 定义Class MyThread 继承Thread类 并且重写run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[]args)</span></span>&#123;</span><br><span class="line">    MyThread mt = <span class="keyword">new</span> MyThread();</span><br><span class="line">    mt.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是 <strong>直接调用对象的run方法是方法调用 而不会启动新线程</strong></p><a id="more"></a></li></ul></li><li>3 线程的一些属性和状态：<ul><li>1 优先级：<br>  一个线程的子类继承其父类的优先级，优先级的范围从1~10,线程调度器在有机会选择新线程时，会优先选择优先级别较高的线程。MIN_PRIORITY = 1，MAX_PRIORITY = 10，可以用setPriority方法来改变线程的优先级。</li><li>2 状态：<ul><li>1 New  </li><li>2 Runnable <strong>Runnable不代表线程正在运行!</strong> 只能说线程具有获得运行时间的能力 </li><li>3 Blocked 被阻塞 当线程试图获得一个内部对象锁 而该锁被其他线程所持有时 该线程被阻塞</li><li>4 Wating 等待 Object.wait或Thread.join 使线程“挂起” 等待通知 即Object.notify </li><li>5 Timed Wating Waiting状态的计时版 当时间超出设定或接到通知 终止该态度</li><li>6 Terminated 被终止 两种情况 因为run方法运行结束而自然死亡 和因未捕获的异常而异常死亡</li></ul></li></ul></li><li><p>4 线程同步：<br>  线程同步是为了解决，因多线程而导致代码片段执行不完整，造成的异常状况。<br>  实现线程同步的方法即给代码块加锁，保证其能够完整的一次执行完，而不会因为CPU的切换造成线程中断从而导致数据出错。<br>  加锁的方法有两种：</p><ul><li><p>1 Java SE 5.0 引入了ReentrantLock类 可以用来保护代码块</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">myLock.lock();<span class="comment">//a ReentrantLock object</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="comment">//critical section</span></span><br><span class="line">&#125;<span class="keyword">catch</span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">finally</span>&#123;</span><br><span class="line">    myLock.unlock()l; <span class="comment">//make sure the lock is unlocked even if an exception is thown</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>2 利用synchronized关键字来保护方法的执行</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> b = <span class="number">100</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">m1</span><span class="params">()</span></span>&#123;</span><br><span class="line">    b = <span class="number">1000</span>;</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">        Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">    &#125;<span class="keyword">catch</span>(InterruptedException e )&#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">"m1 b = "</span>  + b );</span><br><span class="line">&#125;    </span><br><span class="line"><span class="function"><span class="keyword">public</span>  <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">m2</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">    &#125;<span class="keyword">catch</span>(InterruptedException e )&#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    b = <span class="number">2000</span>;</span><br><span class="line">    System.out.println(<span class="string">"m2 b = "</span> + b );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在方法前加<strong>sychronized</strong>关键字来声明，那么对象的锁将保护该方法，如果该对象的其他方法试图获得该锁，那么他将进入阻塞状态，需等待拥有锁的方法执行完毕才能获得锁。<br>Tips:<strong>1 所有阻塞式方法 sleep,wait,await都有可能抛出InterruptedException 需要处理</strong></p><pre><code>**2 sychronized关键字使用起来较为方便 应尽可能的多使用它**</code></pre></li></ul></li><li><p>5 条件对象和死锁</p><ul><li><p>1 条件对象<br>  实际在设计多线程同步的程序的时候，如设计一个银行相互转账的系统，需要用同步方法来保证存取款时不被打断，金额才能正确。<br>不过在取款之前，需要设计一个变量，检测账户是否有足够的余额供取出才能继续执行取出操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> sychronized <span class="keyword">void</span> <span class="title">getMoney</span><span class="params">(acount[],amount)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(accounts[from] &lt; amount)&#123;</span><br><span class="line">        <span class="keyword">this</span>.wait(); <span class="comment">//当取出金额大于被取出账户余额的时候 调用wait方法 挂起线程</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当无法执行操作时，调用Object类的wait方法，使该线程<strong>放弃锁</strong>进入等待集，当锁可用时，该线程不能马上解除，相反，它仍会处于阻塞状态，需要等待另一个线程的通知”notify”</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> sychronized <span class="keyword">void</span> <span class="title">saveMoney</span><span class="params">(acount[];amount)</span></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.notify();<span class="comment">// this.notifyAll();</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当有用户向账户中存钱时，需要调用notify方法，通知被阻塞的取钱的线程，检查是否有足够金额可以取出。<br>注意区别wait和sleep方法：<strong>wait方法会释放对象锁 而sleep仍持有锁 wait方法是属于Object类的 而sleep是属于Thread类的</strong></p></li><li><p>2 死锁<br>  考虑下面的程序</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestDeadLock</span> <span class="keyword">extends</span> <span class="title">Runnable</span></span>&#123;</span><br><span class="line">     <span class="keyword">static</span> Object o1 = <span class="keyword">new</span> Object();</span><br><span class="line">     <span class="keyword">static</span> Object o2 = <span class="keyword">new</span> Object();</span><br><span class="line">     <span class="keyword">public</span> <span class="keyword">int</span> flag = <span class="number">1</span>;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">     </span><br><span class="line">         <span class="keyword">if</span>( flag == <span class="number">1</span>)&#123;</span><br><span class="line">             sychronized(o1)&#123;</span><br><span class="line">              </span><br><span class="line">              &#125;</span><br><span class="line">             sychronized(o2)&#123;</span><br><span class="line">                 System.out.println(<span class="string">"1"</span>);</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">if</span>(flag == <span class="number">0</span>)&#123;</span><br><span class="line">             sychronized(o2)&#123;</span><br><span class="line">                 </span><br><span class="line">             &#125;</span><br><span class="line">             sychronized(o1)&#123;</span><br><span class="line">                 System.out.println(<span class="string">"0"</span>);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[]args)</span></span>&#123;</span><br><span class="line">     TestDeadLock td1 = <span class="keyword">new</span> TestDeadLock();</span><br><span class="line">     TestDeadLock td2 = <span class="keyword">new</span> TestDeadLock();</span><br><span class="line">     td1.flag = <span class="number">1</span>;</span><br><span class="line">     td2.falg = <span class="number">0</span>;</span><br><span class="line">     Thread t1 = <span class="keyword">new</span> Thread(td1);</span><br><span class="line">     Thread t2 = <span class="keyword">new</span> Thread(td2);</span><br><span class="line">     t1.start();</span><br><span class="line">     t2.start();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>  当td1的run方法执行 锁住o1对象后试图锁住o2 而t2线程锁住了o2 t1无法得到o2的锁<br>  而t2也无法得到o1的锁 于是….就发生了线程死锁 两个线程都被阻塞了 这样就是死锁 (DeadLock)<br>  令人悲伤是，Java并没有任何东西可以完全避免或者打破死锁，必须仔细设计程序，确保不会出现死锁现象。</p></li></ul></li></ul><p>   差不多就学到了这里，线程的学习就暂时告一段落，日后应用再多多钻研。<br>    加油~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看着马士兵的视频学习了一些简单的多线程知识，&lt;br&gt;就着《Java核心技术1》 把相关知识梳理一下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 线程和进程的区别:&lt;br&gt;  机器上的每一个exe文件，打开之后就会形成一个“进程”，进程是一个运行中的程序，是系统分配资源的单位，而“线程”，则是进程中多条指令集执行的路径，线程之间共享数据。与进程相比，线程更为轻量级。如果把进程比作一条马路，那么线程就是各个车道。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2 实现多线程的方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 定义Class t 实现Runnable接口 并且重写run方法 在主线程中New Thread 并且将t传入Thread（）&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Runnable&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&amp;#123;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[]args)&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    R r = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; R();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Thread t = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(r);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    t.start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2 定义Class MyThread 继承Thread类 并且重写run方法&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;MyThread&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Thread&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&amp;#123;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[]args)&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    MyThread mt = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; MyThread();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    mt.start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;需要注意的是 &lt;strong&gt;直接调用对象的run方法是方法调用 而不会启动新线程&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Java" scheme="https://tobiaslee.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Black Mirror 观后感</title>
    <link href="https://tobiaslee.top/2016/11/05/%E9%BB%91%E9%95%9C%E8%A7%82%E5%90%8E%E6%84%9F/"/>
    <id>https://tobiaslee.top/2016/11/05/黑镜观后感/</id>
    <published>2016-11-05T11:42:28.000Z</published>
    <updated>2016-11-08T09:14:52.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://www.z4a.net/images/2016/11/05/blackmirror.jpg" alt="BlackMirror Icon"></p><p>刚刚刷完了黑镜的第三季<br>一时间五味杂陈 </p><p>Black Mirror这一个系列讲述的其实都是科技是一把双刃剑，<br>科技的高度发展，带来的一系列副作用。<br>毫无疑问地，存在着夸张放大的成分，<br>但仔细看下来，其实那些技术离我们的也并不远，<br>换句话说，剧中所呈现的道德、伦理、价值观的冲突和矛盾，<br>也许在不远的将来，也将是我们所要面对的。<br><a id="more"></a></p><blockquote><p>第一集，每个人都活在他人的评分之中。<br>第二集，沉浸式游戏的后果。<br>第三集，隐私泄露之后，人们沦为人偶。<br>第四集，死后可以在虚拟世界里永生。<br>第五集，军队用植入大脑的芯片欺骗士兵，让他们对同类开枪。<br>第六集，人们要为自己的言论负责。</p></blockquote><p>我觉得这一部片子是相当有启发意义的，<br>至少对于我，一名软件工程专业的学生来说，<br>It makes me realize how <strong>POWERFUL</strong> the technology is ,which is right in my hand.</p><blockquote><p>技术是一把双刃剑</p></blockquote><p>它的厉害之处就在于它能够把原来深藏的内心底的一些欲望给成千万倍的放大，<br>一个IT人员因为所爱的人被舆论所伤害，他就让所有通过舆论来对他人造成伤害的人，<br>接受以生命为代价的惩罚。<br>而作为掌握这样技术的我们，<br>心里也有欲望，也有那深不见底的黑暗。<br>一着不慎，走上那样的路，做出那样的决定，会怎么样？<br>不敢想。</p><p>怎么办，不知道。<br>我还在寻找答案。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://www.z4a.net/images/2016/11/05/blackmirror.jpg&quot; alt=&quot;BlackMirror Icon&quot;&gt;&lt;/p&gt;
&lt;p&gt;刚刚刷完了黑镜的第三季&lt;br&gt;一时间五味杂陈 &lt;/p&gt;
&lt;p&gt;Black Mirror这一个系列讲述的其实都是科技是一把双刃剑，&lt;br&gt;科技的高度发展，带来的一系列副作用。&lt;br&gt;毫无疑问地，存在着夸张放大的成分，&lt;br&gt;但仔细看下来，其实那些技术离我们的也并不远，&lt;br&gt;换句话说，剧中所呈现的道德、伦理、价值观的冲突和矛盾，&lt;br&gt;也许在不远的将来，也将是我们所要面对的。&lt;br&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Hello Ralee,Our Studio!</title>
    <link href="https://tobiaslee.top/2016/10/22/Hello-Ralee-Our-Studio/"/>
    <id>https://tobiaslee.top/2016/10/22/Hello-Ralee-Our-Studio/</id>
    <published>2016-10-22T14:23:16.000Z</published>
    <updated>2016-11-05T12:34:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://www.z4a.net/images/2016/11/05/Ralee.th.jpg" alt="Ralee Icon"></p><p>Ralee工作室 成立啦<br>虽然现在只有两个菜逼 我和heroHR<br>附上HR的Blog<br><a href="http://37a.club" target="_blank" rel="noopener">37aclub</a><br><em>不过这是first step!</em><br><em>以后我们会走的更远的 加油</em></p><p>挫折好多啊<br>网站备案第一次没通过<br>不过现在已经把材料寄出去啦<br>还要谢谢翔dalao的幕布呢！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://www.z4a.net/images/2016/11/05/Ralee.th.jpg&quot; alt=&quot;Ralee Icon&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ralee工作室 成立啦&lt;br&gt;虽然现在只有两个菜逼 我和heroHR&lt;br&gt;附上HR的Blog&lt;br
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://tobiaslee.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Hello My Blog</title>
    <link href="https://tobiaslee.top/2016/10/19/Hello-My-Blog/"/>
    <id>https://tobiaslee.top/2016/10/19/Hello-My-Blog/</id>
    <published>2016-10-19T09:20:27.000Z</published>
    <updated>2016-10-19T09:21:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>捣鼓了一个下午<br>总算搞好了一个看起来不错的Blog<br>哈哈哈<br>Well Done!!<br>去写大物了<br>试一下引用<br>胡适的一句话</p><blockquote><p>怕什么真理无穷 进一寸有进一寸的欢喜</p></blockquote><p><strong>:) 一脸满足</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;捣鼓了一个下午&lt;br&gt;总算搞好了一个看起来不错的Blog&lt;br&gt;哈哈哈&lt;br&gt;Well Done!!&lt;br&gt;去写大物了&lt;br&gt;试一下引用&lt;br&gt;胡适的一句话&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;怕什么真理无穷 进一寸有进一寸的欢喜&lt;/p&gt;
&lt;/blockquote&gt;
&lt;
      
    
    </summary>
    
    
  </entry>
  
</feed>
